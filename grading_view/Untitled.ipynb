{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file_path = \"/home/santhosh/3rd_Semester/R&D/project/ipython_datatset/notebooks/\"\n",
    "\n",
    "\n",
    "## getting all the names of the file\n",
    "file_list = list()\n",
    "for file in os.listdir(file_path):\n",
    "    if file.endswith(\".ipynb\"):\n",
    "        file_list.append(file)\n",
    "file_list = sorted(file_list)\n",
    "\n",
    "## craeting a dictionary of question anwers\n",
    "\n",
    "ques_ans_dict = {q:[] for q in np.arange(0,17)}\n",
    "\n",
    "for ff in file_list:\n",
    "    with open(file_path+ff) as f:\n",
    "        data = json.load(f)\n",
    "    ques_count = 0\n",
    "    ans_list = []\n",
    "    ques_list = []\n",
    "    for i in range(len(data['cells'])):\n",
    "        if 'nbgrader' in data['cells'][i]['metadata'] and (data['cells'][i]['metadata']['nbgrader']['solution'])==True:\n",
    "            ques_ans_dict[ques_count].append((data['cells'][i]['source']))\n",
    "            ques_count+=1\n",
    "            ques_list.append ((data['cells'][i-1]['source']))\n",
    "            if ques_count == 17:\n",
    "                break\n",
    "\n",
    "## ordering and cleaning answers\n",
    "answer_list = []\n",
    "for i in range(len(ques_ans_dict.keys())):\n",
    "    for answers in ques_ans_dict[i]:\n",
    "        answer_list.append(''.join(answers))\n",
    "## ordering and cleaning questions\n",
    "ques_list = [i[2] for i in ques_list for _ in range(len(ques_ans_dict[0]))]\n",
    "\n",
    "final_dict = {'question':ques_list, 'student_answer':answer_list ,\\\n",
    "             'question_id':[i for i in range(1,18) for _ in range(len(ques_ans_dict[0]))],\\\n",
    "            'student_id':[i.split(\".\")[0] + str(j) for j in range(1,18) for i in file_list]}\n",
    "df = pd.DataFrame.from_dict(final_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': [' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  ' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Define the mathematical model of a neuron, use the appropriate technical terms!',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Explain classification and regression; what is the difference?',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Write down the SOM learning in pseudo code.',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  'Give the basic idea of an SVM using the correct terminology!',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  ' What role does the method of steepest decent have when learning a network?',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Write down and explain the Widrow-Hoff learning rule!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'Explain back propagation, use the correct technical terms!',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'When learning using steepest descent, explain the role of the learning rate? What is a danger?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'How does a Reduced Boltzman Machine work (main idea)?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Define: Echo State Network (ESN), how are they different to FF NNs?',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'Describe: the structure on an CNN.',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Describe how learning based on k-nearest neighbors works, use pseudo code!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!',\n",
       "  'Explain the Bias Variance Dilemma!'],\n",
       " 'student_answer': ['An artificial neural network is a massively parallel distributed processor with simple processing units that has the natural propensity to store experiential knowledge and make use of them.\\n\\nAn artificial neural network is similar to the human brain in two ways:\\n\\n1. The ANN works by the process of learning from its environment.\\n2. Interneuron connections called synaptic weights are used to store the knowledge gained.\\n',\n",
       "  'Artificial neural network consists of:\\n\\n    . Largely parallel distributed processor\\n    \\n    . simple processing units\\n    \\n    . that has ability to store the experential knowledge and making it available to use\\n    \\nIt resembles to human brain in two ways:\\n\\n    . Knowledge is acquired from the environment by the network as learning process\\n    . Synaptic strengths called weights are used to store the knowledge',\n",
       "  'YOUR ANSWER HERE An artificial neural network is a massive distributed processor. It consists of several information processing units which are able to acquire and store knowledge.',\n",
       "  'An ANN is a layered graphical model containing neurons and weighted connections, resembling the excitatory properties of the human brain. Weights of the ANN are changed after presenting it training examples from an environment, where weights are changed based on the training procedure used. Artificial neurons also are biased, just like real ones, adding a constant level of activation before being activated by a (nonlinear) activation function. Depending on the training procedure, both weights, topology or even activation functions may be learned.',\n",
       "  'Artificial Neural Networks are large parallel processing units that have the natural ability to learn experiential knowledge. They are composed of interconnected neurons as basic units; which in turn cosists of weights, squashing functions and adder functions.\\n\\nANN resembles brain in the manner that like in human brain, it is composed of a network of neurons which help in learning by adjusting the synaptic weights of the connections between neurons. This enables it to learn experiential knowledge.',\n",
       "  'An articial neural network consists of neurons. Each neuron can have several weighted inputs, an activation function and output. Usually several neurons are connected together. Often in layers. The network then calculates the output given an input to the network. The human brain works in a similar way. It also consits of neurons that are connected in several ways.',\n",
       "  'An ANN is a\\n- massivly parallel distibuted Processor\\n- made up of simple processing units\\n- which have the capability of storing experiantal knowlenge\\n- and is made up for use.\\n\\nAn ANN resembles the brain because:\\n\\n1) it gets its knowlenge through a learning process from its environment.\\n\\n2) it stores its knowlenge in its interneuron connections (synaptic weights)',\n",
       "  'A ANN is a massively distributed processor. It has the propensity to store experiental knowledge and make it available for use. The knowledge is gained throug a process of learning. The knowledge is stored in the weights between the neurons. This structure resembles the structure of the brain. Neurons are a the basic information unit in the ANN and act similiar to real neurons.',\n",
       "  'An artificial neural network is defined as a learning machine which is divided by layers and each layer is composed by neurons. The neurons from different layers can be connected between each other, and give an output or multiple outputs by a given input. This structure is very similar with the neurological structure of our brain, where neurons are interconnected by synapses. Also important to mention, if a feature is really important for a given task, this wil have more connections and neurons participating (like in the human brain, the important humasn functions have more synapses).',\n",
       "  'An artificial neural network is a graph of small and identical processing units that these small units called neurons and they are connected to each other in different architectures and the whole network adapt and itself to the environment inputs by trying to decrease the error or the cost function and increase its preciseness by manipulating the free variables of the network which are the synaptic weights.\\n\\nIt is similar to human brain because similar to the human brain we have many small processing units that are connected together and they react to the environment and learn from the environment.',\n",
       "  'Artificial neural network is highly parralel processing. It has a mathematical model similar to human brain, which it was inspired from, as human brain does computation in an extremely parallel manner. Similarities also lay in terminology, ANN is using neurons that are smallest computing unit of a network, similarly to human brain.',\n",
       "  'It is a massive parallel distributed processor made up of smaller processing units, that aquire knowledge through the environmnet through a learning process and makes it available for use. It resembles the brain in two ways:\\n\\n- Knowledge is aquired through a stimulating process in the environment\\n- The knowledge is embedded in the synaptic links (weights) of the neurons. ',\n",
       "  'ANN is a learning machine which is composed of neurons as units of computation. The ANN learns via interacting with its environment. The ANN has built-in capacity to dynamically adapt upon input stimulus.\\n\\nThe ANN is motivated from biological brain and resembles human brain in terms of its localized representation for the inputs. In terms of motor cortex, the sensory stimulus to diffrent body-parts activates local part of the brain, similar to ANN local representation of similar type of input.',\n",
       "  'A neural network is a massively parallel distributed prcoessor made up for simple processing units that has a natural propensity for storing experiential knowledge and making it available for use. It resembles the brain in two respects\\n* Knowledge is acquired by the network from its environment through the learning process\\n* Interneuron connection strengths known as synaptic weights, are used to stor the acquired knowledge.',\n",
       "  'Artificial neural network is a massively parallel distributed processor which consists of one or more processing unit called neuron. It resembles the human brain for that it acquires knowledge from the environment through learning process, and that the acquired knowledge is stored in the synapses.',\n",
       "  'Definition:\\n1. Artificial neural networks are massively distributed parallel processor.\\n2. It is made up of small units, \\n3. Which has the propensity for storing the experential knowledge.\\n4. And making it available for use. \\n\\nIt resembles the brain in 2 aspects. \\n1. Similar to the brain, artificial neural network does the process of learning from the environment. \\n2. It as a pair of inter neuron links known as the synaptic weights, which is used for storing the information. ',\n",
       "  'Artificial neural network is massive parallely distributed processor. It comprises of small processing units called neurons. It learns from experiencial knowledge which is then stored and can be used for making predictions. It resembles human brain in 2 ways:\\n* It learns from experiencial knowledge\\n* Knowledge is stored in synaptic interneuron connections.',\n",
       "  'YOUR ANSWER HERE: Artificial neural network is a massively distributed parallel processor which is composed of simple processing units called neurons, which have the natural propensity for storing experiential information and making it available for use. It resembles the human brain in the following aspects.\\n- Knowledge is acquired by the network from its environment through a learning process.\\n- Synaptic links are used to store the acquired knowledge.',\n",
       "  'ANN is a learning machine which can perform complex parallel computation. It has the ability to learn through the interactions withthe environment and store the learned knowedge. \\n\\nIt resembles the human brain in performing complex learning tasks, acquiring information, apadpting to the environment, and exploiting the acquired information.',\n",
       "  'An artificial neural network is a massively distributed parallel processor made up of simple processing units that have the natural propensity for storing experiental knowledge and making it available for future use.\\n\\nIt resembles the brain in the following ways:\\n\\n1. Artificial neural networks have the ability to acquire knowledge from the environment in which they are are embedded.\\n2. Inter-neuron connection strenghts called synaptic links activate each neuron during the learning process.',\n",
       "  'An Artificial Neural Network is a massively parallel distributed processor which interacts with its surrounding environment, with a propensity to store knowledge and make it available to use.  \\nIt resembles the brain in two aspects:  \\n1. It has the ability to learn from its environment  \\n2. The knowledge is stored in synaptic weights',\n",
       "  'Artificial neural network is massively distributed paralled processor containing simple processing units and has natural propensity to store experiential knowledge and use it.It resembles the human brain in two aspects, it gains knowledge from the environment and adapts the synaptic weight to store the knowledge.',\n",
       "  'It is a massively parallel distributed processor consisting of simple processing units, which can store experiential knowledge and make it available for use. it resembles the human brain in 2 ways: 1. knowledge is acquired from environment through a learning process; 2. interneuron connections are used to store the experiential knowledge.',\n",
       "  'Artificial neural network is a massively parallel distributed processor that is made up of simple processing units called neuron. It can replicate human brain by storing information in their weights',\n",
       "  'Artificial neural network is a **massively parallel distributed processor** with synaptic links that can able to **store experimental knowledge** and make it available for use.\\n\\nIt resembles human brain in two ways,\\n\\n* Knowledge is acquired by the neural network from its environment through learning process.\\n* Interneuron connection called synaptic links stores the acquired knowledge.',\n",
       "  'Artificial neural network are the network of the units that learn data from the environment and store them using synaptic weights.\\n\\nThe structure of the artificial neural network is similar to human brain. It has neurons, ie., the store units and the axoms called synapses which link the stored data.',\n",
       "  'Artificial neural network is massive parallel processor made up of simple processing units called neurons. \\n\\nThey are capable of storing experential knowledge and make it available for later use. \\n\\nSimilarity to human brain: \\n1. they learn from the envirnoment \\n2. they store knowledge as synaptic weight in the interneuron connection ',\n",
       "  'An artificial neural network is a highly distributed processor which consists of several simple processing units. It resembles the human brain, because the processing units are neurons, which are connected with weights. The human brain also consists of neurons.',\n",
       "  'A massively distributed processor, consisting of single processing units that have a natural prospensity of storing experimental knowledge and making it available for use.',\n",
       "  'An artificial neural network consists of neurons, which are small computation devices,and synapses, the connections between the neurons. This resembles the brain because it also has neurons and synapses. Also a artificial neural network has weights, which are used to store learned features from the environment. Like the brain a neural network learns from the environment. An artificial neural network also has an activation function, which creates the output.',\n",
       "  'An artificial neural network is a highly parallel computation model with learning and memory capacities. Similar to the brain it learns from the environment by strengthening the synapses between neurons. Once a task is learned it can be quickly used by reactivating those learned synapses.',\n",
       "  'An artificial neural network is a highly parallel working machine which consists of simple processing units (neurons) wich are connected to each other in layers. they are function approximators \\nthe brain is resembled in the architecure, the processing units and thge weights and how the learning process takes place and the properties of the brain: fault tolerance, parallel computing, ... ',\n",
       "  'An ANN is a massivly parrallel distributed learing machine made up of small computational units. Computational units are connected via synapses defined by a weight. It resembles the human brain in two aspectes:  \\n',\n",
       "  'Artificial neural network is massively parallel distributed processor made up of simple computing units called neurons which aquires knowledge from environment through learning. It resembles brainlike structure in two ways, \\n\\n1. It aquires knowledge through learning and experience \\n2. It stores knowledge in interneuron connections called synapses. ',\n",
       "  'ANN is huge parallel distributed processor ,\\n\\nconsist of simple processing units and \\n\\nwhich has propensity of storing experintial knowlegde \\n\\nand making it available for use.',\n",
       "  'Artificial neural network is a massively parrallal distributed processor made up of simple processing units which has a natural propensity to acquire knowledge from the environment and make it available for future use.\\n\\nIt resembels the human brain in following ways.\\n\\n1. Both of them acquire knowledge from the environment.\\n2. The neurons are connected by synapses cahrecterized by their weights which can be adjusted.',\n",
       "  'YOUR ANSWER HERE\\n\\nAn artificial neural network is a massively distributed parallel processor made of simple processing units. It has natural propensity to store experential knowledge and it makes the knowledge available for further use.\\n\\nAn artificial neural network uses inter neuron connections called synaptic weights to store the knowledge acquired knowledge which is very similar to how human brain works.',\n",
       "  'Ein Neuronales Netz ist ein massiver Parallele Rechen Methode, die aus mehreren einfachen Rechen Einheiten besteht, durch Erfahrung lernt und das Wissen verfügbar macht. Es nahmt das Menschlische Gehirn nach dadruch dass es durch Erfahrung lernt, und dadurch, dass das Wissen in Form von Axionen, hier Gewichte, zwischen den einzelnen Neuronen speichert. ',\n",
       "  'ANN is a massively distributed processor, consisting of simple processing units called neurons. These neurons in terms of ANN are similar to neurons in human brain. Both neurons are characterized by synapses(connection links). They represent connections used for data flow between neurons. In both ANN and Human brain, the knowledge is represented by its very structure and activation state of neurons.  ',\n",
       "  'A neuron is the simplest processing unit of a neural network which has:\\n\\n1. synaptic weights to store the knowledge gained.\\n2. Adder function (linear combiner) which adds the weighted values of the input signals to produce the local field.\\n3. An activation function which squashes the local field to a range of values.\\n\\n$ \\\\phi(\\\\sum_{i=0}^{N} w_i \\\\cdot x_i) $',\n",
       "  'Mathematical model of a neuron is given as :\\n\\n   y = $\\\\phi(V)$ , where activation function is applied to local field(V)\\n   \\n   V = $\\\\summation (w_{i}x_{i} + b)$  . Local field is weighted(w) sum of inputs(x) plus bias(b)\\n   \\n   ',\n",
       "  'YOUR ANSWER HERE A neuron is an information processing unit. It consits of: inputs associated with weights, sum of inputs and an acitvation function\\n\\n',\n",
       "  'Input vector $x$\\n\\nWeight matrix $w$\\n\\nNet input $net=\\\\sum x^Tw$\\n\\nNet output $o=\\\\phi(net)$',\n",
       "  'A neuron consists of three basic components:\\n   - *Synaptic Weights*: The synaptic weights are connections between neurons and are adjusted through training.\\n   - *Squashing/Activation Functions*: The squashing functions may be non linear or linear functions that that are applied to the signals from the neurons\\n   - *Adder Functions*: The adder functions help in combining outputs from several neurons. \\n   \\n\\n',\n",
       "  '$N$ number inputs, $x_i$ input i, $v_j$ local field, $\\\\varphi(v_j)$ activiation function, $y_j$ output, $w_{ji}$ weight from node i to j\\n\\n$y_j = \\\\varphi(v_j)$\\n\\n$v_j = \\\\sum_{i=0}^{N}w_{ji}x_i$',\n",
       "  'A neuron is a simple processing unit of an ANN, that is made up of\\n\\n- the synaptik links which are defines by a weights $(w_1,...,w_n)$\\n- a adder function that combines the weighted input $(w_i*x_i)$ plus some bias $(b)$ to the local field  $(\\\\sum{w_i*w_i}) +b=v$\\n- a activation function phi that squaches the local field to the output $(phi(v)=y)$ ',\n",
       "  'The neuron consists of synapses/connecting link each characterised by a weight. A linear combiner sums up the weighted sum of inputs to a local field. The local field is then passed through an activation function. The result of the activation function is the output.',\n",
       "  'A neuron is defined by the following elements:\\n- A number of input values x\\n- A number of weights w\\n- A bias b\\n- An activation function $/phi$.\\n\\nThe inputs x are multiplied with the weights, and the result is summed with the bias (also, the bias can be used just as a weight value b and a single connetion with an stable input equal to 1, for mathematical simplicity). The resulting value, known as local field (v), will be the input to the activation function.\\n\\nThe mathematical model can be summarized in the formula:\\n\\n$v = \\\\sum^{n}_{i = 0} x(i)*w(i) + b$\\n\\n$y = \\\\phi(v)$',\n",
       "  'A neuron consists of a set of inputs and a bias which these inputs and predefined bias will be multiplied by a weight and then we have sum the results of all the inputs and bias multiplied by the weights which called induced field and after that we send this to an activation function which can be a linear or non-linear function and the output of this function is the final output of our neuron. ',\n",
       "  'Neuron is a simplest computation unit of a neural network that consists of input variables, weights, bias, summation term (combiner), activation function and output variables.',\n",
       "  'The neuron is the basic processing unit of a neural network and is made of three main component:\\n- Weights: $w_1, w_2, ...,w_n$\\n- Adder function: it is the linear combination of the input and weights plus bias. (induced local field) $v = \\\\sum w_i x_i + b$\\n- Squashing function: it is the activation function applied to the local field used to limit the output of the neuron. $\\\\phi(v)$\\n',\n",
       "  'A neuron is a computational unit composed of\\n+ synapses which are stored in the form of weights $w$. These are the variables that can are dynamical.\\n+ summing function that computes the weighted sum of inputs: $v = \\\\sum_i (w_ix_i)$\\n+ activation function $\\\\phi$: gives nonlinear nature to network, determines and normalizes the output produced by neuron. e.g. sigmoid function\\n+ bias: another synaptic tunable variable with input 1. Therefore the net output of neuron: $ y = \\\\sum_i (w_ix_i) +b$.',\n",
       "  'The following equations describe a nonlinear model of a neuron, labeled k.\\n\\n1)u_k = sum from j=1 to m w_{kj} x_{j}\\n\\n2)y_k = phi(u_{k} + b_{k})\\n\\nwhere x_{j} are the input signals; w_{kj} are the weights of the neurons; u_{k} is the linear combiner output due to the input signals; b_{k} is the bias; phi() is the activation function; and y_k is the output signal of te neuron.',\n",
       "  'A neuron is a processing unit that contains three main components: a set of synaptic weights that connect the neuron with other neurons; an adder that computes the induced local field, or the weighted sum of the signals flowing through the neuron; an activation function that constrains the magnitude of the output signal from the neuron.',\n",
       "  'YOUR ANSWER HERE\\nA Mathematical model of a neurons consits of a \\n\\n1. A set of synaptic links which are classified based on weights(w1, w2, w3...w_n)\\n2. It consits of a adder function, which performs the weighted sum of the inputs and the bias.\\n$\\\\Sigma_{i=1....n} w_n.x + b$\\n\\n3. It consists of an activation function, used to minimize the amplitude of the neuron output.\\n$\\\\Phi(\\\\Sigma_{i=1....n} w_n.x + b)$',\n",
       "  'A mathematical model of neuron comprises of 2 main units:\\n* Adder functions: it sums up all the product of all synaptic connections and inputs of neuron\\n* Synaptic weights: these are interneuron connections in which the knowledge is stored\\n* Activation function: it is used for introducing non-linearity ',\n",
       "  \"YOUR ANSWER HERE: The neuronal model consists of the following:\\n- Synaptic links characterized by their weights which connects the network to the environment it is embedded in.\\n- An adder function which sums up the weighted inputs and outputs the induced local field of the neuron.\\n-  An activation function which takes the induced local field of the neuron as it's input and limits the output of the neuron.\",\n",
       "  'A mathematical model of neuron consists of 3 important parts.\\nA neuron is the smallest computaional node with:\\n1) Input vectors : set of vectors of a certan dimension to train the model\\n2) Weights (and biases): each of the input vectors are weighted using weight vectors in accordance withthe output that is required. Bias is added when necessary.\\n3) Activation function : The linear combination of weights and inputs are passed through the activation function which produces an output.',\n",
       "  \"The neuron is the fundamental processing unit of an aritificial neural network that is characterised by the followig features:\\n\\n1. A neuron has a set of non-linear synaptic links, an externally applied bias, and possibly one or more linear activation links. The bias is represented by a synaptic link from an input fixed at +1.\\n2. The synaptic links of the neuron weight the respective inputs.\\n3. An adder function (linear combiner) computes the weighted sum of the inputs to the neurons.\\n4. An activation function (squashing function) limits the amplitude of the neuron's output.\",\n",
       "  'Let $x_1$, $x_2$, ... , $x_N$ be the inputs to the neuron, $w_i$ be the corresponding weights of connections, $b$ be the bias and $\\\\varphi(.)$ be the activation function. \\nThen, the induced field $v$ is given by -\\n\\n$$v = \\\\sum_{i = 1}^{N} w_i .x_i + b$$\\n\\nThe output $y$ is given by -  \\n$$y = \\\\varphi(v)$$',\n",
       "  'The mathematical model of neuron has three parts:\\n- a set of synapses or connencting links characterized by weight ,w .\\n- an adder function that calculates the weighted sum of inputs plus some bias\\n- an activation function (squashing function) to minimize the amplitude ',\n",
       "  '$v_k = \\\\sum_{j=1}^{m} w_{kj} x_j + b_k$, $y_k = \\\\phi(v_k)$, $w_{kj}$ is the synaptic weight  connecting neuron k and input data j, $x_j$ is input data, $b_k$ is bias, $v_k$ is induced local field, $y_k$ is output of neuron.',\n",
       "  'A neuron consists of a synapse connecting link, an adder function or linear combiner and an activation function.\\n$$v = \\\\Sigma w_i \\\\cdot x_{i} + b$$,\\nwhere $x_i$ is the input, $w_i$ is the weight and $b$ is bias.',\n",
       "  'A neuron is a basic information processing unit that have a adder function to compute **weighted sum of inputs plus bias** and apply activation function on the result.\\n\\n$$ \\\\phi(v) = \\\\sum\\\\limits_{i=1}^n \\\\omega(i)x(i) + bias $$',\n",
       "  'Each neuron has a set of inputs and their respective weights.\\n\\nThe local field is,\\n\\n$v = \\\\sum(w_{ij} * x_i)$\\n\\nThe local field is passed through a activation function.\\n\\nSo the output of the neuron is,\\n\\n$y = \\\\phi(v)$\\n\\n$y = \\\\phi(\\\\sum(w_{ij} * x_i))$',\n",
       "  'The neurons are the basic processing units in neural network\\n\\noutput of the neuron = $  \\\\phi (\\\\sum w_{i} x_{i})$ \\n\\n\\nthey consist of three parts\\n\\nSynaptic weight: the connections between the neurons. characterised by weights\\n\\nAdder function: calculates the weighted sum of the inputs of the neuron\\n\\nActivation function: limits the amplitude of the output of the neuron. ($\\\\phi$)',\n",
       "  'The model of a neuron consists of synaptic weights which are applied to the input signals. The weighted inputs are then summed which gives the local field. This local field is put into an activation function whose output will be the output of the neuron.',\n",
       "  '$y = \\\\sum_{i=0} \\\\Phi(w*x_i)$ \\n\\nA neuron consists of inputs $x$, synpatic weights $w$, an extra input $w_0$ which is fixed to 1 for the bias, an Adder function, that creates the local field $v$ and a squashing function $\\\\Phi$.',\n",
       "  '$y = \\\\sum f(wx + b)$, where w are the weights, which change the input according to the learned weights, x is the input from the environment, b is the bias, which shifts the learned decision plane, and f() is the activation function, which limits the output to a desired region of values.',\n",
       "  'A neuron consists of one or multiple inputs which are gathered by a summation function. The hereby induced local field of the neuron is processed by a squashing function and generates the output of the neuron.',\n",
       "  'A neuron consist of input connection links with a synaptic weight, a bias, an adder which adds the input singnals and the bias and produces the local field. The local field is processed by the activation function and produces the output of the neuron. ',\n",
       "  'A neuron consists of input nodes x_1 to x_n and weights w_1 to w_n, a linear combiner v= SUM( $ x_i * w_i $) + b, where b is some bias. The result v is called local field and is used as input for an acivation function $ phi(v) $',\n",
       "  'Neuron is consists of three units. \\n\\n1. Synaptic links characterizex by weights which linearly ways the input.\\n2. Adder which adds weighted inputs to generate local field\\n3. Activation function which is nonlinear function sqashing the output of the neuron',\n",
       "  '1) Neuron is consist of sysnaptic  links which measured in terms of weights. neuron is given with inputs.\\\\\\n\\n2)it has adder funtion or combiner which adds all the inputs mulitplied by the weights and bias is extra input to the neuron as well.\\n\\n3) it has a activation link which limit the amplitude of the output of the neuron.\\n',\n",
       "  'Neuron is the basic information processing unit which is the main component of a neural network. A neuron is charecterized by its input ($x_i$), synaptic weight ($w_i$) and activation function $\\\\phi(v)$. Mathematically it can be modelled as $\\\\phi(w_ix_i)$. Activation function bounds the input to a certain level.',\n",
       "  'YOUR ANSWER HERE\\n\\nA neuron has three components\\n\\n* Synaptic weight: w\\n* Adder function: it multiplies input x with the weight\\n* Activation function: It squashes the output of the adder function. Sigmoid, hyperbolic tangent, Rectified linear unit etc.',\n",
       "  'Mehrere Eingänge werden in einer Summationseinheit aufaddiert. Die Summe wird als Eingabe für eine Aktivierungsfunktion verwendet. Derern Ergebnis wird an die nachfolgenden Neuronen weitergegeben.',\n",
       "  'A neuron consist of set of inputs that takes data from environment. Each neuron contains synapses(connection links) that are characterized by weights. All inputs are connected to the summing (adder) function, that computes weighted sum of all input values. This weighted sum is called local field of neuron. The value of this local field (V) is limited(squased) by an activation funtion $\\\\theta(V)$. The result from this squasing funtion is output of a neuron ($y = \\\\theta(V)$).  Additionally, a bias term $(b)$ is added to the input, and its value is always 0, but its associated weight is being changed over training period. Finally, output of neuron is $y = \\\\theta(V)$, where $V = \\\\sum W_j * X_j + b$',\n",
       "  '1. Label one class a positive with label +1 and the other class as negative with -1.\\n2. Augment the data with an additional value for the bias term.\\n3. Invert the sign of the data in the negative class.\\n4. Randomly initialize weights.\\n5. If $w^T \\\\cdot x <= 0$, update weight by $ w(n+1) = w(n) + \\\\eta x(n) $, else leave the weight unchanged.\\n6. Continue step 5.\\n7. terminate when there is no longer a change in any weight.',\n",
       "  '1. Initialization: n(time step or iteration) = 1 and weights are small but randomly initialized\\n2. Activation of perceptron:  Apply training pattern to activate the perceptron\\n3. Compute Output: Apply Activation function to the local field(weighted sum of inputs plus bias)\\n4. Adjust Weights: Adjust weight if current output(y) != desired output(d)\\n5. Continuation: We continue by increasing n during each iteration and repeat from step 2 untill all input pattern are applied to network and also error is minimized ',\n",
       "  'YOUR ANSWER HERE: \\ny denotes the actual result, d denotes the desired result\\npositive train error: y = 0, d=1 $w_{new} = w_{old} + x $\\nnegative train error: y = 1, d = 0 $w_{new} = w_{old} - x$',\n",
       "  'initialize weights with zero or small values;\\n\\nsample data point, feed into network;\\n\\ncompute net output, use the step activation function;\\n\\ncompute error $e=(d-o)$, where d is the true label, o is the predicted label;\\n\\ncorrect weights based on $w(t+1)=w(t)+\\\\alpha(d-o)x$, where alpha is the training rate and x is the input pattern;\\n\\nrepeat for each pattern until convergence is reached;',\n",
       "  'For this case, the parameters that need to be learned are the slope of the line and the intercept. These are the parameters for the weight vector.\\n\\n\\n1. Initialize random small values for weight vector.\\n2. For input_data $x_i$ in Training Data:\\n     - Apply the input to the weight vector.\\n     - e = the difference between the local field and the desired output $(d_i-y_i)$\\n     - Update weight: w(n+1) = w(n) + $\\\\eta e x_i$     ',\n",
       "  '$\\\\varphi(v) = \\\\tanh(v)$, single node network, $\\\\mu$ learning rate\\n\\nrepeat as long as error is too high:\\n\\n1. present sample to network and collect output.\\n2. compare actual output with desired output (d).\\n3. If not equal adapt weights: $w_i(n + 1) = w_i(n) + \\\\mu(d-y)x_i$',\n",
       "  'given $k$ date points $(x_i,y_i)$ and $y_i\\\\in\\\\{1,-1\\\\}$\\n\\ngiven a learning rate\\n\\nfor each point i\\n\\n    add a bias 1 so that point i == (1,x_i,y_i) ;\\n    \\nfor each point i there y_i == -1\\n\\n    point = -1 * point;\\n    \\nw= Nullvector;\\n\\nb = 0;\\n\\nconvergance = false;\\n\\nwhile(convergence == false)\\n\\n    convergance = true;\\n    \\n    for each point i in the training set:\\n\\n        if(w*x<=0) do\\n        \\n            w = w+learningrate*point_i; \\n            convergance = false;\\n    ',\n",
       "  'weights # a weight vector\\nphi = activation function\\neta = learning rate\\nfor each datapoint (x_i,y_i) do:\\n    weights[i] = weights[i] + eta * (x_i[i]-y_i)*weights[i]',\n",
       "  '1: w, b = init_weights_bias() // the weights can be initialized to 0 or random initialized\\n\\n2: n = 0\\n\\n3: WHILE !stop_criteria() DO // iteration until stop criteria is fulfilled\\n\\n4: y = w(n) * x(n) + b // calculate output\\n\\n5: IF x is in C1: e = 1 // if the x belongs to class C1, error i 1, otherwise is -1\\n\\n6: ELSE IF x is in C2: e = -1\\n\\n7: w = w + e * x // update weights using the calculated error\\n\\n8: n = n + 1\\n\\n9: END\\n\\nThe stop criteria can be, if the number of misclassified input data is 0, then stop.',\n",
       "  'The learning process consists of three main steps:\\n\\n1- Positive error:\\n    - calculate the error of all the data sets in the learning set \\n    - change the w(weight): w(n+1) = w(n)+positive error\\n    - seperate the data points based on the new w\\n2- Negative error:\\n    - calculate the error of all the data sets in the learning set \\n    - change the w(weight): w(n+1) = w(n)+negative error\\n    - seperate the data points based on the new w\\n3- No error: \\n    - when we have no error this is the end of the training',\n",
       "  'Define a bias in order to be able to trigger to which class data points will be classefied to. Assign initial randomly chosen weights, use a squashing function for example McCullon Pits, start training proccess and stop when error of output and desired output has reached desired percentage.',\n",
       "  'Initialize the weight vector $\\\\hat{w} = 0$\\n- do\\n-    for every training sample x,d\\n\\n         $v = \\\\sum w_i x_i + b$\\n         $y = \\\\phi(v)$\\n         \\n         if d is not equal to y then\\n             $e = d - y$\\n             $w = w + \\\\eta e(i) x_i$\\n-  until convergence\\n   ',\n",
       "  \"**Pseudo code**\\n+ initiate weights and bias randomly.\\n+ compute output for the given input data $ y' = \\\\sum_i (w_ix_i) +b$.\\n+ compute error between computed $y'$ and desired output $y$.\\n+ update weights: $w(n+1) = w(n) + \\\\eta (y-y') x$\\n+ stop when the error is below some specified threshold or becomes zero in case of data that is perfectly linearly separable.\",\n",
       "  'YOUR ANSWER HERE',\n",
       "  'Initialize the perceptron with each weight equal to 0: $w(0) = 0$.\\n\\nPresent the labeled examples $(x_i, d_i)$ to the perceptron.\\n> for each example $(x_i, d_i)$\\n>> Compute actual output $y_i$ and error signal\\n\\n>> Update weight based on the dlelta rule: $w(n+1) = w(n) + \\\\eta (d(n) - y(n)) x(n)$',\n",
       "  'We use threshold function as activation function. \\n \\n if w.x + b >= 1 \\n \\n label class 1.\\n \\n else label class 0.',\n",
       "  'e(n) = current error  <br>\\neps = convergence criteria  <br>\\nn = learning rate  <br>\\nwhile (change in e(n) not less then eps) {<br>\\n    calculate error e(n) <br>\\n    w(n+1) = w(n) + n e(n) x(n)  (Widrow Hoffmen rule) <br>\\n    }',\n",
       "  'YOUR ANSWER HERE: Perceptron learning algorithm:\\n- Initialize the network by assigning random weights to the synaptic links.\\n- Calculate error as the difference of the desired output with the actual output.\\n- If the input is misclassified with positive error, $w_(new)$ = $w_(current) + input$.\\n- If the input is misclassified with negative error, $w_(new)$ = $w_(current) - input$.\\n- If the input is correctly classified, no changes are made in the weights.\\n- Repeat from step 2 as long as the error is under some defined threshold value.',\n",
       "  'The linear binary classifiable data consists of input vector $X$ which when multipled with weights and added bias, fall into class+ or class- depending on the linear combination output of $WX + b$ being above 0(+) or below 0(class -).\\n\\nAlgo:\\nPara,meters : X,Y(desired output), W, b\\n\\n1) weight vector W is initialized with small random values.\\n\\n2) Input vector is chosen with a probabiity and output is computed using $WX + b$ . If the class Y of vector X is + and output is $<0$, or if the class of X is - and output if >0, then the weights are updated accordingly.\\nOtherwise weights are left unchanged.\\n\\n3) iterated over other input vectors until convergence of output.',\n",
       "  '1. Initialization : At time step  n(0), initialize weight vectors with random values $w_j(0)$\\n2. Activation : Apply the input example $(x_i(n),d_i(n))$ to activate the perceptron with heavyside step function as the activation function. \\n3. If output of the perceptron $y(n) \\\\neq d(n) $, adjust the weight vector using the rule : $w(n+1) = w(n) + \\\\eta x(n)(d(n) - y(n))$\\n4. Go to Activation and repeat until no more change in weight vector is observed',\n",
       "  '1. Inputs X: $x_1$, $x_2$, ... , $x_N$  \\n2. Desired outputs y: $y_1$, $y_2$, ... , $y_N$  \\n3. Initialize weight vector $w$ to random small values  \\n4. For each data point $x_n$ in X:  \\n     Calculate $\\\\hat{y}_n$ from $w$ and $x_n$  \\n     Calculate error $e_n = y_n - \\\\hat{y}_n$  \\n     Update $w$ according to delta rule    \\n   end\\n    ',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'apply input data to input layer and initialize small values weights\\n\\nminimize error according to difference between desired signal and output signal\\n\\nassign the test vector the class that has smallest error',\n",
       "  '1. Compute the initial weights for all input vector\\n2. Apply matrix multiplication from input to weight vector\\n3. Apply linear combiner\\n4. Apply activation function to produce the output\\n5. Compute the error\\n6. Update weights',\n",
       "  \"* Randomly assign values to initial weights\\n* Run the perceptron network and calculate the error (e = y-d) where, e is error, y is output and d is desired response.\\n* Update the weights based on the error.\\n* If error is positive, add the error with the input and update weight.\\n* If error is negative, subtract the error with the input and update weight.\\n* If there is no error, don't update the weights.\\n* Repeat the above process until the calulated error is approximately equal to zero.\",\n",
       "  'YOUR ANSWER HERE',\n",
       "  'w = [random number betrween -1 and 1]\\n\\n\\nfor every data in training set\\n\\n\\n{\\n In the first layer: \\n \\n calculate the weighted sum  using adder function\\n \\n calculate the output of the activation function\\n \\n In the ouput layer\\n \\n calculate the output y  \\n \\n calculate the error e = d - y ; d- desired output\\n \\n \\n change the weights using the formula $ \\\\delta w = \\\\eta x_j e_j$\\n\\n} \\n\\ncontinue till the error converges ',\n",
       "  '    Initialize as many random weights as the dimension of the data points\\n\\n    For each data point:\\n        if the output matches the desired output\\n            do nothing\\n        else:\\n            change the weights in the direction of the datapoint so that the datapoint is classified correctly\\n        end if\\n    end for\\n\\n    if some weight was changed:\\n        start again with the for loop\\n    end if',\n",
       "  '1. Initialize the weights at random or as 0.\\n2. Activate the Perceptron by giving an example.\\n3. Compute the actual output of the neuron.\\n4. Adjust the parameters of there perceptron.\\n5. Continue until convergence is achieved.\\n\\nw = rand\\n\\ny = sum($\\\\Phi$(w*x))\\n\\nfor w_i in w:\\n\\n    w_i = w_i+$\\\\eta$*e*y ',\n",
       "  'Initialize the weights randomly.\\n\\n$y = \\\\sum f(wx + b)$, compute the output of the perceptron using the input x, the weight w, the bias b and the activation function f().\\n\\n$e = d - y$, calculate the error by substracting the actual output from the desired output.\\n\\n$w_{new} = w_{old} + learning\\\\_rate \\\\cdot x \\\\cdot e$, update the weights with this formula. The learning rate is a parameter which changes how fast the perceptron learns.',\n",
       "  'for n iterations\\n\\n    for each datapoint d\\n \\n        error = desired - output\\n        \\n        if error > 0\\n        \\n            weights = weights - error\\n            \\n        if error < 0\\n        \\n            weights = weights + error',\n",
       "  'pick random decision boundary\\nwhile one of data points is in wrong class\\n    turn decision boundary by using vector of wrong data point\\n    (negative rule or positive)',\n",
       "  'training_set := set of labeled linear seperable data points  \\nw := weight vector with dimension of input data  \\nv := local field  \\nphi(v): activation_function (threshold function)  \\ny:= output  \\ne := error (y - d) where d is the desired output from labeled training data  \\nn := learning rate (0.1)  \\n  \\nassign random values for w  \\n  \\nfor x in training_set:  \\n    v = sum(x_i * w_i)  \\n    y = phi(v)   \\n    e = y - d  \\n    w = w + n*x*e // delta rule  \\nend     \\n',\n",
       "  'initialize weights w and bias b\\n\\nset learning rate n\\n\\nset error_threshold (upper bound on error)\\n\\nwhile error < error_threshold :\\n\\n    for every datapoint x in tarining dataset :\\n        y = [w, b] . [x, 1]       (bias is represented as weight of fixed input 1)\\n        if y is positive then x belongs to C1 otherwise to C2\\n        store above predicted class.\\n        find error in predicted output with respect to the labels\\n        store error e\\n    e_sum = sum of all errors e for every data point \\n    w = w + n * e_sum\\n    \\n  ',\n",
       "  'for a binary classifier we can use threshold activation funtion.\\n\\n1) randomly initilize the weights \\n\\n2) you calculte the output of the neruon \\n\\n3) find out the error by subtracting expected output and current output.\\n\\n4) modify the weights related to that input with respect to the error.\\n\\n5)repeat the process 2-4 till the you get minimal error.\\n',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'YOUR ANSWER HERE\\n\\ncontinue_process = true\\n\\nw = randomly_initialize()\\n\\nwhile continue_process\\n\\n  for x in list of points\\n  \\n    y = w.x\\n    \\n    diff = d-y  // d is  the desired output\\n    \\n    if(diff >= 0)\\n    \\n      w = w + x\\n      \\n    else\\n    \\n      w = w - x\\n      \\n      \\n  if all points are classified without error\\n  \\n    continue_process = false',\n",
       "  'Start: Gewicht zufällig wählen.\\nüber alle datensätze iterrieren\\n    datenpunkt klassifizieren und ergebniss überprüfen\\n    fals falsch klassifiziert, gewicht soweit anpassen, das dieser Punkt korrekt klassifiziert wird, in die\\n        richtung in die eine kleinere korrektur notwendig ist\\nwenn alle datenpunkte korrekt klassifiziert wurden terminiere\\n',\n",
       "  'n<- learinig rate\\n\\nRepeat until the MSE is small enough:\\n\\nt=t+1\\n\\n    for each point in training set do:\\n\\n        compute local vield of percepron: V = W*X\\n\\n        apply linear activation function: $y =  \\\\theta(V) = V$\\n\\n        compute current error: e = (d-y)\\n        \\n        \\n        apply delta rule: W(t+1) = W(t) + n*e*X \\n\\n    end',\n",
       "  'Classification:\\n\\nIn classification, the output produced by the NN is a discrete value which indicates which class the input belongs to.\\n\\nRegression:\\n\\nIn regression, the output produced by the NN is a continuous variable. This could be used for instance, to approximate a continuous function.',\n",
       "  'In classification, output values are always discrete.\\n\\n\\nIn regression, output values are continuous',\n",
       "  'YOUR ANSWER HERE\\nA hyerplane is given by y = w*x + b . Regression wants to determine w\\n\\nClassification wants to assign a class to a set of observations.\\n\\nRegression wants to determine separating hyerplane, classification wants to label data points with a class',\n",
       "  'In classification tasks, we assign discrete labels to data points of our training dataset, either being assigned a specific label or not (binary). For supervised learning, these datapoints are labeled with a label vector ground truth. In regression, we try to model a function which fits the data points of the training data, and thus model a function with continous values.',\n",
       "  '_Classification_: \\n   - It refers to classifying given data into discrete classes.\\n   - The output is discrete values.\\n   - Use for activity like pattern recognition, etc.\\n\\n_Regression_: \\n   - It refers to estimating the value of some continuous function given an input.\\n   - The output is continuous value.\\n   - used for activities like motor control, etc.',\n",
       "  'In classification we try to assign classes to input data. Regression we want the network to behave like a given system/formala. This can also be a time series of input and output data.',\n",
       "  'In classification the goal is to saperates points into different classes. The outcome is a class lable. \\n\\nRegression trys to fit a hyperplante to a point cloud best, so that future data is representet by that hyperplane best (LMS). It trys to minimize the distance to all data points. The outcome is a countinius variable.',\n",
       "  'Both are learning tasks of a ANN. \\nIn classification the goal is to assign a class label to new datapoints.\\n\\nIn regression the goal is to eastimate a unkown function.\\n\\nThe only difference between both is that classification uses discrete class labels, while in  regression a continuous output is used',\n",
       "  'The approach of classification is to classify sets of input data into their correct classes (for example, used in pattern recognition). The approach of regression is to approximize to a defined function f by calculating the error between this function and the result of an algorithm. THe difference is that, the classification approach is applied to a discret data (the samples are the different points of the input space), and regression is an analogic approach where the whole function must be approximize (for any input given).',\n",
       "  '- Classification: In classification problems we have different groups of data that have some common properties and after training we want that our model can detect the class of the new sample correctly\\n- Regression: In regression we have a series of values and we want to use the previuos values in this series and predict the next value',\n",
       "  'Classification is a problem of destinguishing to which discrete classes input variables are to be assigned to, regression is estimation of the output, by figuring out the continuous trend of the whole dataset.',\n",
       "  'Classification if to assign a class or category to the data, while regression is when you fit the data to a function.',\n",
       "  '+ Regression: learns model/function that can predict other unseen data well. Target/output is real spaced.\\n+ Classification: learns a model that classifies/maps input to a discrete target label. Targetlabel/output is binary/discrete.',\n",
       "  'Classification describes the application, in which a sample is assigned to one specific pattern of the problem. In comparison to regression is the output deterministic an not continiously. In regression the output is continuous describing ',\n",
       "  'Classification is the task of classifying the input signals into a finite number of groups, so the output is a number that indicates a certain class. Regression is the task of approximating a function by estimating the values given the input signals, so the output can be any real number.',\n",
       "  'Classification: \\nWe need to predict the output data discretely. That is the output space is a discrete space. \\n\\nRegression:\\nWe need to predict the output data continuously. That is the output space is continuous space\\n\\nThe main difference is the discreteness and contionousness.',\n",
       "  'Classification is a problem of assigning a particular class to each data point in a given dataset. <br>\\nRegression is a problem of fitting the given dataset on a particular hyperplane which can be used for representing the given data. It finds the hyperplane which minimises the mean square error. ',\n",
       "  'YOUR ANSWER HERE:\\n- Classification is a problem of assigning labels or classes to the input. The output is a discrete variable.\\n- Regression is a problem of assigning a continuous variable to the input.',\n",
       "  '\\nClassification is a problem of catergorization into discrete classes where as regression is a problem in a continuous space where the goal is to ether minimize or maximize a cost function.\\n\\nClassification is the process of dividing a set of discrete inputs into classes corresponding to similar patterns such as clustering.\\nRegression could be finding a pattern of the distribution of the data such as ftting a line.',\n",
       "  'Classification in machine learning is used to find a decision surface in the form of a hyperplane that can separate a set of input examples (or set of patterns) into their respective classes. Regression on the other hand is used to find the parameters (i.e, the weight vector $w$ and the bias b) for the function thatcan best fit the given data points $\\\\{x_i,d_i\\\\}$ . Thus classification deals with predicting the class label for discrete data points whereas regression deals with fitting a continuous real valued function.',\n",
       "  'Classification is separating the data into classes and the output is a discontinuous variable. Regression is fitting a model and the output is a continuous variable.',\n",
       "  \"Classification is about classifying the given data into different classes, where as regresssion is about finding the local/global minima.We use perceptrons to classify the data and we use unconstrained optimization techniques like newton's method to find regression.\",\n",
       "  'classification: assign a test data to a class that is prescribed\\n\\nregression: approximating an unknown function with minimization errors for input-output mapping. ',\n",
       "  'Classsification: In classification, the output variable takes class labels or identifying group membership<br>\\nRegression: In regression, the output variable takes continuous values or predicting a response',\n",
       "  'Classifaction problem is used to classify set of data points into specific groups.\\n\\nRegression is used to predict time series data.\\n\\nClassification works on discreate set of values and regression works on continuous values.',\n",
       "  'Classification: Classification is done between the classes. The machine determines to what class the data belongs to.\\n\\nRegression: Regression is a expecting output for an input. The machine learns from the given data and models a function and when new input is given it expects the output.\\n\\nDifference: Classification is discrete output where as Regression is a continuous output.',\n",
       "  'Regression: \\n\\nTries to fit a line are curve among the given points\\n\\nThe have continuous output\\n\\nthe output is a function\\n\\nClassification: \\n\\nTries to classify the given points into two or more calsses\\n\\nThey have a discrete output\\n\\nthe output is a value representing the class',\n",
       "  'Classification: Each datapoint is assigned with a class\\n\\nRegression: Each datapoint is assigned with a value\\n\\nIn classification we assign classes or labels to datapoints. The error signal here can be only true or false. In regression we try to learn a function, the error for each prediction can be a number.',\n",
       "  'In classification a binary pattern has to be partitioned into the two classes. In regression a line has to be fitted closest to some datapoints. The difference is, that in Classification mthe output is a single class label, while in regression the output is continuous',\n",
       "  'In classification the input data is split in 2 or more classes. The goal of the neural network is to learn the input data and then be able to classify new input data into the classes. Based on the learned information the network then maps input data into one of the classes, which is discrete space.\\n\\nIn regression the input data is learned aswell. But here the network tries to predict feature values, which are in continuous space. The network tries to predict close as possible to new input data only using the learned model.',\n",
       "  'Classification tries to label discrete data points with distinct classes, while regression tries to approximate a continuous function from discrete data points. Results of these methods are respectively a labeled data set or a continuous function.',\n",
       "  'In classification the task is to give an discrete output value to an input. It assignes one of all defined classes to the current input. Regression try to approximate a function while minimizing error and produces a continous output value. ',\n",
       "  'Classification means mapping inptut data a class label, for example 1 and -1. IN regression on the other hand a continuous function is learned in way that f(x) - F(x) is minimized, where f(x) is the function learned by a learning machine and F(x) is the original function.  ',\n",
       "  'Classification is supervised learning where underlying function  representing the trining data is learn from training data to predict classes of datapoints or patterns drawn from similar distribution as of tarining data. Weights of the neural network are learned to minimize the error in classification. \\n\\nRegression is supervised learning algorithm where underlying function  representing the trining data is learn from training data to predict the value of label or output of some system  for new datapoint or pattern of similar type. Weights of the neural network are learned to minimize the error in prediction of function.\\n\\n\\nDifferences. :\\n1. Output of classification is discrete ( Class 1,2,3 ) whereas output of regression is continuous \\n2. Error in classification is number of wrong classifications whereas Error in classification regression is distance between lable value and predicted value\\n',\n",
       "  'classification is type of problem where algorithm needs to saperate the one data class from the another data class. \\nIf there is 2 classes C1 , C2 . algorithm classify the given data into these two classes. \\nit is discreet process.\\n\\nRegression is the pridicting the next point depending on the previous points.\\nit is continuous process.',\n",
       "  'Classification is the problem where the input data has to be put in two or more classes distinctively different from each other. For example in case of binary classification on class can be -1 and the other +1\\n\\nRegression on the other hand is data fitting. THe main aim is to find a hyperplane which can fit a given input pattern.',\n",
       "  'YOUR ANSWER HERE\\n\\nClassification: Is a task to partition the given input into one of several classes. The calsses are descrete values.\\n\\nRegression: Regression is the tasks of predicting output in a continuous range. The prediction can be any value within a range.',\n",
       "  'Classification: Input einer assoziierten Klasse zuordnen. Regression: aus einer Menge von Daten, eine Funktion herleiten, die diese Daten mit minimalen Fehler beschreibt und somit für weitere Eingabe Daten vorhersagen über die Ausgabe machen kann.',\n",
       "  'In classification task the aim to separate data in  different classes, such that output of NN gives value of class index for each input point. E.g in the task is to classify binary data, then the output of the NN will 0 or 1, and each value, represent on class.\\n\\nIn case of regression task, the aim is to fit data, namely a function that perform input-ouput mapping. Output of NN in this case, will be error value, such that we know how close is out function fitted to data points.',\n",
       "  '1. Arrange the weights in the required topology according to the problem.\\n2. Initialize the weights randomly such that all the weights are different.\\n3. Sample the input from the input space.\\n4. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron.\\n5. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and reduce the learning rate and make sure learning rate is above zero.\\n6. If ordering and convergence is complete, stop. Else continue to step 3.',\n",
       "  'i. First we initialize random weights for neurons\\n\\nii.  Then we choose random input from input space\\n\\niii. We compute distance between input vector and each weight vector. \\n\\niv. Neuron that have minimium euclidean distance with input vector is considered as winner neuron\\n\\nv. Then, we find the neighborhood neurons of the winning neuron\\n\\nvi. We adjust the weights of all neighborhood neurons\\n\\nvii. Reduce the learning parameter and neighborhood size\\n\\nviii. Continue until it converges.',\n",
       "  'YOUR ANSWER HERE\\nw denote weights\\nt denotes threshold\\nh denotes the neighborhood function, which decreases with distance d from winning neuron\\n\\nh(x, x_{win} //neighborhood function\\n return (exp(-2/||x-x_{win}||)\\n\\nw = rand(); //initialize weights with random value\\nwhile (w_{delta} > t){ //proceed until there are no notieable changes\\n    x_{win} = arg min ||x-w||^{2}//determine x which is closest to w (competetive learing) \\n    // Update weights of winning neuron\\n    // weights of losing neurons are not updated\\n    w_{new} = w_{old} + x*h(x, x_{win})*(x-w)//update weights of neuron which a are in neighborhood of winning neuron\\n    \\n    \\n}\\n\\n    ',\n",
       "  \"initialize weights with small values (such that all of the weight vectors are different);\\n\\nsample a datapoint, feed into network;\\n\\ndetermine the winning neuron on the lattice, picking the neuron with the least euclidean distance of its weight vector to the input vector;\\n\\ndetermine the neighbourhood of the winning neuron through the neighbourhood function;\\n\\nchange weights of the neurons, namely spatially 'pulling' the weight vectors of the neighbourhood neurons towards the input vector;\\n\\ndepending on the timestep, reduce learning rate and neighbourhood size based on wether we are in the organizing or finetuning step;\\n\\nrepeat until maximum number of steps;\",\n",
       "  '1. Randomly initialize weights.\\n2. Randomly select an input from the training data.\\n3. Find the nearest neighbour of this input in the weights. This is done by finding the euclidean distance of the input from each weight. And selecting the weight with least distance.\\n4. Update the weights of all the neurons within the neighbourhood $h(n)$ (which is gaussian function, with an exponentially decaying $\\\\sigma(n)$) of the winning neuron with some learning rate $\\\\eta(n)$.\\n    $$\\\\Delta w_{ij}=\\\\eta(n)h(n)(||x_i-x_j||)$$\\n  where,\\n  $$\\\\eta(n)= \\\\eta_0e^{-n/T_1}$$ and $$\\\\sigma(n)= \\\\sigma_0e^{-n/T_1}$$',\n",
       "  'In SOM we start with randomized weights, $\\\\mu$ learning reate, $d_{ji}$ distance between j and i, $h$ neighbour function\\n\\nrepeat as long as error is too high/max iterations are not reached:\\n\\n1. take input sample\\n2. find closest node/weight\\n3. find all it neighbours\\n4. move the weight and its neighbours closer to the given input, use the neighbour function (e.g. gaussian) to reduce effect to far distance neighbours\\n5. (optional) adapt learning rate and neighbour function',\n",
       "  'given a neigbourhood function $h_{ij}(n)$ and a lerning rate over time\\n\\nrandomly assing different weights from the input layer to the neurons in the second layer\\n\\nfor each training point x_i do:\\n\\n- find the winner-takes-all neuron $k$ with $min ||x_i-w_i||$\\n- find the neighbours of $k$ with the neigbourhood function\\n- compute the new weights for those neurons using the neighbourhood function and the learning rate\\n- update (decrease) the neighbourhood function and the learning rate\\nend',\n",
       "  'YOUR ANSWER HERE',\n",
       "  '1: w = init_weights() // equal to zero or random initialized\\n\\n2: n = 0\\n\\n3: WHILE !stop_criteria() \\n\\n4: winner_neuron, y = (x, w) // find on the map layer which neuron is closer to the input (euclidean distance)\\n\\n5: neighborhood = define_neighboor(winner_neuron, n) // define the neighborhood size (first iterations big, and being reduced)\\n\\n6: eta = define_learning_rate(n) // define the learning rate (large value at the first iterations and being reduced)\\n\\n7: diff_w = adapt_weights(neighborhood, eta) // adapt the weights just for the winner neuron and its neighborhood\\n\\n8: w = w + diff_w // update the weights\\n\\n9: stop_critera = must_stop(y, x) // look if the distance between input and the winner neuron is 0 (or really close to 0)\\n\\n9: END',\n",
       "  '- randomly define some values for the synapitc connections in the network\\n- send the first input to the network\\n- in the output layer(map layer) select the neuron that has lowest error(competition phase)\\n- based on a predefined method define the neighborhood of the selected neuron(cooparation phase)\\n- change the weights of the selected neuron and the neurons located in its neighborhood(adaptation phase)\\n- if the stop condition satisfied stop the process',\n",
       "  'Has three parts in it - Competition, Cooperation, Adaptation\\n\\nget input variable and choose amount of neurons to be more than amount of variables\\n\\nthen run competition, where from the input neurons will be compiting to each other on choosing which fits the most\\nafter finding winning neuron change weight of neighbouring neuron only\\n\\nin cooperation weights of neighbouring neurons are adjusted to clusters\\n\\nin adaptation neurons are pulled to input variables to establish the classification',\n",
       "  '\\n- Find the winning neuron\\n- Find the neighbors of the winning neuron.',\n",
       "  '**Pseudo code**\\n+ 1. Initialize map neurons, based on topology it could be a lattice, on a circle, etc.\\n+ 2. competition: find map neuron that is closest to an input neuron by computing distances $d$.\\n+ 3. update the position of closest map neuron with update rule.\\n+ 4. Do 2 and 3 until all input neurons are assigned a map neuron.\\n+ Do 2,3 and 4 until specified iterations or the net cumulative distance goes below some specified value or becomes zero.',\n",
       "  'Produce train_SOM;\\n\\nbegin:\\n\\n    randomize weights for all neurons;\\n    \\n    for (i=1 to iteration_number) do:\\n    \\n        begin:\\n        \\n            take random input pattern;\\n            \\n            find the winning neuron;\\n            \\n            find neighbors of the winner;\\n            \\n            modify synaptic weights of these neurons;\\n            \\n            reduce learning rate and lambda;\\n            \\n        end;\\n        \\nend;',\n",
       "  'Initialize the network with small and random weights.\\n\\nSample the data set by  picking an input randomly.\\n\\n> Determine the winning neuron based on the output value.\\n\\n> Determine the coopertaing neurons based using the neighborhood function.\\n\\n> Update the weights of the cooperating neurons.\\n\\n> Adjust the learning rate.\\n\\n> Stop if the network converges.',\n",
       "  'Begin\\n n = range of data set\\n \\n Initialise the weights. #We give a small random weights. \\n \\n for the range of n:\\n \\n     Select a input signal, \\n \\n     Find the winning neuron based on the similarity between the weights. \\n \\n     Update the weights of the neighboring neuron\\n \\n     Repeat until the convergence.\\n     \\n     \\n1. initalising \\n2. Sampling\\n3 Similarity matching.\\n4. Updating the weights\\n5. continuation. ',\n",
       "  'intialise weights <br>\\nwhile( significant change is observed in topographic pattern) {<br>\\ntake a random input (sampling) <br>\\nfind the winning output neuron     (competition)  <br>\\nadjust the weights of the winning neuron and its neighbourhood neurons (cooperation) <br>\\ncontinue <br>\\n}',\n",
       "  \"YOUR ANSWER HERE: SOM learning\\n - Initialization with random small weights.\\n - Sampling: Picking a input pattern with certain probability.\\n - Similarity matching: Finding the most matching neuron i.e., the winning neuron.\\n - Synaptic updation: Updating the weights of the neuron and also the neurons in it's neighbourhood.\\n - Continuation: Repeat steps 2 to 4 till there is no considerable change in the map.\",\n",
       "  'Parameters: $X$ data vectors, $W$ weights vectors in lattice , $\\\\eta(n)$ -learning rate, $\\\\sigma(n)$ - neighbourhood width, $h_{ji(x)}$- neighbourhood function \\n\\nalgo:\\n\\n1) Initialize the weights to a small, random , non-repeatible values.\\n\\n2) Sample a data vector with a probabiity \\n\\n3) Compute the euclidean distance to weight vectors from the data points and find the winning neuron with minimum distance .\\n\\n4) Update the weights of the winning neuron and its neighbourhood towards the input direction using neighbourhood function.\\n\\n5) reduce the learning rate and the neighbour hood width and iterate from step 2 until no significant changes between weight vectors and inputs are seen.\\n',\n",
       "  '1. Initialization : Initialize the weight vectors with random values such that the $w_j(0)$ is different for all weights.\\n2. Sampling : Draw sample example $x$ from input space.\\n3. Similarity matching : Find the best matching weight vector for the input vector : $W_i = argmin_i  (x - W_i(n))$\\n4. Adjust the weight vectors of neurons in the neighbourhood of the winning neuron \\n5. Go to Sampling step and repeat until no more changes are observed in the local neighbourhood of the winning neuron.',\n",
       "  '1. Initailization: Initialize the weights of each neuron to small random values such that weight of each neuron is different.\\n2. Sampling: Sample an input from the input set\\n3. Similarity matching: Determine the neuron nearest to the sampled input based on its distance\\n4. Weight updation: Update the weights of the neighbouring neurons chosen by the neighbourhood function $h_{ij}(n)$\\n5. Continuation: Continue from sampling until there is no more change in the weights',\n",
       "  'SOM is refered to as Self organized maps which is an unsupervised training algorithm for finding spatial pattern in data without using any external help.The process in SOM is explained below:\\n-Initialization: Initialize random weights w_j for input patterns\\n- Sampling: Take n_th random sample from the input (say x)\\n- similarity matching :for the input x, find the best match in the weight vector.\\n  $i(x) = argmin(x - w)$\\n- update: the next step is to update the weights\\n$w(n+1) = w(n) + eta*h_ji(x)*i(x)$\\n- continuation : continue from sampling until there is no significant change in the feature map',\n",
       "  'Initialization: set random small values to weights wj is different for each neuron\\n\\nSampling: draw n-th sample x from input space\\n\\nCompetition: identify winning neuron i using arg min $||x-w_i||$ which means weight vector of i is most similar to input \\n\\nCooperation: identify neighbors of winning neuron i using neighborhood function $h_{j,i(x)} (n)$ which shrinks with time\\n\\nWeight adaptation: adjustments made to synaptic weights of winning neuron and its neighbors\\n\\ngo to sampling until no large changes in the feature map.',\n",
       "  '\\nGENERATE random weights for all neurons<br>\\nFOR i to max_iteration DO<br>\\n    ------TAKE random input pattern<br>\\n    ------FIND the winning neuron<br>\\n    ------FIND the neighbors of the winning neuron<br>\\n    ------COMPUTE weigths of these neurons<br>\\n    ------REDUCE $\\\\eta$ and $\\\\lambda$<br>\\nEND FOR',\n",
       "  '* Initialize the neuron weights randomly in a way that all neurons have different weights.\\n* Generate random samples x from the input space.\\n* Iterate the samples and **Compare distance between current input and all neurons** in the weight space.\\n* **Find a winning neuron** with shortest distance from current input.\\n* Distance is calculated using **euclidean or manhatten distance**.\\n* Find the neurons in the neighborhood boundary of winning neuron.\\n* **Update the weights** of neighborhood neurons using delta rule.\\n* Adapt the size of neighborhood $(\\\\lambda)$ and learning rate $(\\\\eta)$ at each iteration\\n* Repeat the process until there is no neurons in the neighborhood boundary or all the inputs moved to some neuron.',\n",
       "  'Step1: It selects a datapoint in random through sampling.\\n\\nStep2: Finds the nearest neuron through competitive learning.\\n\\nStep3: Updates the weight of the winner neuron and updates the weight of neighbouring neurons by a fraction.\\n\\nStep4: Continues steps 1, 2, 3 until there is no change in the weights or some stopping criteria is met.',\n",
       "  '\\n{\\ntake a rondom point from the training data\\n\\ncompetitive phase: find the winning neuron - the neuron similar feature, using the eucledian distance formula\\n\\ncooperative phase: find the neighbors of the winning neuron based on the neighbor function (eg: gaussian function)\\n\\nadaption phase: change the weights of the all the neighboring neuron of the winning node using the formula $ \\\\del w = \\\\eta x_j - w_j $\\n\\n}\\n\\n',\n",
       "  '    input: distance function d(x, y), learning rate mu, neighborhood distance n\\n    \\n    Initialize the map layer with random weights\\n    for each input:\\n        find the weight which is closest to the input (minimum d(x, y))\\n        change the weight in the direction of the input depending on the learning rate\\n        change all weights which are within the neighborhood distance n depending on their distance and the learning rate\\n        reduce learning rate and neighborhood distance',\n",
       "  '1. Initialize small random weights.\\n2. Draw the nth sample from the input space.\\n3. Similarity matching: Determine the winning neuron.\\n4. Update the weights of the neuron an the topological neighborhood.\\n5. Repeat steps 2-4 \\n\\nw = random\\n\\nn = example.draw()\\n\\nw_max = get_min(w_i*n)\\n\\nh_n = get_neighborhood(w_max)\\n\\nfor w_i in h_n:\\n\\n    w_i = w_i+$\\\\eta$*h*y ',\n",
       "  'Initialize the weights randomly. Create a term T1 and T2, which decrease the learning\\\\_rate and neighbourhood function respectively.\\n\\nCalculate $i(x) = argmin | w - x |$, the weight which is closest to the input data received.\\n\\ni(x) is the neuron, which wins the competetive process, this neuron and its neighbours weights are updated using $w_{new} = w_{old} - learning\\\\_rate \\\\cdot h(x) \\\\cdot (w - x)$. h(x) is the neighbourhood function which determines, which neurons are updated and how strong they are changed by the update. It is defined using the distance between the neurons.\\n\\nThe learning\\\\_rate is updated using $learning\\\\_rate / T1$, also is the neighbourhood function updated in the same way using T2. The learning\\\\_rate cannot get lower than 0.01, while the neighbourhood function can get as low as only the winning neuron. So in the beginning almost every neuron is updated and at the end only a small neighbourhood or the neuron itself is updated.',\n",
       "  'for n iterations\\n\\n    winner = competition_between_neurons()\\n    \\n    neighbourhood = cooperation_with_neighbourhood_function(winner)\\n    \\n    update_weights(neighbourhood)',\n",
       "  'given a map layer \\nset random small values for weights from input to map layer\\nrepeat until not converged:\\n    find best match of input value and weight of the neurons (competitive process)\\n    adapt (increase) weight of winning neuron and neighboorhood (with gauss function and neighboorhood size) (cooperating process and weight adjustment)\\n    decrease neighboorhood size ',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'Initialize weight vectors of hidden neurons with same dimmension as of data.\\n\\nNumber of hidden neuron should be signifiacntly greater than number of data points.\\n\\ninitialize learning rate n and neghbouring function h\\n\\nwhile (rate of change in weights is significant):\\n\\n    for every datapoint:\\n\\n        calculate distance of each neuron from data.\\n        select winner neuron w with minimum distance (maximum similarity)\\n        error = distnce of winner form datapoint\\n        adjust weights of neurons with the rule w = w + n*h*error\\n        ',\n",
       "  'randomly inilize the weights\\n\\ndraw sample of inputs \\n\\nIncrease the weights of the local neihburhood of winning neuron\\n\\nrepeat the process above process till there is only one winning neuron',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'YOUR ANSWER HERE\\n\\nfor i in num_of_epochs\\n\\n  for p in input_points\\n  \\n    find the winning neuron\\n    find the neighbours of the winning neuron within distance sigma\\n    update winning neuron and neighbours weight\\n    update sigma and learning_rate so that both reduces over time',\n",
       "  'Input x:\\nFinde neuron winner mit ähnlichstem Gewicht zu x respektive Euclidischer Norm.\\nBestimme Nachbar Neuronen von winner mit nachbarf()\\nÄndere Gewichte in richtung x von winner und nachbarneuronen mit changew()\\nvermindere parameter, für nachbarf() der angbit in welcher reichweite ein neuron noch ein nachbar ist(exponentielle funktion)\\nvermindere lernParam der angibt wie startk die gewichte verändert werden\\nRepeat until Convergenz erreicht\\n\\nchangew() ändert die gewichte in abhängigkeit der distanz des neurons zum winner (exponentielle funktion) je weiter weg desto kleiner die änderung, und in abhängigkeit des lernParam, sodass mit der zeit immer kleinere veränderungen durchgeführt werden',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'A support vector machine is a maximum margin classifier in which the width of the boundary of separation is maximized. A margin is defined as the width of the boundary before hitting a point. \\n\\nThis maximum margin intuitively feels safe, and is experimentally good.',\n",
       "  'Basic idea of SVM is to best segregate the data into two classes with the help of decision boundary. This decision boundary is margin, we always try to maximize the margin to make sure data is classified correctly',\n",
       "  'YOUR ANSWER HERE Support Vector Machines goal is to maximize margin between closest data points of separating hyperplane. Separating hyperplane is given by: 0 = w(n)*x(n) + b. By maximizing margin probability of classification errors is reduced. ',\n",
       "  'An SVM is a binary, linear classifier spanning a seperating hyperplane between two classes of datapoints. The hyperplane is spanned between both the positive and negative decision boundaries, and supported by a number of support vectors. Support vectors are the outermost datapoints which span the hyperplane. During training, the distance of falsely classified data points to their correct side of the hyperplane is minimized, utilizing a quadratic programming formulation. ',\n",
       "  'A SVM is a binary classifier with a maximum width boundary separating the two classes. This uses support vectors (vectors that pushes against the boundaries).\\n\\nThe equations of the lines in an SVM are:\\n   - $wx+b>=1$:for class 1\\n   - $wx+b<=-1$:for class -1\\n   - M is the width between these boundaries.',\n",
       "  'Because SVMs are binary classifiers we can use a border to sperate the data. The border is typically placed where it has the largest possible distance to both classes. The vectors the border touches on both sides with its margin are the support vectors.',\n",
       "  'A SVM is an ANN for supervised learning, whicht is able to saperate two classes of data-points by using a hyperlane found by quadratic programming, by finding the biggest margin. The goal is to classify future data in there two classes.',\n",
       "  'The SVM is a maximum margin classifier. It is used the binary classify datapoints in a dichotomy. The idea is to find a line wich linearly seperates both classes. There perfect position of this line is in right in the middle of these classes. To find this line(descision boundary) we define a positive and a negative boundary which are parallel to this line. The boundarys define the margin between both classes. The idea of SVM is that the datapoints which are next to the boundary can be used to define the margin. They are called support vectors. \\nAddittionaly not every problem is linearly seperable so the idea was to transform the input into many higher dimensions using some kernel functions. We discussed the kernel function of polynomial terms and found out that it easy to compute. ',\n",
       "  'Support Vector Machines are a type of learning machines that try to classify different classes of an input space. For linear separable classes, the SVMs try to calculate the line that separates this two classes with maximum margin. The support vectors will be the points closer to this margin. When the input data is noisy, we have an optimization problem of two aspects (maximum margin, proper classification). So, a trade-off (C) will be defined. The trade-off will be calculated by the sum of the distance of misclassified points.\\n\\nFor non-linear separable classes, a kernel will be defined that will transform the input data into a higher dimensional space.',\n",
       "  'In linear SVM we have a linear border line classifier that seperate two different classes(positive and negative planes) and we calculate the distance of the data points from this border line classifier. Also a margin will be defined and this margin will be maximized until it touches some data points in the plane. The data points that the margin pushed agains them will be our support vectors. The error for the wrongly classified datapoints will be calculated by calculating the distance of the data point from its correct plane. The SVM tries to learn the classifier and the margin from the training data. ',\n",
       "  'Support vector machines are classifiers that are using support vectors, which are variables of the dataset. These variable are chosen during learning algorithm. Main advantage of SVMs is that it will not be overfitting by choosing correct margin. Activation functions can be both linear and nonlinear. Output of SVM is always TRUE or FALSE for given variable.',\n",
       "  'Support vector machines are a type of neural network that build a desicion boundary around classes such that the margin of separation between classes is maximized. ',\n",
       "  'SVMs are binary classifier. They learn the classification by memorizing the marginal data points (called support vectors) that make up the decision boundaries (2 : positive and negative).',\n",
       "  'The abbreviation SVM stands for Suppor Vector Machine. SVMs represent a feedforward category of NN. SVMs are binary learning machines whose functionality can be summarized for classification problem as follows:\\nGiven a training sample, the SVM constructs a hyperplane as the decision surface in such a way that the margin of seperation between positive and negative examples is maximized.\\n\\nOne key innovation associated with SVMs is the kernel trick. The kernel trick consists of observing that many machine learning algorithms can be written exclusively in terms of dot products between examples. It allows us to learn models that are nonlinear as a function of x using convex optimization techniques that are guaranteed to converge efficiently. Besides, the kernel function k often admits an implementation that is significantly more computatinal efficient than naively constructing two vectors and explicetly taking their dot product.',\n",
       "  'An SVM, or support vector machine, is a feedforward network with a hidden layer to learn a task in a supervised learning manner. The network tries to construct a hyperplane that separates the data points of two different classes by maximizing the margin of separation, which is the distance from the hyperplane to the closest data points called support vectors. ',\n",
       "  'Given a dataset, support vector machines builds a hyperplane in a such a way that positive and negative samples are seperated to the maximum distance. Width of the margin should be maximum\\n\\nThe vectors to which the margins(margin for positive and negative sample) are pushed on to it are called support vectors. ',\n",
       "  'SVM stands for Support Vector Machine. It creates a hyperplane such that margin of separation between positive and negative classes is maximised.  ',\n",
       "  'YOUR ANSWER HERE: SVM is a linear machine whose goal is to construct a optimal hyperplane such that the marginal separation is the maximum between the decision boundaries. The decision boundaries are drawn parallel to the hyperplane which just push the datapoints closest to the hyperplane. The datapoints closer to the hyperplane are called support vectors.',\n",
       "  'The idea of SVM is to fit a supervised model onto the training data allowing maximum generalization ability. \\n\\nThis is done by computing maximum margin between different classes of data using the support vectors.\\nThe magrin can be computed using differeent kernels for a higher dimensional data.',\n",
       "  \"A SVM is a linear machine which is used in pattern classifcation problems to find a decision surface in the form of a hyperplane for linearly separable classes such that the margin of separation between the classes is as large as possible. SVM's are an approximate implementation of the induction principle of structural risk minimization which is based on the fact that the error rate in testing is bounded by a term that is dependent upon the sum of training error rate and the VC dimension of h. \",\n",
       "  'The basic idea of SVM is to determine the best decision boundary i.e. the one which provides maximum margin so that the boundary can be widened most before it touches any datapoint. It is done using Support Vectors which are the the datapoints the margin pushes against.',\n",
       "  \"SVM refers to support vector machines.In terms of a linear classification problem svm can be defined as creating a hyper plane which is a decision surface and to maximize the width of decision boundary.In cases where the problem is complex svm can be used as it classifies the data by projecting the data in higher dimension.If the data is to be separated in 3 classes , they can use 3 svm's for three different classes.\",\n",
       "  'basic idea of SVM is to construct a hyperplane as the decision surface in such a way that the margin of separation between negative examples and positive examples is maximized.',\n",
       "  'The idea of SVM is to construct a hyperplane as a decision surface such that the margin separation between positive and negative examples is maximized.',\n",
       "  'SVM tries to find a **best hyperplane with widest margin with the help of support vectors** such that all the data points are classified correctly.',\n",
       "  'SVM is used for linearly separable data. A hyperplane is used to separate the data, but there could be so many hyperplanes that separate the data. The best hyperplane is choosen which separates data with a bigger margin. So in SVM we find the hyperplane which has a bigger margin between the hyperplane and both the positive and negative data lines.',\n",
       "  'Given a training set for classification, The basic idea of SVM is to construct a hyperplane as decision boundary such a way that the margin between the positive and negative points is maximum \\n\\nSupport vector is a small subset of the of the training data against which the boundary is pushed',\n",
       "  'A support vector machine classifies given data using a decision boundary. The width of this decision boundary (margin) is maximized to ensure good results, because a maximized width is as robust as possible. The margin width is $\\\\frac{2}{\\\\sqrt{w * w}}$. To maximize it, quadratic programming is used. In order to handle noisy data, slack variables are introduced. To eliminate them, duality is used.',\n",
       "  'An SVM is a linear classifier that divides a binary pattern, by a line that maximizes the margin between its line and the respective support vectors.',\n",
       "  'A SVM learns a decision boundary from the input data. Additionaly it learns two margins, which are parallel to the decision boundary and lie as close as possible at the data points, the support vectors. The decision boundary is chosen so that the margins are maximized. Using kernel functions higher dimensional data and non linearly separable data can be learned aswell.',\n",
       "  'An SVM is a learning machine that tries to learn the support vectors of a two class data set to get the maximum margin, the optimal seperating hyperplane, between the two classes.',\n",
       "  'A SVM uses a few of the data points as support vectors to bild the maximum margin classifier. It searches for the seperating line, which has the maximum margin to the datapoints. In cases of Noise, the seperating line is searched, which minimizes the distance to the points in the wrong category. The data is cast to a higher dimensional space to use covers theorem while using kernels. The data is more likely linearly seperable in the higher dimensional feature space. Using structural risk minimization the dimensionality is reduced. ',\n",
       "  'SVMs are used to linearly seperate Data points. The decision boundary is line or hyperplane in higher dimensions that defines the lable of a data point. The decion boundary is choosen in a way that the margin is maximized. Data points on the decion boundary are called support vectors and define the hyperplane. In 2 dimensions if the data is liner seperable the margin is equal to 2/sqrt(w.w) where w is the weight vector. If the data is not linear seperable, the input can be projected into higher dimension space. This increases the chance of linear seperablity.',\n",
       "  'Support vector machine is classifier which maximizes the margin between boundries learned from two classes.\\nMargin is minimum distance by boundries can be increased before hitting datapoints.\\nSupport vectors are the datapoints against which magin pushes up the boundary.',\n",
       "  'support vector machines are the finding classfiers, draw  the dision boundary which push against the support vectors.',\n",
       "  'The basic idea of Support Vector Machine (SVM) is to find the width of a line or hyperplane which which divides the input data into two classes. The points lying on the edge of the defined width are called support vectors.  ',\n",
       "  'YOUR ANSWER HERE\\n\\nSVM is a classifier that classifies a set of points in a way that maximizes the margin between the points of two classes. The classification can be linear or non linear.',\n",
       "  'Support Vector Machine: Eine Lern Maschine mit maximalen Margin. Sie kann für Dateneingaben die zugehörigkeit zu genau einer Klasse vorhersagen. Beim lernen wird die Trennung der Datenpunkte mit y=1 oder y=-1 so festgelegt, dass der Abstand zum nächsten \"falschen\" Punkt maximal ist. Rauschen, also Datenpunkte die im falschen Bereich liegen werden versucht zu ignorieren. Das lernen wird mithilfe von Quadratischer Programmierung realisiert. Die Vektoren die den margin berühren sind die support Vektoren. Nicht linear trennbare Daten, werden mithilfe von nichtlinearen basis funktionen, z.B. gauschen oder radialen basisfunktionen getrennt. ',\n",
       "  'The idea behing SVM is to find a hyperplane which separate data into classes. First it is required to find a data point which are clossest to hyperplane, and these data points are called support vectors. Next task is to find a maximum possible width of the hyperplain such that support vectors are on the edge of that hyperplane.\\n\\nThis problem is formulated as min-max constrained optimization problem. In order to find a optimum width of hyperplane, (optimimum of a funtction) the idea is to use method of Langrange multiplier. Additionally, when I data is not linearly separable, than an approach is to project data in higher dimension and then to find a hyperplane that separates data in that dimension.',\n",
       "  'In steepest descent, the gradient of the cost function is found by partially differentiating it with respect to the weights. The weights are then updated in the opposite direction if the gradient. This ensures that the weight moves in the steepest direction are reduced. It can also be proven that the weights always reduce. Hence, steepest descent can be used to minimize the cost function.',\n",
       "  'Steepest descent is method of optimizing the algorithm by minimizing the error. Weights are adjusted in the direction of steeping descent, opposite to the direction of the gradient.',\n",
       "  'YOUR ANSWER HERE Steepest descent moves the error within error surface a small step into the opposite direction of gradient. By help of steepest descent we want to minimize error. Steepest descent stops when gradient = 0.',\n",
       "  'When learning weights with a SD method, we try to reduce the error based on following the gradient of an error function in the opposite direction, effectively trailing the error surface towards the minimum. Here, the error function (typically some form of mean squared error) is differentiated w.r.t. the individual weights, expressing how much a weight contributes to the network error and must thus be corrected. Due to the gradient pointing in the direction of steepest ascent, we must thus step in the negative direction.',\n",
       "  '- Steepeset descent is used for error minimization when updating weights.\\n- According to this, we update the weights along a direction which minimizes the error; which is calculated by finiding the slope at the point.',\n",
       "  'Speepest decent is used to minimize the training error of a network given sample inputs and desired outputs. It uses the gradient of the error function to move the weights closer to an optimal weight with lowest output error. Using a learning rate we can influence the speed and stability of this algorithm.',\n",
       "  'The steepest decent is the direction the error function falls the most. We want to change the weights in the direction of the steepest decent (the opposide direction of the gradient) to have a smaller error in the next iteration and to optimize the ANN.',\n",
       "  'The idea of learning a network is to minimize a certain costfunction. We can use steepest descent to minimize this cost function. While there are other optimization techniques which can be used for optimization, steepest decent is a widly used optimization technique. To optimize a network we calulate the partial derivatives(gradient) and use it to update our weights. It is also used in BP.',\n",
       "  'The approach of the method of steepest descent is to find the direction for the minimization of the error in an approximation problem. The cost function e, dependent of the weights w, will be derivated (partial derivative) for all defined weights. This gradient will be used for updating the weights for the next iteration. The direction of the minimization of the error is the oposite direction of the gradien: - g.',\n",
       "  'When the inputs are being send into a network and we calculate the error we need a mechanism to learn and manipopulate the free parameters of the network and the learning uses the error but we must know in which direction in the search(optimization) space we should move so that we can reach the global minima of the error for this we use steepest decent. This method tells us in which direction we need to move by getting the gradient from the error.',\n",
       "  'Steepest descent is a method of weight adaptation. It is using first order derivative to approximate the function. Therefore is rather slow.',\n",
       "  'The steepest descent is an unconstrained optimization method that seeks to minimize an error function. This function is iteratively changed in direction oposite to the gradient vector.',\n",
       "  'Method of steepest descent updates the weights in the direction where the error is minimum.',\n",
       "  'The steepest descent method is an algorithm for finding the nearest local minimum of a function which presupposes that the gradient of the function can be computed. This property is used to determine the optimal weights of the NN.',\n",
       "  'Steepest descent is used to update the synaptic weights of a network based on a cost function expressed by the errors of the output. The weights are adjusted in the direction opposite to the gradient of the cost function.',\n",
       "  'In steepest descent the adjustments done on the weight vector are in the direction of the steepest descent which is in the direction opposite to that of a gradient descent. \\n\\nIn a learning problem, it basically used to reduce the cost based on the weight. The main goal is to find an optimal weight.',\n",
       "  \"Method of steepest decent is an unconstrained optimization technique used for learning in a network. It is used in iterative manner to minimize the error in supervised learning. It finds the direction of maximum gradient. So we go in the opposite direction hoping to find the minima. Convergence of the algorithm depends on the learning rate and also the condition that it doesn't get stuck in local minima.\",\n",
       "  'YOUR ANSWER HERE: In steepest descent method, the network moves towards the direction of the maximum gradient. The learning with steepest descent method can be slow to converge and can exhibit zigzag behavior.',\n",
       "  'Steepest descent involves weight updation in the direction of maximum steep or maximum derease in the cost function ot in the direction opposite to the gradient funcion. The weight update is $\\\\Delta w(n) = - \\\\eta g(n)$ where $\\\\eta$ is the learning rate which defines the magnitude of learning using the gradient g(n) which is the gradient of the cost function of errorsin the nth iteration. Higher $\\\\eta$ will result in rapid learning but with oscilations in responses.',\n",
       "  'The method of steepest descent is used to find the direction in which the error function viewed as a function of weights is decreasing most rapidly and then take a small step in that direction. When learning a network, steepest descent enables to iteratively adjust the weight vectors until the optimal weight vector that minimises the cost function (i.e, the error function where error is computed as the difference between the desired and actual response of the network) is found.',\n",
       "  'When learning a network the steepest descent algorithm updates the weights in such a way that the error decreases in every iteration.',\n",
       "  'The method of steepest descent moves in the direction opposite to the gradient to minimize the cost funcion .',\n",
       "  'steepest decent method is based on minimization of error cost function $\\\\xi(w) = 0.5 e^2_k(n)$, so synaptic weight of network is updated in a direction opposite to gradient vector of $\\\\xi(w)$, that is $W_k(n+1) = W_k(n) - \\\\eta \\\\nabla \\\\xi(w) = W_k(n) - \\\\eta e_k(n) x(n)$, $\\\\eta $is learning rate.$e_k(n)$ is neuron k error signal, $x_j(n)$ is input data.',\n",
       "  'The steepest is used to find a direction in which E is decreasing most rapidly. The adjustments applied to the weights are in the direction of steepest descent.',\n",
       "  'Steepest decent helps to **minimize the value of error function $E$** by finding the **right direction **to move the weight vector to reach global minima.\\n\\nThe direction is always **opposite to the direction of actual gradient vector**.\\n',\n",
       "  'Method of steepest descent is used to reduce the error. In backpropogation during backward pass we need to know how by how much amount the weights should be changed, this can be known if we use steepest descent, find the gradient of error and use it to reduce the error.',\n",
       "  'The steepest descent finds the direction of the error function and tries to reduce it by adding in the opposite direction\\n\\n$ del w = - \\\\eta g(n)$\\n\\ng(n)- gradient of the cost function',\n",
       "  'The steepest descent is used to find the right direction in which the weights should be changed while learning a network. The derivate of the error is used and weights are changed in that direction which makes the error smaller as fast as possible.',\n",
       "  'The steepest descent can be used to optimize the weights of a network. In steepest descent the error function is a function of the weights. So we determine the direction of the steepest descent on the error surface and go into that direction to minimize the error of the weights on optimize them. ',\n",
       "  'The method of steepest descent is used to minimize the error function. The error function is the gradient of the error $\\\\Delta e = d - y$, where d is the desired output and y is the actual output of the neuron.',\n",
       "  'Steepest descent is the basic learning algorithm others are derived from. The goal when learning a network is to minimize the error. This is achieved by starting at a random position and going in the opposite direction of the gradient vector, the steepest descent.',\n",
       "  'the error function is computet. to adapt the weights (learn the network) the error function is followed in small steps in direction of steepest descent to decrease the error. using iterations the error is decreased in each step and end in a (local) minimum\\nused in back-propagation',\n",
       "  'In error correction learning the weights of a network are learned in a way that e(x) is minimized, where e(x) is some error function. In order to minimize the error function the method of steepest desend is used. The negative gradient of e(x) points in the direction of steepest decend. Doing steepest descend in a single layer feed forward network leads to the delta rule.',\n",
       "  'Steepest descent adjust the parameters (weights and bias) of the NN to minimize the error. It does so by adjustinmg the weights in the direction of steepest descent of the error function. ',\n",
       "  'Steepest decent while move in the direction of the max improvement ( in terms of decrasing) in the cost funtion or error.\\n\\nif the learning rate is large then the it follows the zizag motion.\\n\\nif the learning rate is too low then it takes time for converging .',\n",
       "  'The method of steepest descent is responsible for weight adjustments in the network. The weights are adjusted in the direction of the steepest descent that is equal to the negative grad of the error. It ensures that the weights are decreased in every iteration step.',\n",
       "  'YOUR ANSWER HERE\\n\\nSteepest decent method helps in making the adjustments of the weights in a neural network in a way that minimizes the average squared error.\\n\\nIn each step it gives the direction towards which the maximum decrease of the average squared error can be achieved.',\n",
       "  'Steepest decent findet verwendung beim trainieren von Netzen bei verschiedenen Netz Architekturen.Voraussetzung dafür ist, dass beim Training für die Daten die gewünschte Ausgabe bekannt ist und dass die Aktivierungsfunktion der Neuron ableitbar ist. ',\n",
       "  'The method of steepest decent is used for finding minimum of a cost(error) function. The steepest decent iterates over possible values of weight vector to optimize the function. It is used for deriving error function in ADALINE (adaptive linear element) algorithm, and it is used also in backpropagation method in training of Multi-layer NNs.',\n",
       "  'A dataset $A \\\\subseteq X $ with N datapoints has $2^N$ binary maps. If for any of these binary maps, a hypothesis $h \\\\in H$ splits the positive data from the negative data such that there is no training error, then it is said that h shatters the dataset A.',\n",
       "  '',\n",
       "  'YOUR ANSWER HERE A a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$ if there exists a an $\\\\alpha for every training set with zero training error',\n",
       "  \"... when our learned machine achieves zero training error on every classification problem of the dataset A. Since we got a selection of $n$ points in the dataset A, the number of problems in binary classification is 2 to the power of $n$ (I didnt find the 'Dach' symbol on the english keyboard :) )\",\n",
       "  'Considereing a dataset $A \\\\subseteq X$ ,where X is the instance space and A contains N elements. Now there are $2^N$ binary maps or learning problems when we wnat to separate two classes.\\n\\nIf any of these problems can be separated completely by hypothesis $h \\\\in H$ then h is said to shatter A.\\n\\ni.e., a hypothesis shatters a dataset, if it can completely separate the classes with zero error for all possible combination of labels in the dataset.\\n',\n",
       "  'when every possible combination of input and desired output can be classified using $h$',\n",
       "  'A hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X$, then for every point $x_i \\\\in A$ there is a label $y_i \\\\in \\\\{1,-1\\\\}$ and the $H$ can saperate these two classes using $h$ with no training error.',\n",
       "  'there exists an arrangement of these points in A sucht that for each possible combination of labels to these points  the hypothesis h has zero training error',\n",
       "  'An hypthesis *h* shatters a dataset A, if for a given data set, h is able to distinguish (or separate) the different classes of this data set.',\n",
       "  '\"h\" shatters A if for any set of input data points in A there exist at least one training error of zero.',\n",
       "  'H shatters A when for example in given dataset (X_1,X_2...X_r) output are in a form (X_1, Y_1),(X_2,Y_2)...(X_r,Y_r) there has been found a 0 error.',\n",
       "  'A machine F can shatter a set of points $x_1, x_2, x3_,..., x_n$ if and only if for every training set, there is a weight vector $\\\\alpha$ that produces zero training error.',\n",
       "  'A hypothesis $h \\\\in H$ shatters a dataset $A \\\\subset X \\\\Leftrightarrow$ for each assignable configuration of $(x_i, y_i)\\\\in A$, $h$ perfectly classifies all elements of the set $A$.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'A hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow$ \\n\\nat least on possible combination of dataset $A$ can be classified by the hypothesis $h \\\\in H$ with zero training error.',\n",
       "  'Given a dataset $A \\\\subseteq X $ where X is the instent data space, for a given problem with the dataset A, if a learning machine is able to successfully split the positive and the negative data, then we say that A is shattered by the learning machine. ',\n",
       "  'Suppose X is a training dataset and A is the subset of training dataset then hypothesis h is said to shatter if can correctly classify all the points in A i.e zero training error.',\n",
       "  'YOUR ANSWER HERE: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$ if the hypothesis can clearly distinguish the positive examples from the negative examples in A.',\n",
       "  'A hypothesis h is model that separates a dataset consisting of {(x_i , y_i)} samples into positive and negative samples. h is said to shatter a given subset of a dataset if it can successfully separate at least one configuration of the subset of dataset.',\n",
       "  'A hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X $ if there exists an $\\\\alpha$ for which there is zero training error',\n",
       "  'for each of the $2^N$ (where N is the size of A) combinations of input output mappings of the form $(X_i, y_i)$, h is able to classify the data correctly that is with zero error. ',\n",
       "  '',\n",
       "  'For all possible binary labeling of dataset A, we can find a hypothesis h that can separate the positive examples from negative examples, the H shatters A.',\n",
       "  'The hypothesis $h$ can shatter any points of $x_1$, $x_2$, ..., $x_n$ if and only if for every possible training set of the form $(x_1, y_1), (x_2, y_2), ... (x_n, y_n)$ there exist some values of $\\\\alpha$ that gets zero training error.',\n",
       "  'A hypothesis space H shatters a dataset, if and only if there is a **possbile $\\\\alpha$ (weight vector)** on hypothesis space that **seperates all the positvie data from negative data**.',\n",
       "  'a hypothesis $h \\\\in H$ shatters $A \\\\subseteq X$ if and only if there exists a value of $\\\\alpha$ for which the training error is zero',\n",
       "  'H is the vc dimension of a learning maching that can shatter h points. \\n\\nVc dimension of a learning machine is the maximum number of points that can be arranged so that the learning machine can shatter them\\n\\nShattering:\\n\\nThe learning machine is said to shatter points $(x_1 ... x_r)$ if and only if all the possible training set of $((x_1,y_1) ... (x_r,y_r))$ can be classified with zero training error',\n",
       "  'A hypothesis shatters a dataset if it can correctly classify all combinations of labellings of the points in the dataset.',\n",
       "  ', if there exists a configuration of $X$, so that $h$ gets zero training error on any dichotomy of the datapoints.',\n",
       "  'there exist w weights, which produce a perfect classification.',\n",
       "  'for all possible classified subsets of dataset A the hypothesis h can seperate it',\n",
       "  'when all combinations of position and labeling of the data can be separated in the given classes by the hypothesis',\n",
       "  'h shatters A when and only when for all possibilities of (a_1, y_1), (a_2, y2), ... ,(a_n, y_n), where y is the class lable (1 or -1) there exists some $ alpha  $ for a learning machine f that produces 0 training error.',\n",
       "  'If there exist atleast one configuration of A for which training error of h is zero. i.e. it successfully classifies all oints in A. ',\n",
       "  'and there exist a linear saperater which saperates positve examples from the negavtive examples correctly. then we say that A can be shatter at h.',\n",
       "  'Ginven a data set A if it is possible to find a hypotheis H which separates the data set into binary form without any error, we can say that hypothesis $h \\\\in H$ shatters dataaet $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$.',\n",
       "  'YOUR ANSWER HERE\\n\\nIt means that for all the points in A with input output pair (x,y), for any combination of ($x_i$,$y_i$) there exist parameter $\\\\alpha$ of h that enables h to classify the points with zero error',\n",
       "  'gdw. für alle möglichen Trainingsdaten, die Daten so angeordnet werden können, dass h sie korrekt zuordnet (binäre zuordnung).',\n",
       "  'We say that a hypothesis h shatters a dataset A, iff the h produces a zero training error for certain data set A. In other words, we say that a hypothesis h shatters a dataset A, when h separates data A in two classes without erorr.',\n",
       "  '$ \\\\Delta w = \\\\eta e(n)x(n) $, where $\\\\eta$ is the learning rate.\\n\\nWidrow-Hoff rule states that the change in weights is proportional to the product of the error and the input in the corresponding synapse.',\n",
       "  'Weights adjusted are proportional to the product of error signal and the input vector\\n\\nw(n + 1) = w(n) + $\\\\eta(d-y)x(n)$\\n\\n$\\\\eta$ is learning rate, d is desired output, y is current output. x(n) in input vector. ',\n",
       "  'YOUR ANSWER HERE Adaption of weight is proportional to product of input and error:\\n$w_{new} = w_{old} + x*e$',\n",
       "  'For neurons with a linear activation function (ADALINE): $w(t+1)=w(t)+\\\\alpha (d-y)x$, where x is the input pattern, d is the true value and y is the net output. Notice that the delta rule looks similiar to the perceptron learning rule, but was derived from SD, whereas the perceptron works with a step function which is not fully differentiable.',\n",
       "  'Widrow-Hoff learning rule is also known as error correction rule is used to update the weights as:\\n$\\\\Delta w = \\\\eta (d_i-y_i)x_i$ where, d is the desired output and y is the output the network generates and x is the input.',\n",
       "  '$w(n+1) = w(n) + \\\\mu (d(n) - y(n))x(n)$\\n\\nThe change of the weights is determined using the error ($d(n) - y(n)$) and the input that was given to the network. The learning rate can improve learing speed. The new weights are dependent on the old ones and the change calculated',\n",
       "  '$w_{ij}(n)= w_{ij}(n-1)+ learningrate*(d_j-y_j)*x_i$\\n\\nwe change the weights by computing the error $e_j= (d_j-y_j)$ for the input and multiply it by the learningrate and the $x_i$ and adding it to the old weight. This minimises the squared error function (our cost function) and is the online variant of the steepest decent method. ',\n",
       "  '$$ \\\\Delta w(n) = \\\\eta * e(n)*w(n) $$\\n$$ e(n) = (y-d) $$\\nThe widrow hoff learning rule is error correction learning. It is used to train a network in a supervised manner. The widrow hoff learning rule can be derived from gradient decent. The rule consists of the error e(n) the neuron has and is muliplied with the weight so that the impact of the weight to the error is incorporated into the update.  A learning rule is use as a adjustment in how much we trust the weight change. The error is calculate by the difference between the current and expected output.',\n",
       "  \"The Widrow-Hoff learning rule is defined as: $w(n + 1) = w(n) + \\\\eta * x(n) * e(n)$\\n\\nThe Widrow-Hoff learning rule is a rule for adjusting the weights of a NN for a error correction learning task. This learning rule is derived from the steepest descent method, where the direction for the minimization of the error is the defined as the oposite direction of the cost function's gradient. This gradient can be simplified as $x(n) * e(n)$, where e(n) is defined as the difference between the desired response and the actual response of the learning machine (NN): $e(n) = d(n) - y(n)$.\\n\\n$\\\\eta$ defines the learning rate used.\",\n",
       "  'YOUR ANSWER HERE',\n",
       "  'Windrow-Hoff rule is \\n$$W_{new}=x_{input}*W_{old}*(d_{output}-y_{output})*eta*a $$\\n\\nwhere \\n$W_{new}=new weight,W_{old}=old weight,d_{output}=desired output,y_{output}=actual output,x_{input}=input, eta=learning rate, a=learning constant$',\n",
       "  'The Widrow-Holf or delta rule is a gradient descent learning rule used to adapt weight in a perceptron. \\n\\n$\\\\Delta w(n) = - \\\\eta(d(n) - y(n))x(n) $\\n\\n$\\\\Delta w(n) = - \\\\eta e(n)x(n) $',\n",
       "  'The widrow-Hoff (delta) learning rule is given by\\n$$ w(n+1) = w(n) - \\\\eta x(n) e(n)$$\\nwhere $e(n)$ is the error vector, $\\\\eta$ is learning parameter, $x(n)$ is input vector.',\n",
       "  'The Widrow-Hoff Learning rule is also referred to as Delta, or Least Mean Square (LMS) Rule. It is used to minimize the cost function and is defined as follows:\\n\\nDelta w_ji(n) = eta (partial x_i(n) / (partial w_ji(n))\\n\\nwhere eta is the learning rate paramter, x_i(n) is the total instantaneous error energy and w are the weights.',\n",
       "  'The Widrow-Hoff learning rule, also called delta rule, is used for learning a network by adjusting the synaptic weights of the network with the error signals:\\n\\n$$ w(n+1) = w(n) + \\\\eta (d(n) - y(n)) x(n) $$\\n\\nwhere $n$ is the number of iteration, $\\\\eta$ is the learning rate, $d(n)$ is the desired output signal, $y(n)$ is the actual output signal, and $x(n)$ is the input signal. $(d(n) - y(n))$ is the error signal.',\n",
       "  \"Widrow hoff's learning rule states that the adjustment of the weight of a synapses are propotional to the product of the error function and the input which is given by the synapses based on the problem. \\n\",\n",
       "  'Widrow Hoff rule is based minimising the mean square error using gradient descent alogirthm. Weights are adjusted in following manner:<br> \\nw(n+1) = w(n) - n (gradient of mean square error) <br>\\nIt takes the gradient of the mean square error $0.5 e^{2}(n) = e(n) \\\\frac{\\\\partial e(n)}{\\\\partial w} = e(n) x(n)$',\n",
       "  'YOUR ANSWER HERE: Widrow- Hoff rule:\\n- $\\\\Delta w$ = $\\\\eta e(n) x(n)$\\n- Widrow-Hoff rule states that when an input x(n) produces an error e(n), then the change in the weight is directly proportional to the error signal and the input signal.',\n",
       "  'Widrow Hoff learning rule is also called as error corresction learning rule. The error is defined as the difference between the desired and the actua output of the learning machine. Assuming the desired signa is available, the error is computed and weights of the neural network are upadted in the direction of reduction of errors. The error for each input sample for a neuron k is computed using $e_k(i) = d_k(i) - y_k(i)$. weight change $\\\\Delta W = W*e$, that is the dot product of error and the weights is computed.',\n",
       "  'Given a neuron k excited by an input signal $x_i$, if $w_{ki}$ is the synaptic weight of the neuron, then the Widrow-Hoff learning rule gives the weight adjustment $\\\\Delta w_{ki}$ applied to the neuron k in mathematical terms  as follows: $\\\\Delta w_{ki} = \\\\eta x_i(n)e(n)$ where e(n) is the instantaneous value of the error signal. Thus the  Widrow-Hoff rule states that the synaptic adjustment applied to the weights of a neuron is proportional to the product of the input signal to the neuro and the instantaneous value of the error signal. This rule assumes that the neuron has an external supply of desired response so that the error can be computed. ',\n",
       "  'The Widrow-Hoff learning rule is given by\\n$$w(n + 1) = w(n) + \\\\eta e(n) x(n)$$  \\nwhere \\n$w(n)$: Weight in iteration n  \\n$e(n) = d(n) - y(n)$: Error  \\n$d(n)$: Desired output  \\n$y(n)$: Actual output  \\n$x(n$: Input    \\n$\\\\eta$: Learning rate',\n",
       "  'Widrow -Hoff learning rule states that the adaptation made to the synaptic weights is proportional to the product of input and the error function.It basically states that if the error is high then the product of input and error will also be high , and thus the adjustment made to the weight would be more.\\n$w_j(n+1) = w_j(n) + eta*(error)*input$',\n",
       "  'it is based on minimization of error cost function $\\\\xi(w) = 0.5 e^2_k(n)$, so synaptic weight from neuron k to input j is updated in a direction opposite to gradient vector of $\\\\xi(w)$, that is $w_{kj}(n+1) = w_{kj}(n) - \\\\eta \\\\nabla \\\\xi(w) = w_{kj}(n) - \\\\eta e_k(n) x_j(n)$, $\\\\eta $ is learning rate.$e_k(n)$ is neuron k error signal, $x_j(n)$ is input data.',\n",
       "  'Windrow-Hoff or error correction learning rule says that the adjustment of a weight is proportional to the product of the error signal and the input signal of the weight. ',\n",
       "  '$$\\\\bigtriangleup \\\\omega_{ji} = e_j * x_i$$\\n$$\\\\omega(n+1) = \\\\omega(n) + \\\\eta \\\\bigtriangleup \\\\omega_{ji}$$\\n\\nWidrow Hoff learning rule says that, the synaptic weight update is directly proportional to the product of error and the input.',\n",
       "  'Widrow-Hoff learning rule: The rules states that the weight update is directly proportional to the product of the input to the neuron and the error.\\n\\n$\\\\Delta w_{ij} = \\\\eta e(n) \\\\sum x_i(n)$',\n",
       "  'delta $ w_{kj} =  \\\\eta e_k . x_j $\\n\\nWidrow hoff rules states that the change in synaptic weight is proportional to the product of the error signal and the input signal ',\n",
       "  '$\\\\Delta w(n) = \\\\mu * x(n) * e(n)$\\n\\n$\\\\mu = $ learning rate\\n\\n$x(n) = $ input at timestep n\\n\\n$e(n) = d(n) - y(n)$\\n\\n$d(n) = $ desired signal at timestep n\\n\\n$y(n) = $ output of the network at timestep n\\n\\nThe Widroff-Hoff (or delta rule) changes the weights depending on the input and the error, which is the difference between the output of the network and the desired output. This weight change can be scaled by a learning rate.',\n",
       "  'The Widrow-Hoff rule is used in error-correction learning and uses the current error and output of the system to determine the new weights.\\n\\n$w(n+1) = w(n)+\\\\eta \\\\cdot e(n) \\\\cdot y(n) $',\n",
       "  '$\\\\Delta w(n) = learning\\\\_rate \\\\cdot x(n) \\\\cdot e(n)$, where x is the input data, $e = d - y$ is the error from the desired output and the actual output, and the learning_rate is a parameter chosen as necessery to change the speed of learning.\\n\\n$w_{new} = w_{old} + learning\\\\_rate \\\\cdot x \\\\cdot e$, this is the formula to update the weights and to learn the input data.',\n",
       "  \"weights(t) = weights(t-1) * learning_rate * (desired(t) - output(t))\\n\\nThe Widrow-Hoff rule, also the delta rule, is used to update the weights of neural networks in a learning algorithm. It uses the previous weights' result and compares it to the desired result. This discrepancy is then applied to update the weights based on a learning rate.\",\n",
       "  'w_new = w_old + learning_parameter * error(n) * input(n)\\n\\nwhile error is: desired_input - current_output\\n\\nthe new value for the synaptic weight is computed of the old value plus a learning rate times the current error and the input. The output error is decreased in each step until the change is to small or the generalization is sufficient ',\n",
       "  'Rule: w+1 = w + n * x * ( y - d)  where n is the learning rate, x is the input, y is the ouput of the network d is the desired output  \\nThe widrow-Hoff rule minimizes the error (y-d). The weight change is proportional the ibnput x and the error. It can be derived from steepest descend.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'This the basicly the calulating mean squared  error (MSE) from the expected output and real output.\\n\\nModifiying the weights for Minimizing MSE it . ',\n",
       "  'Widrow-Hoff rule states that the weight adjustment is proportional to the product of input and the error in the output. It is also called the delta rule.\\n$$\\\\Delta w_{ji}  = \\\\eta x_ie_{ji}$$\\n$\\\\eta$ is the proportional constant also called as learning constant\\n$$W(i)=W(i-1)+\\\\Delta W_{ji}$$',\n",
       "  'YOUR ANSWER HERE\\n\\n$\\\\Delta W_{ji}$ = $\\\\eta e_jx_i$\\n\\nAdjustment made to the weight of a neuron is proportional to the product of the error in that neuron and input applied to the neuron.',\n",
       "  \"Fehler korrektur. w(n+1) =learnparam* (1/2 e*e(n))'\\nEs wird der Fehler an der Ausgabe gemeßen, mithilfe der Ableitung der Aktivierungsfunktion wird der Fehler korrigiert. Korrektur erfolgt in abhängigkeit eines lern parameters, der die schrittgröße angibt. \",\n",
       "  'Widrow-Hoff learning rule is derived from LMS error method, and it is defined as: $W{t+1} = W{t} + \\\\mu \\\\cdot \\\\Delta W$, where $\\\\mu$ represent learning rate, and  $\\\\Delta W = -(gradient \\\\ of \\\\ instantaneus \\\\ erorr) = -(d - y)X $, Here $d$ represent desired signal, while $y$ represent output signal of a neuron. $X$ represent input of a neuron',\n",
       "  'In backpropagation, the gradient of the error produced at the output layer (by partially differentiating the cost function with respect to the weights) is propogated backwards one layer at a time back to the input layer. This propagated gradient is used to update the weights in the corresponding layer. Backpropagation is necessary because the desired output at every layer is not known and it is only possible to formulate the cost function at the output layer.',\n",
       "  'In back propagation, there are two phases:\\n\\n1. Forward Phase: First we apply input to the network and compute the current output. \\n2. Backward Phase: We compute the error between current and desired output. Error is minimized by computing gradient of error with respect to weight. In return, weights are adjust.\\n\\nAfter adjusting weights in backward phase, we again go to forward phase and compute the current output, check whether error is minimized or not. ',\n",
       "  'YOUR ANSWER HERE Back propagation wants to minimize the error function E. E is given by: \\\\( $ \\\\frac{1}{2}\\\\sum e(n)^{2}$  \\\\). THe error function can be minimized by calculating the gradient starting from the output. Term for calculating the gradient differs. It depends on whether the neuron for which the gradient to be calculated is an output neuron or  a hidden neuron.',\n",
       "  'Backpropagation is the general form of the delta rule, formulated for networks with multiple hidden layers. Here, we propagate the error of the network back to the input layer to determine the change of weights, using the error signal in the output layer and subsequently the local gradients in the hidden layers. In the forward pass, we compute the net output forwards. In the backward pass, we propagate the error backwards. The BP rule was derived from the error gradient w.r.t. the weights, and application of the chain rule.',\n",
       "  'Back propagation is propagation of error from the output layer to the hidden layer in network with multiple layers. \\n\\nThis is done by calculating the local gradient of each node and then using this (along with the weight) to determine how much of the error is to be propagated to the particular node',\n",
       "  'Back propagation is used in multi layer network. It consits of two phases: Forward and backward.\\nIn the forward phase we give and input to the network and caculate its outputs. Also memorize the local field of each node. The local gradient (delta) is used to adapt the weights of the layers. It is different for output and the remaining layers. \\n\\nFor node i in an output layer: $\\\\delta_i(v_i) = \\\\varphi^\\\\prime(v_i)(d_i - y_i)$\\n\\nFor node i in other layers: $\\\\delta_i(v_i) = \\\\varphi^\\\\prime(v_i)\\\\sum_{j\\\\in C} w_ji \\\\delta_j(v_j)$, where $C$ are all the nodes that use node i output as an input\\n\\nrepeat this process for all input data until error is small enough',\n",
       "  'The back propagation algorithim is there to train a mulilayer feedforward ANN. We change the weights by computing the local gradiant at each neuron by using the neurons in the layer befor. The local gradient of the output neurons can be computed easaly. The activation function has to be differantable for the backpropagation algorithm.\\n\\nIn the forwart pass we compute the output y at the output layer.\\n\\nIn the backard pass we use the output y and our desired output d to compute the local gradients at the output layer. Then we go back layer by layer and use the local gradients from before to compute the new local gradients.\\n\\nBy that we minimize the average squared error function.',\n",
       "  'Backpropagation is a learning algorithm for Multilayer FF NN. It is supervised error correction learning. \\nThe weights are initialised randomly\\n\\nThe algorithm has to steps:\\n\\nIn the forward pass the the output is calculated by using the current weights.\\n\\nIn the backward pass the weight update for the outputlayer is as like in single layer ff. The error is used to update the weights. BP allows us to also calculate the error of hidden layers. For each hidden layer we use a local gradient as the error. The local gradient is the sum of weighted error of the following layer, which is passed trough the derivate of the activation function. So it is possible to backpropagate the error from the output layer to to first layer.\\n\\nA common Problem in BP is the vainshing gradient problem. Depending on the activation function used the local gradient gets smaller in each layer until it is eventually less than the floating point precision used. This limits the number of layers that can be stacked.',\n",
       "  'The back propagation algorithm is a learning algorithm for updating in the weights in a multi-layer neural network. For updating the weights of all the layers, the error of each neuron must be calculated. In the back propagation algorithm, two phases will be defined:\\n- Forward phase: the output of the neural network will be calculated and also the error of the neurons in the output layer.\\n- Backward phase: the gradient of each neuron will be calculated, by using the calculated error on the output layer and the defined connections between the hidden layer and the output layer. If multiple hidden layers are defined, the error will be iteratevely will be given backwards and the weights at each neuron will be updated.',\n",
       "  'Back propagation is a steepest decent method that uses the final produced error and the local gradient to define the amount of change needed for each synaptic weight.\\n\\nIn this method we have two phase:\\n    - forward phase: in this phase we feed the input to the network and the network calculate the output\\n    - backward phase: in this phase we first calculate the error and then use the local gradient to propagate the error to the network from the last layer to the first and manipulate the synaptic weights',\n",
       "  'Back propagation consists of two steps:\\n1. forward pass - data is passed through the network and weights are atapted\\n2. backward pass - by using local field of each neuron error signal is propagated backward by using local field of each neuron from end to beginning and stacking them up. Local field is partial derivative of the output signal of a a neuron, for output neuron it is simplest to calculate as it has only desired output and actual output to deal with.',\n",
       "  'Backpropagation is a learning algorithm in multi layer networks that consists of two phases, a forward pass and a backward pass. In the forward pass, the output is calculated by passing activations layer through layer starting from the input, then through hidden layer and finally output. Then the error is calculated in the output layer and propagated backward through the network. In the forward pass, the weight do not change. In the backward pass, the weights change in proportion to the local gradient.',\n",
       "  'Backpropagation is a neural network based learning algorithm where the network learns by propagating the error through the network. BP consists of two stages:\\n+ Forward pass: where the error is computed by feeding the input to the network.\\n+ Backward pass: where error is propagated through the network for doing the weight updates locally.\\nSince BP has vanishing gradient problem, it is useful to use activation functions which are infinitely differentiable such as sigmoid function.',\n",
       "  \"The back propagation algorithm is used to calculate the error contribution of each neuron after a batch of data is processed. Required is a known desired output of each input value. Thus the back propagation algorithm is a supervised method. The algorithm can be subdivided into two phases:\\n\\n1) Propagation:\\n* Propagation forward through the network to generate the output value(s).\\n* Calculation of the cost error term.\\n* Propagation of the output activation back through the network usin the training pattern target in order to generate the deltas (differences between desired and actual output) of all output and hidden neurons / by recursevliy computing the local gradient of each neuron.\\n\\n2) Weight update:\\n\\nFor each weight the following steps need to be applied:\\n* The weight's output delta and input activation are multiplied to find the gradient of the weight.\\n* A ratio (percentage) of the weight's gradient is substracted from the weight. This ration is also referred to as the learning rate and influences the speed and quality of the learning.\\n\\nLearning is repeated for every new batch until the network performs adequately.\",\n",
       "  'Backpropagation is an algorithm for training a neural network, and it contains of two main stages. The first stage is to compute the actual output given the input; in this stage, the signal flows forward from the input layer to the output layer, and the synaptic weights are fixed. The second stage is to update the synaptic weights by propagating the error signals backward from the output layer in a layer-by-layer manner; for each neuron, the local gradient, the partial derivative of cost function to the local field, is computed. ',\n",
       "  'Back propogation usually occurs in a multi layer perceptron. \\n\\nIt uses a non linear activation function. \\n\\nBasic elements: \\n1. Functional signals: These are the input signals, which passes through the network from left to right. As the name denotes it performs a usefull function at the output of the neuron and another reason for the name is that the functional signals are calculated based on the parameters and the activation function. \\n\\n2. Error signals: Error signals propogate usually in the reverse direction which contains the error based on the desired output. \\n\\nIt consists of 2 phases: \\n1. Forward phase: In the forward phase the signals propogate from left to right. Weights are fixed and passes through all the layers of the network, that is undergo all the activation. \\n\\n2. Reverse phase: In the reverse phase, the local gradients are calcualted and are propogated through in the backward direction. Here weights change.',\n",
       "  'Backpropogation is used for training multi layer networks. It constitutes of forward pass and backward pass. In forward pass network computes the output. Based on this the errors are calculated based on difference between network output and desired output. These errors are the backpropogated to network during backward pass and used for adjusting the synaptic weights. ',\n",
       "  'YOUR ANSWER HERE: Backpropagation is used for Multilayer perceptron network. It consists of two passes.\\n- Forward pass: The outputs are calculated at every computational node and passed till the output node where the error is calculated by difference of desired output and the actual output. In this pass, the weights of the synaptic links are not changed.\\n- Backward pass: The error generated at the output neuron is passed in the backward direction i.e., against the direction of the synapses and the local gradient of the error is calculated at every neuron.',\n",
       "  'Back prop is a way of training a neural network by adapting the weights using error produced. It consists of two phases, forward and backward. Forward phase computes the output along the network using the function signal. In the backward phase, the error of thr outpur fromthe derired output is computed and a local gradient of the error is used to update the weights iof the network. The local gradient considers the credit or blame of the corresponding weights of neuron in producing the output.',\n",
       "  'The back propagation algorithm is based on the error correction learning rule and consists of two passes:\\n1. Forward pass : The input signal applied to the source nodes of the network is propagated forwards through the different layers of the network, and the output is computed at the output layer of the network.\\n2. Backward pass : The error signal computed at the output is propagated backwards, with a local gradient computed at each of the hidden layer neurons, in order to adjust the synaptic weightsof the neuron in the network.',\n",
       "  'Back propagation is moving the error backwards recursively through the network by calculating the local field of every neuron to update the weights. It is based on the chaining rule of derivatives.',\n",
       "  'Backpropagation is a neural network which has two stages:\\n-Forward pass: In forward pass the error is calculated in the output layer with the help of the desired output and the given output. e = d - y\\n- Backward pass: It begins in the output layer , in this case the error is passed backwards with the calculation of gradients at each layer of the neural network\\nSo in back propagation the adjustment to weights is made based on the local gradients which is calculated at each layer.',\n",
       "  'It contains forward pass and backward pass. In the forward pass, input is applied to the network and propagate it forward through the network, then compute the output of neurons in output layer and errors for output neurons. In the backward pass, compute local gradients and update the synaptic weights according to error correction rule for each neuron layer by layer in a backward direction.',\n",
       "  'Back-propagation algorithm consists of two passes:<br>\\n1. Forward pass: the input vector is applied to the network layer by layer\\n2. Backward pass: the weight is adjusted based on error correction learning rule.\\n<br>\\n<br>\\nBack propagation uses error correction learning rule and the objective is to minimize the average of squared error.',\n",
       "  '* Backpropagation is a steepest decent method that calcualtes the error at the output neurons and backpropagates those errors backwards to update the weights of each neuron.\\n* The synaptic weight updated is directly proportional to **partial derivatives**\\n* Local gradient is calculated at ouput neurons and hidden neurons.\\n* Local gradient at output neurons are calculated using the observed error.\\n* But the error function is missing in the hidden neurons, so the local gradient of hidden neuron j is calculated recursively from the local gradients of all neurons which are connected directly to the hidden neuron j. ',\n",
       "  'Backpropogation has 2 steps. \\n\\nForward pass: In forward pass the data is run through the network and the error is calculated.\\n\\nBackward pass: In Backward pass the weight is adjusted using local gradient of error such that the error is minimized.\\n\\nThere are many ways for weight adjustment like, steepest descent, Newtons method, Gauss newton method.',\n",
       "  'The back propagation is a learning method in neural networks. Back propagation enables the feed forward netwowrk to represent XOR gate.\\n\\nIt has two phases:\\n\\nforward pass: the initial weights are used to calculate the value of the output neuron\\n\\nbackward pass: starts from the output layer and travels backward. During this phase the weights are changed based on the local gradients of each neuron|',\n",
       "  'Back propagation is used to learn weights in a multi-layer feed forward network. It is divided into two steps: forward and backward. In the forward step one input is passed through the network to calculate the output of the network. This output is used to calculate the error of each output neuron given the desired output. After this forward step, in the backward step the weights are changed beginning in the end of the network. Each weight is changed by taking the derivative of the activation function of the neuron times either the error, if the following neuron is an output neuron, or all local gradients of connected neurons times the corresponding weights. The weight changes are the local fields.',\n",
       "  'Backpropagation is used in Multilayer Perceptrons to give a method of adapting the weights. First the forward phase is run like in a regular feedforward network. Then after the output and thus the error is determined the error is backpropagated from ouput layer through the network. Since we have multiple layers, there is only a desired output of the network for the last layer. To counteract this problem a gradient is calculated for every neuron during the backward pass. The gradient is giving a measure of the contribution of this neuron to the final error. The gradient is then used to update the neurons weights. If the neuron is not part of the output layer, the previous gradients are used to calculate the new gradient instead of using the error.',\n",
       "  'Back propagation consists of two steps:\\n1. step - Forward pass: Here the input data is fed into the network and the output is calculated at the output nodes. The usual calculations of the induced local field are done by using this formula $v = \\\\sum wx + b$. The output is then calculated using this formula $y = f(v)$, where f() is the activation function.\\n\\n2. step - Backward pass: Here the error is backpropagated through the network from the output layer to the input layer. In the output layer the error is calculated using this formula $\\\\delta = d - y$, using the desired output d and the actual output y. In the layers before the output layer the local gradient is used to calculate the error using the error from the output layer $\\\\delta = w\\\\delta x$. Additionally the weights are updated using $w_{new} = w_{old} - learning\\\\_rate \\\\cdot \\\\delta x$',\n",
       "  'Back propagation is a learning algorithm for multilayer neural networks. At first, the input is propagated through the network until the end is reached. Here the error is calculated with the desired result. Then the error is used to update the weights from the back to the front. For the output layer the weights can be updated directly with the calculated error. The following layers have to use the local gradient of the previous error, which is calculated with the derivative of the activation function and its error. This is then used to update the weights and repeated until the front is reached.',\n",
       "  'back propagation is used in multilayer feedforward networks. first the forward pass is computed. The given error at the output nodes is used to compute the weight changes using widrow-hoff learning rule. then the error is given back layer by layer in the backward pass to compute the error and weight changing for each layer recursivly. The learning can be done in sequential (online) or batch mode (offline) ',\n",
       "  'In multi layer ff networks the error is only available in the last layer. Therefore the error is propagated back through the network using the backpropagtion algorithm. In order to do so the local gradient has to be calculated. \\nUpdate of the weight: w+1 = w + n * x * gradient where the iput x is the output of the previous layer.\\nThe local gradient is calculated diffrently depending if the neuron is in the output layer or in the hidden layer.  \\nOutput layer: $ gradient = phi`(x) * (y -d) $  \\nHidden Layer: $ gradient = phi_j`(x) * SUM(w_i * local gradient_i) $    ',\n",
       "  'In steepest gradient weights are adjusted in decreasing direction of error function. But for hidden neurons there is no labels available to calculate the error. Hence final ouput error is backpropogated through the layers inside the hidden layers of NN. This is possible with continuous activation function and chain rule on its derivatives. \\nFinal error is differentiated with respect to hidden weights. Chain rule is applied to find local error on hidden neurons. ',\n",
       "  'the back propagation algorithm  it consist of forward pass and backward pass\\n\\ncomputes the output of the neuron \\n\\nthen it propagates in backward direction while recursively compute local gradient of the neuron \\n\\nweights are adjusted accordingle.',\n",
       "  'Back Propagation is the process of learning in Multi Layer Perceptron in which the error from, the output of the network is fed back into the network to adjust the weights in the hidden layer. That is the error back prpagates into the network to enable the network to learn by adjusting the synaptic weights based on it.',\n",
       "  'YOUR ANSWER HERE\\n\\n* Back propagation is a process to make adjustment to the weights of a neural network in a way that minimizes the average squared error of the training data.\\n\\n* It uses steepest decent method. In each step it moves towards the direction that gives maximum decrease of the error.\\n\\n* In back propagation the error is propaged backward from the last layer towards the earlier layers. The adjustments made to the weights is proportional to the partial derivative of the error with respect to the weight.\\n\\n* The partial derivative is calculated using repeated application of the chain rule.',\n",
       "  'Algorithmus zum trainiern eines Feed Forward Netzes. Erwünschte Ausgabe für Trainingsdaten bekannt\\nBesteht aus 2 Schritten:\\nForwardpass: Berechnen des Fehlers an den Ausgabe Neuronen.\\nBackwardpass: Rückpropagieren des Fehlers durch rekursives berechnen des Fehlers der einzelnen Neuronen mithilfe des lokalen Gradienten und anpassen der Gewichte. Dies wurde hergeleitet mithilfe der Chain-Rule',\n",
       "  'The idea of back propagation method is to propagate error from ouput (final) layer backward to hidden layers, and adjust the weighs of neurond in hidden layer, based of this error. This is required because we do not have error information for hidden layers, only for output neurons. The error from output layer is propagated to hidden layers using idea from steepest descent method. Namely, local gradients are computed for each neuron in backpropagation, and these local gradients define how error changes, in terms of weights. Local gradients are derived from chain rule for each layer. The fact that local gradient for each hidden layer is derived based on local gradient of a previos layer, defines that as we propagate more and more in hidden layers of NN, the gradient of a error function vanishes, which means that as we go deeply back in NN, the change in weights is becominng smaller and smaller. This is a drawback of back propagation method. ',\n",
       "  'Learning rate controls the speed of the descent. When learning rate is low, the weight updation is overdamped and convergence is slow. When the learning rate is high, the weight updation is underdamped and a zigzagging behaviour is exhibited in the weight space. When the learning rate is too large, learning becomes unstable.',\n",
       "  'If learning rate is very smaller, then transition are over-damping, trajectory of weight vector follows the smooth path.\\n\\nIf learning rate is large, then transition are under-damping, trajectory of weight vector exhibits the zigzagging(or oscillatory) behavior\\n\\nIf learning gets higher than some threshold, then learning algorithm gets unstable or diverges',\n",
       "  'YOUR ANSWER HERE Learning rate n determines stride of delta of weight. If learning rate is too large weights starts to ziggerate. ',\n",
       "  'When training with SD, the learning rate determines the step size we take towards the negative gradient. When the learning rate is too small, the weights may be overdamped and reach the error function minimum slowly, eventually getting stuck in local minima. When step size is too big, the weights may be underdampened, bouncing between ridges of the error surface and never find the minimum (especially when the minimum is in a steep ravine of the error surface)',\n",
       "  '- Learning rate is used to control how much the wright update is affected by the error correction or so on.\\n- Learning rate too low: Learning is slow and takes more time\\n- Learning rate too high: Learning is fast, but causes zigzagging behaviour in convergence.\\n- If the learning rate is too high, it may result in situations where the zigzagging behaviour will cause it to overshoot, and may never finally converge.',\n",
       "  'The learning rate defines the speed of the weight change. A learning rate too high can lead to oscillation around the optimal weight such that its never reached. A learning rate to low results in very slow learning and slow convergence.',\n",
       "  'The learning rate is needet to make the algorithm more stable. \\n\\nA high learning rate makes the weightchanges zickzacking and the algorithm might not converge\\n\\nA low learning rate makes the path in the W-plane more smooth.\\n\\nIf the learning rate gets to a certan critical value the algorithm might not converge at all',\n",
       "  'The learning reate is a factor of how much we trust the datapoint. Normally it is in the range of [0,1]. A high learning rate normally results in a faster convergence while a lower rate in a slower conversion. If the rate is choosen to high, it is possible that the cost function diverges. If the rate is to slow it is possible that the rate so conversion is so slow that we never reach a local minimum.',\n",
       "  \"The learning rate is a parameter using on updating the weights in a given iteration. This parameter represents the importance that is given to the adaptation of the weights. So when setting the learning rate small, the learning machine will learn slower but also in a more stable way. On the other hand, when setting the learning rate with a large value, the learning machine will learn faster but in an unstable way. The danger here, is that depending on the learning rate's value, the algorithm may never come into the perfect value. If the learning rate is too small, it may land into a local minimum and never approach the global minimum of the function. If the learning rate is too big, the learning progression will have a zig-zagging behaviour and never approach the ideal value.\",\n",
       "  '- The learning rate defines the size of steps that the method moves in the search space.\\n- If the learning rate is too small the method needs to take huge number of steps and maybe it stuck in a local minima\\n- If the learning rate is too big the method will converge very fast toward the global minima but there is a probability that it oscilates around the global minima and never reachs it\\n',\n",
       "  'If learning rate is to large, then proccess will oscillate a lot and might not converge.\\n\\nIf learning rate is to small, then convergance will happen very slowly',\n",
       "  'The learning rate tells us how confident we are of the error, and it affect the convergence rate. A low learning rate will slow the convergence, making the system overdamped. A high learning rate will speed the convergence but the value oscilates, making the system underdamped. The system can become unstable if the learning rate is above a threshold value.',\n",
       "  '+ If the learning rate is too small, then the system is overdamped and the algorithm takes a long time to converge.\\n+ If the learning rate is too large, then the system is underdamped and the algorithm oscillates around and optimal solution or could potentially make the system unstable.',\n",
       "  'The steepest descent method is an algorithm for finding the nearest local minimum of a function which presupposes that the gradient of the function can be computed. The method of steepest descent starts at a point P_0 and as many times needed moves from P_i to P_(i+1) by minimizing along the line extending from P_i in the direction of gradient f(P_i) the local downhill gradient. The danger of the algorithm is, that it can get stuck in a local minima.',\n",
       "  'The learning rate determines the rate of learning: the smaller the learing rate is, the slower the learning process is, but the path of weight adjustment is smoother. The larger the value is, the faster the learing process is, but it can result in oscillation and instability. ',\n",
       "  'The learning rate is $\\\\eta$ So based on the learning rate, it undergoes various oscillation. \\nWe could see zigzagging behaviours. \\n1. When the learning rate is large, the system is said to be under damped. \\n2. When the learning rate is small, the system is said to be over damped. Here we can see a zigzagging behaviour towards the convergence phase. \\n3. After the learning rate crosses a certain value it becomes unstable. \\n\\nIt may stuck in a local minima which is considered to be another danger ',\n",
       "  'Learning rate in steepest descent can directly affect the convergence of the algorithm. If the learning rate is very small then algorithm can take long time to converge i.e response is ovderdamped. But if the learning rate is amde very high then we may observe zig-zagging (oscillatory) behaviour and sometimes algorithm may fail to converge  (underdamped response).',\n",
       "  'YOUR ANSWER HERE:\\n- When the learning rate is small, the learning is very slow.\\n- When the learning rate is large, the learning is unstable and can exhibit zigzag behavior.\\n- When the learning rate is too large, the learning never converges.',\n",
       "  'The learning rate defines the efficiency of learning machine. If it is small, the system response may be overdamped, if large , the response may be underdamped and if it exceeds a critical value, the response may diverge.\\n\\nThe danger is the possibility of the system output to not converge. This should be ensured by scaling the learning rate using the largest eigen value of the correation matrix of the input.',\n",
       "  'The value of the learning rate parameter $\\\\eta$ controls the speed of descent and convergence towards the optimal weight vector. For small values of $\\\\eta$, the transient response of the algorithm is overdamped and the weight trajectory follows a smooth path. On the other hand if the value of $\\\\eta$  is large, the transient repsonse of the algorithm is underdamped, and the weight trajectory follows an oscillatory path in the W-plane.',\n",
       "  'Learning rate $\\\\eta$ has a profound impact on the learning in steepest descent. \\n1. If $\\\\eta$ is too small, the system is underdamped and convergence is slow. \\n2. For larger $\\\\eta$, the system is overdamped and tends to oscillate.  \\n3. If $\\\\eta$ exceeds a certain critical value, steepest descent may even diverge!',\n",
       "  'Learning rate has huge impact on convergence of the network. If the learning rate is low then the transient response of the algorithm is overdamped and the trajectory of w(n) is smooth. If the learning rate is high then the transient response of the algorithm is underdamped and trajectory of the w(n) is zigzag. If we choose the wrong learning rate then  the network might not converge.',\n",
       "  'learning rate controls the speed and convergence of steepest descent method. 1. if it is small, the trajectory of weight vector follows a smooth path in W plane; 2. if it is large, the trajectory of weight vector follows a zigzaging path; 3. if it exceeds a critical value, then the algorithm is unstable.',\n",
       "  '1. Large learning rate $\\\\eta$ results in a zigzagging behavior but it can converge quickly.\\n2. Small learning rate $\\\\eta$ results in a smooth behavior but it is slow to converge',\n",
       "  '* Learning rate tells the network that how much steps it should move towards direction opposite to the gradient vector.\\n* If the learning rate is too large, the weight updation will be high. \\n* So the danger is, learning may oscillate or the network overfit the data.',\n",
       "  'Learning rate is used to regulate the speed of learning. If the learning rate is small then the learning is slow and if the learning rate is high then it oscillates. If it exceeds the critical value then the algorithm is unstable.',\n",
       "  'Learning rate is used to decide how fast the network should converge during the training phase\\n\\nIf the learning rate is too high - the system oscillates and  becomes overdamped\\n\\ntoo low - the system becomes underdamped and learns very slow',\n",
       "  'The learning rate tells how long one step in the method of steepest descent is. If the learning rate is too high, the learning will oscillate and may not converge. If the learning rate is too small the convergence will take many iterations.',\n",
       "  'If we use steepest descent we use the learning rate to adjust the speed of the convergence to a minimum error. If the learning rate is too small, the learning is going on rather slow. If the rate is high, the error is zigzagging on the error surface towards the minimum. If the learning rate is to high, it might not converge but diverge.',\n",
       "  'The learning rate is a value between 0 and 1, which determines how fast the network learns. When using small values for the learning rate, the network converges slowly and needs alot of processing. When choosing big values the learning oscillates and becomes unstable. The goal is to choose the learning rate in a way that it does not learn to slow, which needs more input data for convergence, and that it does not become unstable.',\n",
       "  'The learning rate defines the speed of the learning convergence. High values lead to faster learning und low values to slower learning. However, high values can lead to oscillations in the learning space and may overshoot the desired result and never reach it.',\n",
       "  'The learning rate gives the speed of learning. It defines the stepwidth in direction of steepest descent. If the learning rate is small, the learning is more stable but slower. When it is high, the learning is more unstable but faster. The danger is to overcome a minimum and result in oscillating behaviour ',\n",
       "  'A too small learing rate can lead to a very slow convergence or to no convergence at all if the time learn becomes too long. A high learning rate can lead to an oscillating behavior and prevent convergence.',\n",
       "  'Learning rate is a scalar multiplied with adjustment term to adjsut the weights. It ensures the rate of learning. It is typical greater than 0 and less than equal to 1. It govers the rate of sliding alond the curve towards the minima. \\n\\n1. Lower learning rate will result in slow learning but chances of finding optimal minima are greater. \\n\\n2. Higher learning will result in hopping on either side of minima hence zigzag behaviour.\\n\\n3. Very high learning may not converge. ',\n",
       "  'if the learning rate is large then the it follows the zizag motion.\\n\\nif the learning rate is too low then it takes time for converging .\\n\\nif the learning rate is very large or critcal then it becomes unstable.\\n\\nwhile processing there is possiblity that it will get stuck in local minima.',\n",
       "  'When using steepest descent the learning rate($\\\\eta$) determines the speed at which the weihts are adjusted in the NN. There can be two possible danger related to leraning rate depending on its magnitude:\\n1. Low learning rate(eg, $\\\\eta = 0.01$)  results in smooth variation of the weights but makes the process becomes slow.\\n2. Hight learning rate (eg, $\\\\eta = 0.01$) results in faster weight adjustment but it leads to an oscillatory nature in the learning which is unwanted.',\n",
       "  'YOUR ANSWER HERE\\n\\n* With a small learning rate the network will converge very slowly towards the optimal weight of the network but it will give better perfomance in generalization.\\n\\n* With a high learning rate there can be zigzag effect. because of the large rate the network may miss a local minima and jump to a higher point.\\n\\n* With a very high learning rate the network may become unstable.',\n",
       "  'learning rate zu klein: Convergenz wird nur sehr langsamm erreicht.\\nlöearning rate zu groß: Zig zaging um minima herum, Convergenz wird unter umständen nie erreicht.',\n",
       "  'The learning rate defines the speed of steepest descent search for min of a error funtion. In other words, it defines how strong the change in weights will be, throughout optimization procedure. Higher learning rate, faster learning, but then learning is characterized by oscilations in searhc for min. This is dangerous because if learning rate, becomes bigger that a certain value, it can make search with steepest descent unstable. IN this case steepest descent will start to diverge, istead of converging to min. \\nIn other case, when learing rate is small that lerning is slower but safer, and the learining path is not oscilatory.',\n",
       "  'The reduced blotzman machine works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.',\n",
       "  'It is a recurrent network. It opreates by flipping. \\n\\nIt has two groups of neurons: Visible neurons and hidden neurons. Visible neurons provides interaction between environment and network. Hidden neurons are running freely. \\n\\nIt has two modes of operation: \\n    . Clamped State: states of the neurons are clamped.\\n    . Free running state: Neurons are running in free condition',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'RBM implement a combination of graphical and probabalistic ideas, using probabilites of activations inspired from energy based networks. We present a training input to the RBM, and determine the hidden activations based on a probability of net input and edge weights. Then, when unclamping the training data from the network, sample from the distribution of the hidden layer, where the RBM tries to rebuild the distribution of the input data. RBM may be used for data completion or denoising, where e.g. incomplete images are complted based on the learned probability distribution.',\n",
       "  'RBM has two layers and are interconnected (recurrent) operates by flipping the internal states (+/- 1)> Unlike the boltzmann machine, reduced boltzmann machine does not contain interconnections among the same layer. The weight update is done by the differnce in correleation in clamped and  free running mode.',\n",
       "  'It consists of only two layers: input and hidden layer. During training data is presented to the input. The hidden layer starts oscillating.',\n",
       "  'The Reduced Boltzman Machine is an stochastical recurrent ANN, that operates with two classes of neurons : hidden and visible. It operates by neuron-flipping with a probability impacted by the neurons arount. So it uses the hebbian rule. An Reduced Boltzman Machine can learn the classify data and can repoduce the learned patterns.',\n",
       "  'The structed of RBM is a bitpartied graph. It uses hebbian learning for training and the neurons used are binary stoachastic neurons, which have a binary state, which fire based on a probability. The training is achived by passing the information a many times between the hidden layer and the input layer. There weightsare updated on the pass into the hidden layer. Weigths between input and activations in the hidden layer are increased, weights between gernerated inputs of the rbm and the hidden layer are decreased.',\n",
       "  'The main idea of an RBM can be defined as follows:\\n\\n- Two layers will be defined, where each neuron will be connected to every neuron of the other layer.\\n- The input will be passed from the first layer to the second one, and the state of each neuron of the second layer will be calculated.\\n- The neurons with active states will pass again its values to the input layer.\\n- The values given from the second layer will be compared with the input values, and with the two states, the weights will be adjusted.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'They are neural network with only one hidden layer, neurons from input to hidden layer are fully connected, neurons from hidden layer to output layer are fully connected as well. ',\n",
       "  'The Reduced boltzman machine works by flipping neurons. It can operate in clamped or free running state.\\n- If two connected neurons are activated at the same time, the weight is increased.\\n- If any of the two neurons are fired asynchronously, then the weight is reduced or removed.',\n",
       "  '+ Reduced Boltzman Machines (reduced because inputs do not share information via synapses) are one of the initial NNs which consists of input layer and hidden layer. The system adapts its internal weights and tries to reproduce the inputs.',\n",
       "  'A RBM is a shallow two layer network containing a visible and a hidden layer. Each noden in the visible layer is connected to each node of the hidden layer. It is considered as restricted, because no two nodes of one layer share a connection. A RBM is the mathematical equivalent of a two way translator. In the forward pass a RBM takes the inputs and translates them to a set of numbers that encode the inputs. In the backward pass it takes the set of numbers and translates them back to form the reconstructed inputs. A well trained RBM will be able to perform the backward translation with a higher degree of accuracy. \\n\\nThree steps are repeated over and over through the training process:\\n\\n1) Forward pass.\\n\\n2) Backward pass.\\n\\n3) Evaluate quality of reconstruction as visible layer (often solved with KL divergece)',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'RBM is an unsupervised learning technique. It has visible neurons and hidden neurons. Neurons are in either +1 or -1 states. It uses the idea of simuilated annealing to flip the neuron states based on energy function and pseudo temperature. It operates in 2 states - clamped state and free flowing state. In clamped state only hidden neurons are flipped and in free flowing state both visible and hidden neurons are flipped. Weights are adjusted based on avergage correlation difference between all the neurons in clamped and free flowing state.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  \"RBMs work on the principle of binary states, free-running or clamped. The weight update is done based on the Botlzmann's formula using the pseudotemperature, which gives the proobability of error.\",\n",
       "  'The Reduced Boltzman Machines function by using two types of neurons : visible neurons that provide an interface between the environment and he network, an hidden neurons that operate freely. The learning can proceed under two conditions, namely:\\n1. Clamped state : where the visible neurons are clamped to a particular state of the environment\\n2. Free running state : where both visible and hidden neurons operate freely. \\n\\nIf $\\\\rho^+_{ij}$ indicates the probability of correlation between the states of neurons i and j in clamped state, $\\\\rho^{-}_{ij}$ indicates the probability of correlation between the states of neurons i and j in free running state, then the weight adjustment $\\\\Delta w_{ij} = \\\\eta (\\\\rho^+_{ij} - \\\\rho^{-}_{ij})$',\n",
       "  'A Reduced Boltzmann machine (RBM) consists of two layers of neurons: visible and hidden. The neurons may only have two states i.e. activated or not and they flip according to a certain probability based on the weights and states of other neurons. The RBM has two modes:\\n1. Clamped: The visible layer is clamped to a certain input while the hidden neurons are allowed to change state until the network settles. The correlation in this state is given by $\\\\rho_{ij}^+$\\n2. Free-running: In this state, the network is allowed to flip all neurons until it settles. The correlation is $\\\\rho_{ij}^-$  \\nThe weight update rule is given by \\n$$\\\\Delta w_{ij} = \\\\eta (\\\\rho_{ij}^+ - \\\\rho_{ij}^-)$$',\n",
       "  'Boltzmann machines is a neural network having recurrent structure.It is in two states either on which is +1 or off which is -1.The energy function is given by \\n\\n$E = 1/(1+exp(-delta E/Temperature))$\\n\\nThe state of the input x is turned from +1 to -1 based on the change of the energy delta_E and the pseudo temeperature T.',\n",
       "  'The neurons operate in a binary states, \"on\" or \"off\". In clamped condition, all visible neurons are clamped into specific states by the environment; in free running condition, all neurons including visible and hidden neurons operate freely.',\n",
       "  'It uses an energy function to oversee the learning process',\n",
       "  'Reduced boltzman machine work based on **flipping operation** and calculating the probability invariances of clamped state and freely running state.',\n",
       "  'RBMs run on boltzmann learning rule. The neurons have 2 modes of operation clipped and free running.\\n\\nAll the neurons are binary units. Their status can be changed by flipping. All the neurons that are in on position are clipped together.',\n",
       "  'It has the structure of recurrent neural network. \\n\\nIt has two layers of neuron visible and hidden.\\n\\nthe neuron can store only binary values\\n\\nthey work based on flipping\\n\\ntheere are modes free running and clamped\\n\\nthe weights are changes based on the correlation of the neurons in the free running mode and clamped mode',\n",
       "  'In a Reduced Boltzman Machine there are one visible and at least one hidden layer. The visible layer is the input and acts as output at the same time. For each input the neurons of the visible layer will be assigned with a value. With their weights, hidden neurons may either be activated or not. Once the input has been passed through the hidden layers, the values are passed all the way back to the visible layer. For this, different weights are used since the values move in the opposite direction. ',\n",
       "  'In RBMs there are two states, the free running and the clamped state. During the clamped state, the input neurons are clamped to the output neurons. While the network is clamped the probabilities of the Hidden states to be in a certain state are calculated to determine a probability of the output to be correct.',\n",
       "  'The Reduced Boltzman Machine hast an input layer and a hidden layer. Each neuron has a state and a probability to turn on. If the neuron turns on the data passes trough it and the weights are updated. The probability of turning on is calculated by the network.',\n",
       "  'Two fully connected layers, one input and one hidden layer are used. The input layer is the only connection to the environment. The RBM has a specified energy level which can not be changed. However the distribution of this energy to the nodes can be changed. Based on the data input every node has a chance to flip based on its input connections.',\n",
       "  'the binary state of each neuron is flipped by a given probability. Stochastical learning ',\n",
       "  'Neurons have to states e.g. on or off. Each neuron has a probability to flip from one state to another. ',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'the main idea of the RBM is compute the Least mean square error of the difference between expected output and real output.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'YOUR ANSWER HERE\\n\\n* It is a Recurrent neural netwokr\\n* It uses two groups of neurons, hidden and visible\\n* It process the training data by flipping the neurons',\n",
       "  'Boltzman lernen ist äquivalent zum simulierten abkühlen.',\n",
       "  'Reduced Boltzman Machine is a biparted (two parts) Reccurent NN, that has two layers visible and hidden layers. In Reduced Boltzman Machine neurons can have two states, namely, + or - 1, depening on current time step. At each time step, the states of neurons are flipped. Here the visible layer represent interface for connection between evironment and hidden layer, and it operates in clamped mode (limited values by environment). WHile hidden layer, operates in free mode.',\n",
       "  'Echo State Network is a type of Recurrent Neural Network and has atleat one cyclic (feedback) connection. ESN consists of a dynamic reservoir and a output layer with neurons. The dynamic reservoir consists of randomly initialized neurons with random sturcture and connections (with atleast one feedback connection). The output layer combines the dynamic behaviours of the reservoir in a required fashion. Only the weights of the output neurons are updated while learning.\\n\\nAn ESN consists of feedback connections while a FF NN does not.\\n\\nAn ESN could have persisting activations even when there is no input which is not the case in FF NN.\\n\\nAn ESN can approximate dynamic systems while a FF NN cannot.',\n",
       "  'Echo State network are recurrent neural network, which means these networks have feedback. While, in feedforward neural networks, there is no feedback. In feedfoward, training data or inputs are not dependent on each other. They do not have any system memory. In ESNs, training inputs are dependent on each other and they have system memory\\n\\n\\nIn Echo state network, there are fixed, random generate reservoir weights. These weights are not trained. While, only output weights are trained',\n",
       "  'YOUR ANSWER HERE An ESN is a recurrent neural network with many layers and fixed weights. There are several differences. An ESN has a cycle that means witin the network there are backwarded connections. Withn a FF NN there are only feedforwad connections. Within a FF NN all weights are trained. Within an ESN only output weights are trained. An ESN can produce an output without any input. A FF NN needs an input to produce an output.',\n",
       "  'ESN are different to FFNN in so far that they consist of a reservoir of hidden neurons, which may be connected recurrent, as opposed to having a feed forward architecture. Here, the inputs are connected to the recurrent dynamic reservoir, whereas the DR is connected to the linear output layer. The Output layer may be again connected to the DR, whereas during training only weights of the last layer are learned. Weights of the DR of the ESN are thus initialized and never learning, although since have been extended to minimal complexity architectures.',\n",
       "  'ESN are recurrent neural networls with a large reservoir (or echo chamber) with many nodes (recurrent). The weights are learnt only for the connection between this reservoir and the output layer. The weights are not learnt for the nodes inside the reservoir. The main idea is that during training, the input layer cuases the states inside the reservoir to behave in caertain way, and the weights in the output layer is adjusted to match this and the labelled output.\\n\\nFFNN are feed forward networks, i.e., they do not have any recurrent connections, which is the main difference with respect to ESN\\n\\n',\n",
       "  'ESNs are a special class of recurrent neural networks. In contrast to ff they also allow backward node connections and thus are able to memorize data.\\nThey are defined by: $x_i$ input i, $y_i$ output i, a dynamic resaviour, and weights connecting all the components.\\nThe dynamic resaviour is generated randomly and fixed. Its topology including weights is never changed.\\nOnly the weights between output layer and dynamic resaviour are changed during training. Because the dynamic resaviour allows all kinds of connections between its nodes it can contain memory that is able to remember data. It also has a spectral radius.',\n",
       "  'An ESN is a recurrent ANN with randome, sparse and fixed interneuron connections in the hidden layers. Just the output layer weights get trained, because the network itself is so complex, it can model very much. If the training was not successful we can just create a new randome ESN. Training an complete ESN would by very complex and would take very very very long.\\n\\nA FF NN is not recurrent (no feedback) and all its weights get trained and most of the time the interneuron connections are not sparsly.',\n",
       "  'A Echo State Network is a RNN, is has a dynamical reservoir of neurons which are connected with each other and itself. the DR typically consists of more that 100 neurons. The outputlayer consists of linear readouts of the DR. So a neuron in the output layer sums up the weighted behaviours of the DR neurons. The DR is randomly initialised and only the output layer is trained by supevised learning. \\n\\nThe main Diffrence is that ESN is a RNN. In contrast to FFNN it can resemble any dynamical system. Usually it is used for time series prediction.',\n",
       "  \"Echo State Networks are a type of recurrent neural networks, where the input layer is interconnected to a reservoir (a random initialized group of neurons with also random interconnections), and this reservoir is connected to the output. The reservoir will not be adjusted, but the output weights. The output weights can also have recurrent connections with the reservoir. The states on the reservoir neurons will be calculated, and with these states and the output weights, the output will be extracted. \\n\\nThe main difference with the Feed Forward Neural Networks (FF NN) is that in the FF-NNs there's no recurrency, so the input values will be passed to the next layer.\",\n",
       "  '- In the ESN we have a huge recurrent network which is called \"Dynamic Reservoir(DR)\" and we have an output layer connected to this DR and we will train the network by adapting and manipulating the connection weights just to the output layer\\n- Unlike a feedforward network in a ESN because of the DR we have at least one loops that returns the output of a neuron with some time delay therefore we have memory in our network but in FF NNs we don\\'t have any memory',\n",
       "  'Echo State Networks are recurrent neural network type, meaning there are feedbacks in its structure. It is usually only 1% connected. Main difference is that it has a reservoir as a hidden layer where neurons are very randomly connected, with random weight etc. During learning phase only weight outputing neurons are changed. It is required more that 100 neurons to be in a reservoir.',\n",
       "  'The ESN is a type of neural network model that uses a recurrent neural network as a large, random, fixed dynamic reservoir that remains unchanged during training and only changes the weight of the reservoir to output layer. \\n',\n",
       "  'ESNs are a form of recurrent neural networks with a least one recurrent input. The ESNs are reservior computers which have memory and can be activated without the inputs. In ESNs, instead of training we evolve the network state by feeding it input sequence.\\nESNs are different from FF NNs because ESNs contains at least one recurrent connection (feedback).',\n",
       "  'The basic idea of ESNs is to use a large, random, fixed recurrent NN (referred to as dynamical reservoir) and to train only connections from the reservoir to the output.\\n\\nThe main difference to FF NN lies in the recurrent part of the network, where back passes are built in, giving feedback previous layers. It is not possible to maintain the reservoir beforhand so it suits the given problem. There is a lack of investigation of reservoir construction. ',\n",
       "  'An Echo State Network (ESN) is a modified version of a recurrent network. It has a reservoir, which is a large number of hidden neurons with sparsely-connected random and fixed weights. To train an ESN, only the weights connecting the reservoire and the output layer are adjusted; therefore, the efficiency is better than a normal recurrent network. ',\n",
       "  \"Echo state network provides structure and supervised learning for recurrent neural networks. It mainly 1) Directs the fixed, large reccurent neural netorks by providing an input stimuli and also fix a response signals to the neurons which are present inside the reservoir(Pool of neurons) 2) It can be directed to get the desired response by the trainable linear combiner of the response signals. \\n\\nUnlike FF NNs, ESN's have memory. They can be also activated without an input stimuli, whereas in case of FF NN, they require a external stimuli so that they are activated. Also the neurons needs to connected in one full cycle. \",\n",
       "  'Echo State Newtors are type of RNN. It has dynamic reseivoir units which exhibits different dynamics. Weights of these reseivoir units are fixed and are not changed during the training phase. Only the reseivoir to output weights are changed to learn the inputs. These networks converge only if reseivoir units exhibitg echo state property i.e its ouput depends only on the previous inputs. this property is satisfied if spectral norm reseivoir weights is less then 1.',\n",
       "  'YOUR ANSWER HERE:\\n- Echo state networks are recurrent neural networks that have a large resorvoir of oscillator functions that are connected to the input layer.\\n- In FF NNs, consideredthe outputs at the hidden layers are also considered but in ESNs, the ouputs from the reservoir to the final output layer are only considered.',\n",
       "  'ESN are another implementation of RNNs where training method is completely different. They comprise of a dynamic reservoir with fixed hidden to hidden connections which makes up an RNN with sparse connetivity. Only the output weights which connect the dynamic units and the output of the reservoir are trained using error, unlike RNNs, where the hidden weights are also trained. ESNs are less compuationaly expensive since they can be easiliy trained with experimentation .However, RNNs use much less hidden units compared to ESN for a similar task.',\n",
       "  'An Echo State Network (ESN) is a neural network that uses a reccurent neural network (RNN) as dynamic reservoir which is not changed during training, and trains only the connection from the dynamic reservoir to the output layer. An echo state network is different from FF NNs due to the presence of feedback connection with the dynamic reservoirs which enables it to maintain activation even without inputs. Each unit within the dynamic reservoir in ESNs are excited differently to different inputs.',\n",
       "  'ESNs are recurrent neural networks with at least one cyclic connection and are based on the concept of reservoirs. In contrast FF NNs do not have any cyclic connections. Additionally, in ESN the output weights are trained but the reservoir weights are not whereas in FF NNs all weights are trained. The ESN has memory while FF NNs do not have memory.',\n",
       "  'ESN refers to echo state networks. Echo state networks are the recurrent neural networks where the hidden to hidden layer weights are selected randomly and are fixed and hidden to output layer weights are changed by the learning process.Since ESN is recurrent neural network hence the output echoes throgh the network even when there is no input where as in ff nets there is no feedback so there is no output if there is no input.',\n",
       "  'ESN is a kind of Recurrent NN, which has a large, random , fixed RNN called dynamic reservior and only the weights connecting the reservior and output layer are trained. So ESN combine the desired system function and input/output history echo function.',\n",
       "  'ESN provides an architetcure of supervised learning principle for RNNs. It is different from FF NNs, because it has a reservoir (based on RNNs) to find a non linear signal response and combine the desired output by a trainable linear combination of these response.',\n",
       "  '* Echo state networks are recurrent neural netwoks with **Dynamic Reservoirs.**\\n* Weights initialized in the dynamic reservoris will not be updated during training.\\n* Only the weights in output layer (readout states) is updated after each iteration.\\n* In FF NN, all neurons are connected with other neurons in next layer and all the weights are updated in each iteration.\\n* But in ESN, the **neurons are connected randomly** with other neurons and it is **recursive** and the **weights are not updated** in the dynamic reservoir.',\n",
       "  'ESN is a type of RNN. It has a dynamic reservoir. All the neuron are connected to each other. ',\n",
       "  'The Echo state netwrork has a large number of recurrent neural network in them. this set of RNN is called the dynamic reservoir. \\n\\nThey can approximate any dynamic model \\n\\nthey train the model by changing only the weights of the connection of output of the dynamic reservoir and output of the network\\n\\nFF: \\n\\nthey can approximate any continuous function\\n\\nThey train by adapting all the weights in the network',\n",
       "  'An echo state network contains of an input layer which is connected to a reservoir, which is a big recurrent network. The output layer is connected to the neurons of the reservoir. While learning in an ESN, only the weights between the reservoir and the output layer are changed, no changes within the reservoir.\\n\\nDifferences to feed forwared networks are, that the reservoir is recurrent and that during the training not all weights are changed, but only the ones between ouput layer and reservoir.',\n",
       "  'In contrast to regular feedforward networks, ESN belongs to the group of Recurrent Neural Networks. It has a regular input layer like the FF, then comes a dynamic reservoir, which is a layer of neurons, where at least one full cycle of connections between the neurons is given. The connections inside this reservoir are not constrained and can thus be any possible connection. This reservoir is randomly initilaized and kept that way. Only the respective connections to the output layer are trained during the learning process. ',\n",
       "  'ESN have an input layer connected to a reservoir, which is a recurrent neural network. The reservoir is connected to the output layer. On the connections to the output layer are weights, which are updated by the network. The weights of the reservoir are chosen randomly and not updated at all.',\n",
       "  'An ESN is a recurrent neural network, that consists of an input layer, a dynamical reservoir and an output layer. In the dynamical reservoir feedback loops are possible in contrast to a feedforward network. However, this dynamical reservoir is only randomily initialzed and not learned. Only the connections to the output from the reservoir are learned. Normally, in FF NNs all connections are trained.',\n",
       "  'echo state networks have dynamical reservoir as hidden layer. The dynamical reservoir consists of recurrent non-linear neurons. Only the linear connections from dynamical reservoir to the output layer are trained. The difference to FF NN is, that the ESN is a recurrent network',\n",
       "  'The core of an ESN is an arbitrary network with recurrence.',\n",
       "  'Echo state networks have dynamic reservoir with echo state property which is a randomely initialized RNN. \\nHence it can maintain its own internal state. Which is not possible in FF NN. RNN have feedback connections which ecoes back the state of reservoir as well as previoulsly applied inputs. Hence it can model dynamic systems which not possible with FFNN. ',\n",
       "  'ESN are the RNN recurrent neural network which has at least one feedback cyle. \\n\\nFF NN are normally forward moving networks where the input from one layer is fed into next layer and generated the output . \\n\\nbut IN ESN the out put is again fed back as input . ESN is tend to have Resvoir where its randomly connected.',\n",
       "  'Echo State Network is a type of neural network which has a recurrent network of 100 to 1000 neurons called dynamic reservior, as the hidden layer. The weights are choosen randomly. The synaptic weights from the resorvoir to the output layers are only adjusted during the learning process.  \\n\\nThey are different from the FF NNs in the following regards:\\n1. ESN have atleat one loop wheras the FF NNs dont.\\n2. Only the output weights are adjusted in ESN , in FF NNs both the input and output weights are adjusted.\\n3. ESN s have a memory, FF NNs dont.',\n",
       "  'YOUR ANSWER HERE\\n\\n* ESN uses a large set of recurrent neurons called reservior.\\n* The weight of reservior neurons does not change after initialization.\\n* The network only lears the weight of reservior to output.\\n* It works very well for one dimentional time series data\\n\\nThe Feed forward networks works differently. The input is feed through the network layer by layer and error is propaged backward to make the adjustments till the first layer. In case of ESN the adjustment is made to the reservior to output weight only.',\n",
       "  'ESN besteht aus eingabe Schicht, einem pool und einer Ausgabeschicht.\\nDie eingabe Schicht ist verbunden mit dem Pool, optional auch mit der Ausgabeschicht. Der Pool ist Recurrent verbunden mit sich selbst und forwärts verbunden mit der Ausgabeschicht. Optional kann auch die Ausgabeschicht Rekurrent mit dem Pool verbunden sein. Nur die Verbindung von Pool zur Ausgabeschicht wird verändert der rest ist statisch.\\n\\nUnterschiede:\\nFFNNs haben keine Recurrenten Verbindungen.\\nBei FFNNs werden alle Verbindungen gelernt/verändert\\nBei FFNNs wird das wissen in form der Gewichte gespeichert, bei ESN auch durch den Zustand des Netzes',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'In a Convolutional Neural Network, the layer order is:\\n\\n1. Convolutional layer (has kernels which convolve over the input image incase of first layer or feature maps otherwise).\\n2. Activation layer (ReLU activation).\\n3. Pooling layer (max or average pooling). These 3 layers can be repeated any number of times.\\n4. Finally one or more fully connected layers followed by softmax layer.\\n',\n",
       "  'In CNN, there are mainly three layers:\\n\\n    i. Convolution Layer: It is used to capture the low-level and high level features using kernal over the image.\\n    \\n    ii. Pooling Layer: It is used for dimensionality reduction, and for translation invariance\\n    \\n    iii. Fully Connected Layer: This layer is same as regular NNs, where all the nodes are fully connected with each other. There is mostly sigmoid activation function is used to compute the probabilities of each output/class.\\n    \\n    Furthermore, In CNNs, we use Rectified linear unit(ReLU) activation function  ',\n",
       "  'YOUR ANSWER HERE A Convolutional Neural Network has a kernel which is much smaller than the input. This is why it can operate much more efficient than a normal neural network. Normal Neural network O(n \\\\times m), convolutional neural network O ( n $ \\\\times $ k), k is much smaller than m. A convolutional Network operates no large images. The input is preproessed in many layers before it is given to a normal neural network. Preprocessing transforms input into a linear separable problem.',\n",
       "  'CNN learn on grid data (images, 3d volumes) using filters instead of matrix multiplication. Here, the filters are convoluted with the input in the Convolution layer per neuron, where we slide the filter (defined by fiter size $S\\\\times S$) over the input with a stride (step size), and optional zero padding. Strictly speaking, since for RGB images we are working have three color channels, we work with volumes of filters (For example for RGB images of size $32\\\\times 32\\\\times 3$, a filter of window size $S=5$ has the dimensions $5\\\\times5\\\\times3$). Instead of learning a volume of weights for each convolution step, we share weights, considering that one feature detected in one part of the image may be of interest in another part. Then, we apply a nonlinearity, commonly the ReLu activation, as to introduce nonlinearity into our model. To reduce spatial size of our input, we can either use higher strided convolutional layers or pooling layers, for example the popular max pooling layer, where the maximum value over a subvolume is picked. These layers are then stacked, while in the last layers fully connected neurons are typically used to reduce data to for example a classification vector.',\n",
       "  'A CNN uses convolution instead of matrix multiplication. After this there is a non linearity which may be a function like ReLU. There is also a pooling stage which is used to pool the important features.\\n\\nCNNs are translation invartiant.',\n",
       "  'A convolutional neural network consits of convolutional layers.\\nA convolutional layer applies one or multiple kernels (matrix) to an input vector/matrix (typically image) instead of connecting all single inputs of the input vector to the next layer with seperate weights.\\nInstead in training only the kernel is updated. \\nAfter a convolutional layer there is typicall a pooling layer.\\nGiven a window size it reduce the dimensional size of the output of the convolutional layer by using e.g. max or avg pooling.\\nAfterwards the activation layer applies an activation function to the output of the pooling layer.\\nIn the end of a cnn there are typically some fully connected regular layers resulting in a softmax activation function, which assigns the probabilities to the classes output.',\n",
       "  'An Convalutional neuron network assumes the input is an image. Because of that it has a achitecture, so that there are (abwechselnt) covalution and subsampling layers. After the last subsampling layer there is a normal FF NN which classifys the input.',\n",
       "  \"A CNN typically consists of multiple CNN layers and a few fully connected FF Network Layers. I'll assume the fully connected part is not so relevant to this questions.\\n\\nA CNN layer is typically a convolution layer and a pooling layer\\n\\nIn the convolution layer a kernel is convolved onto the input. If zero padding is used the result is in the same dimensionality. Depeding on the Kernel the convolution can be 1, 2 or 3D. \\n\\nIn the Pooling layer the result of the convolution is reduced to focus ont the importan features. It also helps on translational invariance.\",\n",
       "  'A Convolutional Neural Networks has the following structure:\\n - The input is defined in a grid, so any image or video sequence will be used.\\n - A several number of convolutional layers, where also subsampling (pooling) can be used.\\n - In the convolutional steps a filter will be used for each layer.\\n - After applying multiple convolutional layers, a normal feed-forward networks can be applied, where for example a back propagation algorithm can be used for updating the weights in the numerous iterations.',\n",
       "  'A CNN network consists of:\\n    - Input layer\\n    - conolution layer\\n    - Detection layer\\n    - Pooling layer\\n    - Next layer(because CNN consists of many layers this will be another block of layers similar to what described)',\n",
       "  'Convolutional neural networks are so that first layer is not fully connected but in a way that neuron connections overlap, leading to a grid type structure with overlapping circles. Another layer is connected only with nodes that are responsible for a particular feature (convolutions), then next layer is choosing wich of those convolutions from each ensemble is the most apropriate, after that next layer is fully connected to output neurons.',\n",
       "  '- The CNN has an input layer\\n- The input layer is connected to a convolution layer consisting of three phases:\\n    - convolution stage\\n    - Detector stage\\n    - pooling stage\\n- The next layer (can be a traditional FFNN)',\n",
       "  'CNNs are feed forward neural networks which replaces matrix multiplication task with convolution operation which is much sparse. CNN contain followng stages:\\n+ Convolution (learns local features)\\n+ max pooling (coarse-graining to learn better abstraction of input image)',\n",
       "  'In comparison to other NN, in CNN matrix multiplication is replaced with convolution. Everything else remains the same.',\n",
       "  'An CNN (covolutional neural network) contains a set of hidden layers for feature extration (convolutional layers, pooling layers), and fully-connected layers that classifies the features. The covolutional layers are used to carry out the covolution between the incoming signals with a set of filters, resulting in a set of feature maps. The pooling layers are used to reduce the dimensionality of the feature maps, and make the features invariant of rotation or displacement.',\n",
       "  'A CNN \\n1. starts with a input, where we perform the convolution, which provides a piece of activation. \\n2. Next it is being sent through the activation layer otherwise known as the detection layer. \\n3. Then the final stage is the pooling. ',\n",
       "  'In CNN we have different Kernels which are used for extracting certain properties of the inputs. These are called feature maps. After this there is a detection phase which introduces non-linearity. Further there is pooling which introduces translational invariance. There can be many such layers of feature maps and pooling. Finally its reduced to single row input and trained using traditional methods like Back Propogation algorithms.',\n",
       "  'YOUR ANSWER HERE: Convolutional neural networks have 4 main layers where input layer is connected to convolutional and subsampling layers followed by another set of convolutional and subsampling layers connected to the output layer. They are designed to specifically recognize 2-d shapes are invariant to skewing, rotation and the actual location of the object.',\n",
       "  'CNN comprises of multile layers of neurons which perform specific tasks. The initia layer is the convolution layer which performs convolution of the input with the elements of a given kernel. Simpler tasks such as edge detectoíon are performed. Detector layer forms a seconf layer here the output of convolution layer if fed through an activation function such as ReLU. Further, the data is pooled in the pooling layers where downsamping is done to reduce dimensionaity. These layers are repeated to perform more complex feature extraction operations.',\n",
       "  'A CNN is a neural network that replaces matrix multiplication with a mathematical operation called convolution in one or more layers. The main idea behind the structure of a CNN is to replace the activation of neuron with a flipped filter (Convolution layer) and then apply another function called pooling to adust the output further.',\n",
       "  'A CNN consists of several stacked Convolutional layers which can be separated by other layers such as Pooling, Activation, Zero-padding and Dropout which is a form of Regularization. The output layer is generally dependent on the task but could be a Softmax Activation from a Fully connected (also called Densely connected) layer. The number of outputs is usually the number of classes.',\n",
       "  'The structure is as follows:\\n-Convolution: In this layer convolution takes place instead of matrix multiplication\\n-Deconvolution: In this layer deconvolution takes place , by matrix multiplication\\n-Average weight layer: This is a max pooling layer ',\n",
       "  '1. first stage, the layer performs several convolution parallel to produce a set of linear activation\\n2. detector stage, each linear activation is run through a non-linear activation\\n3. third stage, use a pooling function to modify the output of layer.',\n",
       "  '1. Convolution or matrix multiplication: it produces output to hidden layer\\n2. Deconvolution matrix multiplication by transpose matrix: apply back propagation error for output to input.\\n3. Weight update: apply back propagation error from output to weight.',\n",
       "  '* Input layer\\n* Convoluton layer (Affine transformation)\\n* Filtering layer (Sampling)\\n* Learning layer\\n* Output layer',\n",
       "  'CNN has basically four types of layers. They are: convolutional layer, ReLU layer, Pooling layer and the fully connected layer.\\n\\nWe can arrange the convolutional layer and ReLU layer in different ways.\\n\\nOne of the ways is to have 1 convolutional layer, 1 Pooling layer, 1 ReLU layer and repreat this 3 layers again and then finally a fully connected layer.\\n\\nAnother way is to have 1 convolutional layer, 1 pooling layer again repeat the convolutional and pooling layer and then 1 ReLU layer and finally fully connected layer.\\n\\nConvolutional layer is used to find the feature space.',\n",
       "  'THe CNN will have a \\n\\ninput layer\\n\\nconvolutional layer - Here the convolution and sub sampling of the feature maps take place\\n\\nFeed Forward - Neural Network layer\\n\\nOutput layer',\n",
       "  'A convolutional neural network uses the steps of convolution and subsampling alternating in the beginning. Using different kernels during convolution, many feature maps are created. The subsampling step merges the maps to reduce their amount. After some of these steps, a classical feed forward network is in the end to transform the different feature maps to one output layer.',\n",
       "  'A Concolutional neural network has alternating layers of convolution and pooling. The convolutional layer is applying a filter to the input, while the pooling layer sub-samples the input. In some networks this is replaced by strided convolution, which combines these two steps into one. The structure at the end of a CNN is equal to that of a regular feedforward network.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'A basic CNN can be structured into the three layers convolution, detector and pooling.\\n\\nIn the first layer the convolution operation is performed on the inputs.\\n\\nIn the second layer the the activation function, mostly ReLU, is applied to the result of the convolution.\\n\\nThe last layer can be used to reduce the size of the resulting convoluted images, e.g. by max pooling.\\n',\n",
       "  'Convolutional Neural Network\\n\\nit has often images or video sequences as input. the input is computed by convolution (with different kernels) and downsampling in many steps to smaller but many more input matrices. In last step the matrices are connected to a classical FF NN.  ',\n",
       "  'A CNN conists of one or more convolution layers as well as subsampling or pooling layers followed by a fully connected standard FFN. In the convolutution layer kernels are used to create feature maps. A kernel is smaller matrix that is apllied to all possible positions on the input matrix. In the pooling stage the dimension of the rfeature map is reduced. for example by max pooling. ',\n",
       "  'CNN uses convolutional layers to extract primitive information from pattern. \\n\\nFirst data is convolved with the first layer to extract some features.\\n\\nOutput of this layer is passed through RELU function to rectify it. \\n\\nThen is downsampled by pulling layer. It basicaly chooses only relevant outputs of convolution layer for further processing. \\n\\nRELU is chosen instead of sigmoid because it doesnt allow gradient to vanish in backpropogation. \\n\\n',\n",
       "  'CNN is has multiple layers and they dont use multiplication matrix. ',\n",
       "  'Convolutional Neural Network(CNN) has three main layers in them\\n1. Convolutional Layer\\n2. Pooling or Subsampling Layer\\n3. Output layer.',\n",
       "  'YOUR ANSWER HERE\\n\\nCNN has three components,\\n* Input\\n* Convolution stage\\n* Feed forward network\\n\\nIn CNN the input pass through one or more convolution stage befor it is feed into a feed forward network. The convolution stage uses a hierarchical set of filters, RELU and polling to extract low level as well as high level concepts from the input. The feed forward network along uses the output of the convolution stage and back propagation is used to make adjustment to the network weights as well the filters in the convolution stage.',\n",
       "  'Eingabe eine große Menge. Danach kommen mehere Schichten. In jeder Schicht gibt es parallel unabhängige Netze, dabei wird jeweils eine Teilmenge der Eingabe an eines dieser Netze weiter geleitet, diese teilen jedoch die Gewichte. Durch Pooling wird am Ende der Schicht die Datenmenge der Eingabe reduziert und an die nächste Schicht weitergegeben. Am Ende werden alle parallelen Netze mit einer Ausgabe Schicht verbunden. Jedes der parallelen Netze ist für die Klassifizierung einer Klasse zuständig. ',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'Three items to learn in a RBFN:\\n\\n1. Centroids of the input clusters.\\n2. Widths of the clusters.\\n3. Weights of the synapses connecting the hidden layer and the output layer.\\n\\nThe centroids and widths are learned in an unsupervised fashion while the weights in a supervised fashion. So an RBFN combines unsupervised and supervised learning while a regular NN is completely supervised or completely unsupervised.\\n\\nLearning is fast and is not so sensitive to the unsupervised part.',\n",
       "  'In RBF network, we need to learn **centre** and **width** of gaussian function. We also learn **output weights**\\n\\nDifference between RBF and NNs:\\n\\ni. In RBF, there is only one hidden layer, while in NNs, there can be more than one hidden layer\\n\\nii. In RBF, activation function of hidden layer is Gaussian so parameters are in euclidean norm. While, in NNs, parameters for activation function are product of weights and inputs. \\n\\niii. Parameter computation is different in RBF as compute to other NNs. Like, we compute centre of cluster in RBF with the help of K-means clustering.',\n",
       "  'YOUR ANSWER HERE RBF network need to learn center of activation function. Differenec to other NN is that there are as many activation functions as data points. One con of Radial Basis Funtion is that due to many activation function RBF networks have a huge computational effort. ',\n",
       "  'In RBF, we learn the centers of the radial basis functions using unsupervised clustering methods, the weights of the last output layer, and the width of our radial basis functions. As opposed to Multi layer NN, we dont need expensive backpropagation as we only need to train the last layer, while the unsupervised training algorithm does the work the RBF centers. A possbile Con would be that if the RBF centers dont represent the training data point distribution well, some data points may be hard to model.',\n",
       "  '1. weights\\n2. centres (or means) of clusters\\n3. $\\\\sigma$ which is the width of the clusters\\n\\nDifference: Uses functions which are radially invariant.\\n\\nPros:\\n- Easy to learn\\n- Non-linearity\\n- Only dependent on the radial distance\\n\\nCons:\\n- Data required is more\\n- OVerfitting\\n\\n',\n",
       "  'An RBF network relies on a clustering algorithm. This can be e.g. k-means clustering.\\nThe three items to be learned:\\n1. Cluster center\\n2. Cluster size\\n3. weights connecting the hidden nodes to the output layer\\n\\nDifference to other NNs:\\n- only three layers: input, hidden and output\\n- each node in the hidden layer uses a different activation function depended on the cluster assigned to it\\n- only output weights are trained',\n",
       "  'If a RBF network used a gauss function as the activation fnction these thinks have to be learned:\\n\\n- centroide $c_i$ (unsupervised)\\n- sigma (unsupervised)\\n- weights of the output layer (supervised)\\n\\nThe RBF network is easy learning and not so sesitive to the unsupervised learning part.\\n',\n",
       "  'This question is really unspecific: Difference to other NNs...\\n\\n- Centers\\n- Widths\\n- Weights\\n\\nThe main diffrence is that the RBF uses localized activation functions and it has only one hidden layer.\\nIt applys a non-linear transformation from the input space to the hidden space and a linear transformation from the hidden space into output space. \\n\\nIt is important to use regularization for RBF\\nRBF work well for interpolation, so it should work good for regression',\n",
       "  'A Radial Basis Function Network has the following structure:\\n - An input layer\\n - A hidden layer, where a non-linear dimensional transformation will be used.\\n - Each neuron of the hidden layer will have a defined center (extracted in previous steps).\\n - A linear transformation will be used to the hidden data space, and the output will be calculated.\\n \\nSo, the three items that must be learning in the RBF networks are:\\n - The centers of each hidden neuron (using for example k-means neighbours algorithm).\\n - The radial function that will be used for the non-linear transformation.\\n - The weights applied into the output layer.\\n \\n',\n",
       "  'The three items that must be learned in RBFs are:\\n    - The center of the kernel\\n    - The size(standard deviation) of the kernel\\n    ',\n",
       "  '',\n",
       "  '- Use distance to center as argument for computation of local fields.\\n- Use radial basis functions as activations\\n- RFBs are only global approximators, \\n- splitted learning instead of global learning',\n",
       "  '+ Kernels\\n+ Only neighbourhoods are computed based on distances.\\n+ Radius of neighbourhoods\\n\\nPros\\n+ RBF are simple and easy to compute. \\n\\nCons\\n+ They remember the data points',\n",
       "  'Differences are:\\n* RBFN has a single hidden layer. Nonlinear hidden layer.\\n* Linear output layer.\\n* Argument of hidden units: Euclidean norm. \\n* Universal approximation property. Local approximators. \\n* Splitted Learning.\\n',\n",
       "  'The mean of the k clusters, the ',\n",
       "  'The three items that needs to be learnt are the centers, widths and depth. Compared to other NN they have a standard 3 layer structure. They can have just one hidden layer. ',\n",
       "  'In RBF first inputs are transformed to higer dimension using non linear transformation. This is based on unsupervised learning. Inputs are then learned using least square estimation which is an supervised learning. RBF is based on Covers theorem which states that there is higher probability that data will be linearly separable in higher dimension. ',\n",
       "  'YOUR ANSWER HERE:\\n- RBFs are only dependent on the radial distance i.e., distance from the center to the input',\n",
       "  'The three parametrs to be learnedin Generalized RBF are 1) cluster centers of the basis functions 2) spread or the width of the basis functions $\\\\sigma$ , and 3) weights of connecting the input and the hidden layers.\\n\\nRBF are differenent from NNs in different ways.\\n\\n1) The kernels are localized functions where as NNs are gobablized\\n\\n2) They use euclidean distance in their activation functions where as NNs use inner products\\n\\n3) They have a single hidden layer and output is a linear combinaation but NNs compulsarily are not the same.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'The three open parameters of an RBF network are:\\n1. The centers $c_i$\\n2. The widths $\\\\sigma_i$ and\\n3. The weights $w_i$  \\nThe number of centers $k$ has to be determined by trial and error.',\n",
       "  \"In rbf the main advantage is that it follows cover's theorem and the complex pattern classification problem can be solved .\",\n",
       "  '1. non-linear transformation function from input space to feature space\\n2. centers of input data that is used for each hidden neuron\\n3. synaptic weights connecting hidden layer and output layer',\n",
       "  '-',\n",
       "  'Three items to be learned,\\n* origin\\n* center\\n\\nPros:\\n* It can transform data from n dimension to infinity dimension.\\n* It can solve non linear problems easily.\\n\\nCons:\\n* It may overfit.\\n* Learning is slow.',\n",
       "  'Center of the hidden neurons, synaptic weights connecting the neurons and\\n\\nRBFs have only 1 hidden layer. There is a non0linear tranformation between the inputs and the hidden space and a linear tranformation between the hidden space and the output space. \\n\\nPros: It can be used for non-linearly separable data.\\n',\n",
       "  '1. Weigths in the network\\n2. the center of the clusters\\n3. variation of the cluster ($\\\\sigma$)\\n\\nDifference: \\n\\nRBF always have only three layers\\n\\nRBF can also trained in an unspervised method\\n\\nRBF can also approximate any continuous function',\n",
       "  '- The centroids of the radial basis functions\\n- The weights of the neurons\\n- The amount of needed neurons\\n\\nA difference to other neural networks is that the centroids of the radial basis functions need to be there.',\n",
       "  'The centers of the clusters, the widhts of the clusters and the weights. \\nIn contrast to other NNs the output only depends on the radial distance to the center of the clusters.',\n",
       "  'The weights, the interpolation matrix have to be learned. The RBF maps the input space into a higher dimensional feature space nonlinearly. The feature space is mapped into the output space linearly. The output space is much smaller than the feature space.\\n\\nPros: local learning\\n\\nCons: feature space can be really large, curse of dimensionality',\n",
       "  'The clusters, the width of the basis function and the weights.\\n\\nThe clusters and the width are learned in an unsupervised fashion. While the weights are learning by a standard supervised steepest descent method.\\n\\nPros\\nRBFs can be very easily trained.\\nRBFs can achieve better results with less complexity.\\n\\nCons\\nNot as easy to understand',\n",
       "  'Centers of the radial basis functions\\nbest model (rbf)\\ndistance of each input pair \\n\\npros:\\nnon-linear functions application\\nease to compute \\nusing covers theorem \\n\\ncons:\\nhigh-dimensional ',\n",
       "  'Centroids, width, and parameter of function  \\nThe learning of an rbfn is splitted in an unsupervised and a supervised part.   \\nOnly one layer, no vanishing gradient.  \\nPros: easy learning  \\nthe unsupervised part is not very sensitive.  \\nCons:   \\nDifficult to approximate constants  ',\n",
       "  '1. Input layer connecting RBF to environment. \\n\\n2. Hidden layer: nonolinear tranformation of input space to hidden space \\n\\n3. Output layer: linear tranformation of hidden space to output space. \\n\\nIt is different than other NNs because for learning patterns, it nonlinearly tranforms the input space to higher dimmensional space. Other NNs do not tranform input. \\n\\nAs it tranforms input patterns to high dimmensional nonlinear space, patterns which are not separable in lower dimmensions have greater chance to be separated. \\n\\nBut if we select basis functions equal to datapoints, problem is ill-formulated.\\n\\nProcessing is computationallly heavy. \\n\\nRegualation becomes problem specific.\\n\\nHence, unsupervied learning is employed to clusters data initially. ',\n",
       "  'data varaince\\n\\nFeatures\\n\\nRBF uses suport vector machine which is classifier. it uses different kernels , it doesnot have feedback cycle. \\nit also classifies non linear classification problem. it mainly works with 2 classes C1 ,C2.\\n\\nother NN is can also reggression and there can be feedback (RNN)\\n',\n",
       "  'The difference of RBF to other NNS are\\n1. RBF has only one hidden layer wheras their is no hard limitation on number of hidden layers on other NNs\\n2. The activation function used in RBF is non linear.',\n",
       "  'YOUR ANSWER HERE\\n\\nA RBF network learns,\\n\\n* The radial function\\n* weight of the hidden to output neuron\\n* Centroid of a cluster\\n\\nDifference:\\n\\n* A RBF is composed of input layer, 1 hidden layer and the output layer. Other NN can generally use as many hidden layers as required.\\n* The transformation from input to hidden layer in RBF is non linear and hidden to output is linear. In most other NN both are non linear.\\n\\nPros/Cons:\\n* This is a very simple learner\\n* There are many variations of RBF available.\\n',\n",
       "  'Items:\\nCenter der radialen Basisfunktionen.\\nGewichte des einen Hidden units zur Ausgabeschicht (Lineare Aktivierung)\\nAnzahl und gewählte Center für die radialen Basisfunktionen.\\nUnterschied zu anderen NNs:\\nHier wird Lokal gelernt anstatt global.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'K-nearest neighbors:\\n\\n1. Take the input data to be classified.\\n2. Find the first nearest neighbour in terms of euclidean distance.\\n3. Push the class of this nearest neighbour into a list of labels.\\n4. Repeat step 2 and 3 for each K which needs to be odd.\\n5. After all K nearest labels are collected in the list, count the labels in each class.\\n6. Assign to the input data, the class which as maximum count (majority vote).',\n",
       "  'i. First we initialize the random points, those points are considered as centroids of clusters\\n\\nii. Then, for each new points, we compute euclidean distance, and points closest to centrodis are assigned their respective clusters\\n\\niii. We again re-calculate the centroids of clusters\\n\\niv. Repeat 2 and 3 untill convergence is achieved, by making sure, no centroids are moving and cost function is minmized ',\n",
       "  'YOUR ANSWER HERE k-nearest neighbor wants to determine encoder $\\\\C which assigns N inputs to K clusters based on a rule to be defined.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  '1. get the input\\n2. find the k- nearest neighbours by finding the distance (euclidean) from the input to all the nodes and selecting the k closest ones\\n3. Class of the input is the most frequent class in the k-neighbnours found (as such, k needs to be odd number)',\n",
       "  '$N$ number of clusters.\\n\\nGiven sample data select $N$ different cluster centers by random.\\n\\nAssign all sample points to the closest cluster\\n\\nrepeat until no further change:\\n - recalucate the cluster centers\\n - Assign all sample points to the closest cluster',\n",
       "  'given a fixed $k$\\n\\ngiven a point to classify $new$\\n\\ngiven an empty $class$\\n\\ngiven a List of all points $L$\\n\\nfrom 1 to k do\\n\\n    find nearest point $x$ to $new$ in $L$\\n    add class of neares point $x$ in list $class$\\n    new list L = L without nearest neighboor $x$\\n    \\nclass of new = most class in $class$\\n\\n',\n",
       "  'Define K centroids, random intialised\\nassign each data point a class label\\nwhile the is no change anymore\\n    for each k \\n        calculate the centroid of the datapoint beloging to that label\\n        for each datapoint \\n            determine the nearest centroid\\n            assign a new class label which belongs to the centroid.',\n",
       "  'K-nearest neighbors can be seen as an unsupervised learning method, where for a defined number of groups k, the nearest neighbors will be calculated.\\n\\n1: For a given input data\\n\\n2: Define value k\\n\\n3: Get the k points that are closer to the given points. \\n',\n",
       "  '    1- randomly define a predefined number of cluster centers(CC)\\n    2- calculate the distance of each datapoint from each CC\\n    3- Each data point belongs to the cluster that has the least distance from its CC\\n    4- Calculate a new CC by getting the average of all the points inside a cluster\\n    5- Go to 2 and repeat this process untill we reach the termination condition',\n",
       "  'Firstly identify nearest neighbouring weights\\nthen choose k amount of neighbors and adapt their weights\\n',\n",
       "  'Initialize k_neighbors = {}, \\n\\nfor every neuron\\n\\n find the nearest neighbor and add it to k_neighbors\\n \\nReturn nearest k_neighbors\\n \\n',\n",
       "  '** Pseudo Code **\\n1. Initiate weights randomly\\n2. Assign labels to k-inputs that are map neuron is closest to.\\n3. append all inputs to map neurons using 2.\\n4. Find centroid of the cluster and move the map neuron to the centroid.\\n5. Do 2, and 4 until some convergence criteria is reached, e.g. maximum iterations is reached or no updates are performed or net distance is below some specified distance.',\n",
       "  \"Given: L; X_TEST not element of L; k = number of neighbors that will taken into consideration; function class_of()\\n\\nSet x'={}, L_0=L, Classf={};\\n\\nfor j=1,...k do:\\n\\n    L_{j-1} \\\\ x'; //exclude all the data points which have been identified as nearest neighbors already\\n    \\n    x'=find the closest neighbor of X_TEST in L_j; //e.g. compute eucldea distance\\n    \\n    c = class_of(x');\\n    \\n    Classf=push(c)\\n    \\nset c(x_TEST)= most frequently value in Classf;\",\n",
       "  'Train the knn by storing the data labeled points.\\n\\nPresent a test point.\\n> Compute the distance between the test point and all the training data points.\\n\\n> Sort the distance, and choose the k datapoints with smallest distance.\\n\\n> Determine the class of the test point by majority vote.',\n",
       "  \"L1 - Data set (x1,x2,x3,x_n)\\nL2 - Storing the dataset based on the number of neighbors. \\nx_test - Test data set. \\n\\nSo we basically have the k value to be an odd number, so that we can select a majority value. \\n\\nfor i based on the number of l:\\n    x' = xtest - distance from the neighboring neuron i. \\n    L2 = smallest x' in this based on the number of K \\n\\nx_test = max(L2)\\n\\nWe select the neurons from the neighborhood by calculating the euclidean distance based on weights. \\nThen if K is 3, we have 3 neurons. So from that we select the label which is fixed maximum on the dataset given in the K-fields. \\n\",\n",
       "  'define criterial for finding k nearest neighbours <br>\\nfind k nearest neigbours of test input in training dataset <br>\\nfind the class to which most of the neghbours belong <br>\\nassign that class to the test input <br>',\n",
       "  'YOUR ANSWER HERE: Learning based on K-nearest neighbors:\\n- All the input-output samples from the training set are stored in the memory.\\n- For a test input, find the k-nearest neighbors.\\n- Assign the test vector with the class of the most of the neighbours in the neighborhood.',\n",
       "  '\\nParameters: k -number of clusters, x datapoints , c classes\\n\\n1) Initialize randomly k centroid of the custers \\n\\n2) select a data point and compute the set of nearest neighbours of the point using euclidean distances.\\n\\n3) Find the class that maximum number of neighbours belong to and assign the class to the datapoint.\\n\\n4) Once the class is assigned, compute the centroid of each cluster or class, considering all the class members.\\n\\n5) Iterate over all the datapoints and repeat over all points (from step 2) until no update in centroids is required.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  '1. Given: Classified data $X$\\n2. For a new sample $x$:  \\n    Determine the $k$ nearest neighbours in X  \\n    Output $y$ := majority vote of the class of nearest neighbours',\n",
       "  \"$ L = {x1,x2...x_n} $\\n\\n$L = L_0$\\n\\n$x' = {}$\\n\\nfor the input (x,d) :\\ndo {\\nx_test \\n\\n}\",\n",
       "  '1. identify k classified patterns that lie nearest to the test vector\\n2. assign the test vector to the class that is most frequently presented to the k nearest neighbors',\n",
       "  '1. Define the number of cluster (K)\\n2. Generate random weights\\n3. Find the center of each k (mean)\\n4. Cluster the other outputs by determining the closest neighbor\\n5. Update the weights',\n",
       "  '* Choose a value for k\\n* K represents the number of neighbors\\n* Get a sample from the input space\\n* Find the class based on the majority of votes received from the neighbors. \\n* For example, if the value of k is 3, then let say there are 2 neighbors from class one and 1 neighbor from class two, then the new input sample belongs to class one.',\n",
       "  'Step1: We randomly place the n neurons.\\n\\nStep2: For each data point whichever neuron is closer to it, the datapoint is assigned to that neuron.\\n\\nStep3: Once all the datapoints are assigned, the mean of the datapoints attached to each neuron is calculated and the neuron is shifted to the mean value.\\n\\nStep4: Step 2 and 3 are done until there is no more shift in the neurons position.\\n\\nIn this way the neurons are adjusted.',\n",
       "  'Step1 : Randomly select the k centers \\n\\nStep2 : Cluster the datapoints based on the centers\\n\\nStep3 : the centroid of the cluster becomes the new mean\\n\\nStep4 : repeat step 2 and 3 until there is no more evidential cahnge in the network',\n",
       "  '    input: labeled data set, one unlabeled data point, number k\\n\\n    find the k labeled points, which are closest to the given unlabeled point\\n    from these points, find the label which occurs most often\\n    assign this label to the unlabeled data point',\n",
       "  \"1. Get the nearest neighbor of the current x'\\n2. Remove it from L \\n3. Get the class of the current x \\n4. Classify x' as the class that occurs the most often in the neighbors\\n\\nfor 1 to K:\\n\\n    L_i = L/x'\\n    \\n    x_nn = min(|x-x'|)\\n    \\n    c = getclassof(x_nn)\\n    \\n    AmountofClasses.add(c)\\n    \\nsetclassof(x') = Max(AmountofClasses)\",\n",
       "  'For the input data x the distance to every other data point is calculated using a distance measure.\\n\\nTake the k data points, which have the minimum distance to x. These are the k-nearest neighbours.\\n\\nThe most frequent class from the neighbours is assigned as the class of the input data.',\n",
       "  'This learning is based on the memory introduced into the dataset. For each data point the nearest neighbours are found via a distance function.\\n\\nfor each datapoint d\\n\\n    neighbours = get_k_nearest_neighbours_of(d)\\n\\n    d.class = get_most_represented_class(neighbours) ',\n",
       "  'for a given input \\n    compute distances to other input points\\n    pick k nearest neighboors\\n    look at labeling of neighboors\\n    decide labeling (classification) by highest number of neighboors in one class (german: Mehrheitsentscheid)',\n",
       "  'training_set := training data  \\ndefine #clusters  \\nselect #clusters datapoints as centroids randomly  \\n\\nfor datapoint in training_set:   \\n    calculate distance to centroid\\n    lable dataPoint according to closest centroid\\nend for\\n\\niterate over clusters:  \\n    calculate centroid',\n",
       "  '(K-nearest neighbours is memory based learning.)\\n\\ntake input x \\n\\ncalculate calculate distance of x from each training point. \\n\\nSelect K training points with minimum distce from the data. \\n\\nFetch classes of selected K nearest points. \\n\\nCalculate number points per class in k nearest points. \\n\\ndetermine the class C having maximum points in k nearest pioints \\n\\nThe class of the input point is C.',\n",
       "  'K-nearest neighbors basically works as follows\\n\\n1) the they define randomly the cluster points .\\n\\n2) clacluate the mean of the equlidian distance between the data points. here the points from the previous step acts as centrioids.\\n\\n3) check the variance of the clusters. \\n\\n4) repeat 1-2-3 till you get the proper clusters.\\n',\n",
       "  '1. Slect random number of neghbourhood initially\\n2. Find out the input which is nearest to the weight vector using competitive learning\\n3. Change only the input which wins\\n4. decrease the size of neighbourhood\\n5. Repeat',\n",
       "  'YOUR ANSWER HERE\\n\\nfor x in input_points\\n\\n  neighbours = find_nearest_k_points(x)\\n  \\n  for n in neighbours\\n  \\n    v = get_vote_of(n)\\n    \\n    update_votes_count_for(x,v)\\n    \\n  max = get_max_vote_for(x)\\n  ',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'Let L be set of labeled data in memory, L ={x1,x2....x_n}, while x_prime is nearest point to the x_test point, in term of euclidean distance. Let class_of be funtion that return class type if certain data point x. And let K be constant number of neighboring points consired in algorithm search. \\n\\nInitialize x_prime = {}, L_0 = L, list_of_classes = {}\\n\\nfor j= 1; j<=K; j++ do:\\n    \\n   L_j = L_(j-1)/x_prime\\n   \\n   x_prime = nearest neighbor to X_test form L_j data\\n   \\n   c = class_of(x_prime)\\n   \\n   list_of_classes.append(c)\\n   \\nend\\n\\nc(x_test):=  most frequent class in list_of_classes',\n",
       "  'In machine learning, a choice always needs to be made for the tradeoff between bias and variance. Bias determines how close the result is to the true value and variance determines the sensitivity to flutuations in the training dataset. If bias is reduced variance increases and vice versa. So an optimum tradeoff needs to be chosen which presents a dilemma.',\n",
       "  'Bias Variance dilemma is used to analyse the generalization error of the algorithm. \\nIf the value of Bias is very high, then network does not learn relations between features and outputs correctly(overfitting)\\nIf the value of variance is very high, then network may model the random noise, and it does not learn intended ouputs(underfitting)\\n\\nWe have to to tradeoff between Bias and Variance so that our model can generalize properly',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'When training a model on a limited training data set, we must decide wether we accept a biased model which makes assumptions about the test data, but has a better performance on the train data, or a model with more variance which might model the entirety of the data better but be prone to data noise. Usually we have to decide on a trade off between the two, where we may select well balanced models based on VC dimensions or Cross validation results. ',\n",
       "  'Bias and variance are both undesirable to the learning. Bias defines how far the generated output differs from the true value. Variance defines how much the o/p change on changing the input dataset. However, in most cases, it is only possible to decrease one at the expesne of other. Thus, it is called  Bias Variance Dilemma.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'The bias is the error we make in the assumption by creating the learning machine (how much we we are away from the actual truth)\\n\\nthe variance is how much the learning machine changes with different training data sets.\\n\\nif we have a high bias we habe a low variance and if we habe a low variance we habe a high bias\\n\\n',\n",
       "  'You have to to a tradeoff between high bias or high variance. You cannot have both. High vaiance means the model is overfitting the data and therefore the variance on input can be quit hight. High bias means the model is generalization is to unspecific. ',\n",
       "  'The Bias is defined as the grade of correctness that a learning algorithm will use. The Variance is defined as the grade of flexibility that the algorithm have given a model to learn. When having the Bias high, but the Variance low, the algorithm will not be flexible into data and will discard any data is not exactly the data that fits into the model. On the other hand, when having the variance high but the bias low, the algorithm will be very flexible into the data and will accept any error data as part of the model to learn.',\n",
       "  '- Bias: the bias is the differnce between the predicted value and the desired value in the generalization run\\n- Variance: is the inadequity in the produced value in the regression and the desired value that we expect from the network',\n",
       "  'Bias Variance dilemma is coming from the fact that you can not have both at the same time. Your network can not be equally great at outputing with extremely high accuracy extremely hight amount of variables. Therefore you need to find balance between the two that suits needs of your neural network.',\n",
       "  'It is refers to the problem of tryning to mantain a balance between two causes of errors in learning algorithms such that the network is able to generalize data beyong that used for training. Namely the bias error and the variance error. Having a high bias error may cause the network to miss important features in the training data, which leads to underfitting. High variance will make the network to memorize noise present in the training data rather than learning features, which lead to overfitting.\\n',\n",
       "  '+ One cannot optimize simultaneously the learning algorithm both for learning maximum variance in the data and learning localization which can be termed as bias.',\n",
       "  '',\n",
       "  'The Bias Variance Dilemma tells us that the bias (the difference between the actual and desired output) and the variance (output difference between each trial) cannot be decreased at the same time. A complex model results in small variance and larger variance.',\n",
       "  'So in machine learning problem, minimizing the two main source of error simultenously does not allow the networks to be generalised very easy. \\n \\nIf bias increase, variance decrease. And vice versa also holds.\\n\\n1. Bias tells us how close we are to the true value! \\n2. Variance tells us how they vary for different data set. \\n\\nSo this is a standard problem in NN',\n",
       "  'High value of bias means netowrk is unable to learn the data whereas higher variance means its difficult to learn the training data successfully.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'Bias and variances are the estimation errors.\\n\\nBias corresponds to the inability of the learning machine to appropriately approximate the function to be learnt. Hence this induces a deviation from the actual function\\n\\nVariance is the inadequacy of the training data to allow the a learning machine to succesfully learn the function. \\n\\nThe dilemma is that , to completely learn the actual function( to reduce variance-related error), the training data required should consist of infinite samples. However, this resuts in slower convergence, inturn, bias error increases.\\n\\nTherefore trade of between both the errors need to be made.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'Bias is the difference between the predicted and true value. Variance is the range of several predicted values of the same datapoint. It is desirable to have low bias and low variance to ensure the predicted value is consistently close to the true value. The Bias Variance dilemma is that to achieve low bias, the variance becomes high and vice versa. Hence, there is always a tradeoff between the two.',\n",
       "  'Bias variance dilemma refers to the problem of minimizing the two sources of error bias errror and variance error simultaneously which creates probblem in generaliztion of the network.\\nBias error: It is the error that occurs while setting the parameters of the network\\nvariance error:It refers to how sensetive the network is to the fluctuations in the dataset.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'Bias variance dilemma is a process of simultaneously decreasing two sources of error that prevents supervised learning algorithm from generalizing beyond the trained data.',\n",
       "  'Bias is used to affine transform of $u$.\\n\\nIt helps to shift the classifier line.\\n\\n$$v=u+b$$',\n",
       "  'Bias: How close the estimate is to the true value.\\n\\nvariance: How much does the estimate vary for different training sets.\\n\\nwe always have either hugh variance low bias or low variance high bias.',\n",
       "  \"Bias : differnce between the estmated output and the actual output\\n\\nVariance: The range of output of a network for different training set. \\n\\nBias and Variance can't be decreased at the same time for many networks. ONly one at a time can be decreased\\n\\n\",\n",
       "  'NO ANSWER HERE',\n",
       "  \"When adapting the parameters of a network we can either have a small bias or a small variance. If we have a small bias the approximation of the network is close to the real one, but the variance between trials is very high. If we have a low variance, the bias can't be minimized and the network has a bigger error between the apüproximation and the real value. \",\n",
       "  'YOUR ANSWER HERE',\n",
       "  'Ideally bias and variance would be 0 after learning a machine. However, bias and variance counteract eachother: when bias decreases, variance rises and respectively in the other direction. This leads to the dilemma that either one of the values has to be present.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'Usualy only one of Bias and Variance can be minimized. In an RBFN for example  \\nfew kernels with greater width leads to a high bias but a low variance. If you choose many kernels with smaller width the bias is low but the variance is high. Higher complexity models need more training data.',\n",
       "  'YOUR ANSWER HERE',\n",
       "  'Bias is an proides an affine transformation. and it is treated  a extra inputs. which noramll taken as +1',\n",
       "  'High bias and variance is desirable in input. Bias Variance Dilemma is the property of input data where if the bias is increased the variance decreases and vice versa. It is difficult to find a tradeoff between them. ',\n",
       "  'YOUR ANSWER HERE\\n\\nBias: Bias means how much the prediction differs from the true value\\n\\nVariance: Variance means how much the prediction varies for different datasets\\n\\nThe Dilemma is that both generally can not be reduced simultaneously. A learning machine can reduce one at the cost of other.',\n",
       "  'Bias: Unfähigkeit des Netzes korrekt  zu adaptieren.\\nVariance: Unfähigkeit der Daten, die zugrunde liegende Hypothese korrekt zu beschreiben',\n",
       "  'YOUR ANSWER HERE'],\n",
       " 'question_id': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  14,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17],\n",
       " 'student_id': ['mas-usb-011',\n",
       "  'mas-usb-041',\n",
       "  'mas-usb-051',\n",
       "  'mas-usb-061',\n",
       "  'mas-usb-071',\n",
       "  'mas-usb-081',\n",
       "  'mas-usb-091',\n",
       "  'mas-usb-101',\n",
       "  'mas-usb-111',\n",
       "  'mas-usb-121',\n",
       "  'mas-usb-131',\n",
       "  'mas-usb-141',\n",
       "  'mas-usb-151',\n",
       "  'mas-usb-161',\n",
       "  'mas-usb-171',\n",
       "  'mas-usb-181',\n",
       "  'mas-usb-191',\n",
       "  'mas-usb-201',\n",
       "  'mas-usb-211',\n",
       "  'mas-usb-221',\n",
       "  'mas-usb-231',\n",
       "  'mas-usb-241',\n",
       "  'mas-usb-311',\n",
       "  'mas-usb-331',\n",
       "  'mas-usb-341',\n",
       "  'mas-usb-351',\n",
       "  'mas-usb-361',\n",
       "  'mas-usb-371',\n",
       "  'mas-usb-381',\n",
       "  'mas-usb-391',\n",
       "  'mas-usb-451',\n",
       "  'mas-usb-461',\n",
       "  'mas-usb-471',\n",
       "  'mas-usb-491',\n",
       "  'mas-usb-501',\n",
       "  'mas-usb-511',\n",
       "  'mas-usb-541',\n",
       "  'mas-usb-571',\n",
       "  'mas-usb-581',\n",
       "  'mas-usb-012',\n",
       "  'mas-usb-042',\n",
       "  'mas-usb-052',\n",
       "  'mas-usb-062',\n",
       "  'mas-usb-072',\n",
       "  'mas-usb-082',\n",
       "  'mas-usb-092',\n",
       "  'mas-usb-102',\n",
       "  'mas-usb-112',\n",
       "  'mas-usb-122',\n",
       "  'mas-usb-132',\n",
       "  'mas-usb-142',\n",
       "  'mas-usb-152',\n",
       "  'mas-usb-162',\n",
       "  'mas-usb-172',\n",
       "  'mas-usb-182',\n",
       "  'mas-usb-192',\n",
       "  'mas-usb-202',\n",
       "  'mas-usb-212',\n",
       "  'mas-usb-222',\n",
       "  'mas-usb-232',\n",
       "  'mas-usb-242',\n",
       "  'mas-usb-312',\n",
       "  'mas-usb-332',\n",
       "  'mas-usb-342',\n",
       "  'mas-usb-352',\n",
       "  'mas-usb-362',\n",
       "  'mas-usb-372',\n",
       "  'mas-usb-382',\n",
       "  'mas-usb-392',\n",
       "  'mas-usb-452',\n",
       "  'mas-usb-462',\n",
       "  'mas-usb-472',\n",
       "  'mas-usb-492',\n",
       "  'mas-usb-502',\n",
       "  'mas-usb-512',\n",
       "  'mas-usb-542',\n",
       "  'mas-usb-572',\n",
       "  'mas-usb-582',\n",
       "  'mas-usb-013',\n",
       "  'mas-usb-043',\n",
       "  'mas-usb-053',\n",
       "  'mas-usb-063',\n",
       "  'mas-usb-073',\n",
       "  'mas-usb-083',\n",
       "  'mas-usb-093',\n",
       "  'mas-usb-103',\n",
       "  'mas-usb-113',\n",
       "  'mas-usb-123',\n",
       "  'mas-usb-133',\n",
       "  'mas-usb-143',\n",
       "  'mas-usb-153',\n",
       "  'mas-usb-163',\n",
       "  'mas-usb-173',\n",
       "  'mas-usb-183',\n",
       "  'mas-usb-193',\n",
       "  'mas-usb-203',\n",
       "  'mas-usb-213',\n",
       "  'mas-usb-223',\n",
       "  'mas-usb-233',\n",
       "  'mas-usb-243',\n",
       "  'mas-usb-313',\n",
       "  'mas-usb-333',\n",
       "  'mas-usb-343',\n",
       "  'mas-usb-353',\n",
       "  'mas-usb-363',\n",
       "  'mas-usb-373',\n",
       "  'mas-usb-383',\n",
       "  'mas-usb-393',\n",
       "  'mas-usb-453',\n",
       "  'mas-usb-463',\n",
       "  'mas-usb-473',\n",
       "  'mas-usb-493',\n",
       "  'mas-usb-503',\n",
       "  'mas-usb-513',\n",
       "  'mas-usb-543',\n",
       "  'mas-usb-573',\n",
       "  'mas-usb-583',\n",
       "  'mas-usb-014',\n",
       "  'mas-usb-044',\n",
       "  'mas-usb-054',\n",
       "  'mas-usb-064',\n",
       "  'mas-usb-074',\n",
       "  'mas-usb-084',\n",
       "  'mas-usb-094',\n",
       "  'mas-usb-104',\n",
       "  'mas-usb-114',\n",
       "  'mas-usb-124',\n",
       "  'mas-usb-134',\n",
       "  'mas-usb-144',\n",
       "  'mas-usb-154',\n",
       "  'mas-usb-164',\n",
       "  'mas-usb-174',\n",
       "  'mas-usb-184',\n",
       "  'mas-usb-194',\n",
       "  'mas-usb-204',\n",
       "  'mas-usb-214',\n",
       "  'mas-usb-224',\n",
       "  'mas-usb-234',\n",
       "  'mas-usb-244',\n",
       "  'mas-usb-314',\n",
       "  'mas-usb-334',\n",
       "  'mas-usb-344',\n",
       "  'mas-usb-354',\n",
       "  'mas-usb-364',\n",
       "  'mas-usb-374',\n",
       "  'mas-usb-384',\n",
       "  'mas-usb-394',\n",
       "  'mas-usb-454',\n",
       "  'mas-usb-464',\n",
       "  'mas-usb-474',\n",
       "  'mas-usb-494',\n",
       "  'mas-usb-504',\n",
       "  'mas-usb-514',\n",
       "  'mas-usb-544',\n",
       "  'mas-usb-574',\n",
       "  'mas-usb-584',\n",
       "  'mas-usb-015',\n",
       "  'mas-usb-045',\n",
       "  'mas-usb-055',\n",
       "  'mas-usb-065',\n",
       "  'mas-usb-075',\n",
       "  'mas-usb-085',\n",
       "  'mas-usb-095',\n",
       "  'mas-usb-105',\n",
       "  'mas-usb-115',\n",
       "  'mas-usb-125',\n",
       "  'mas-usb-135',\n",
       "  'mas-usb-145',\n",
       "  'mas-usb-155',\n",
       "  'mas-usb-165',\n",
       "  'mas-usb-175',\n",
       "  'mas-usb-185',\n",
       "  'mas-usb-195',\n",
       "  'mas-usb-205',\n",
       "  'mas-usb-215',\n",
       "  'mas-usb-225',\n",
       "  'mas-usb-235',\n",
       "  'mas-usb-245',\n",
       "  'mas-usb-315',\n",
       "  'mas-usb-335',\n",
       "  'mas-usb-345',\n",
       "  'mas-usb-355',\n",
       "  'mas-usb-365',\n",
       "  'mas-usb-375',\n",
       "  'mas-usb-385',\n",
       "  'mas-usb-395',\n",
       "  'mas-usb-455',\n",
       "  'mas-usb-465',\n",
       "  'mas-usb-475',\n",
       "  'mas-usb-495',\n",
       "  'mas-usb-505',\n",
       "  'mas-usb-515',\n",
       "  'mas-usb-545',\n",
       "  'mas-usb-575',\n",
       "  'mas-usb-585',\n",
       "  'mas-usb-016',\n",
       "  'mas-usb-046',\n",
       "  'mas-usb-056',\n",
       "  'mas-usb-066',\n",
       "  'mas-usb-076',\n",
       "  'mas-usb-086',\n",
       "  'mas-usb-096',\n",
       "  'mas-usb-106',\n",
       "  'mas-usb-116',\n",
       "  'mas-usb-126',\n",
       "  'mas-usb-136',\n",
       "  'mas-usb-146',\n",
       "  'mas-usb-156',\n",
       "  'mas-usb-166',\n",
       "  'mas-usb-176',\n",
       "  'mas-usb-186',\n",
       "  'mas-usb-196',\n",
       "  'mas-usb-206',\n",
       "  'mas-usb-216',\n",
       "  'mas-usb-226',\n",
       "  'mas-usb-236',\n",
       "  'mas-usb-246',\n",
       "  'mas-usb-316',\n",
       "  'mas-usb-336',\n",
       "  'mas-usb-346',\n",
       "  'mas-usb-356',\n",
       "  'mas-usb-366',\n",
       "  'mas-usb-376',\n",
       "  'mas-usb-386',\n",
       "  'mas-usb-396',\n",
       "  'mas-usb-456',\n",
       "  'mas-usb-466',\n",
       "  'mas-usb-476',\n",
       "  'mas-usb-496',\n",
       "  'mas-usb-506',\n",
       "  'mas-usb-516',\n",
       "  'mas-usb-546',\n",
       "  'mas-usb-576',\n",
       "  'mas-usb-586',\n",
       "  'mas-usb-017',\n",
       "  'mas-usb-047',\n",
       "  'mas-usb-057',\n",
       "  'mas-usb-067',\n",
       "  'mas-usb-077',\n",
       "  'mas-usb-087',\n",
       "  'mas-usb-097',\n",
       "  'mas-usb-107',\n",
       "  'mas-usb-117',\n",
       "  'mas-usb-127',\n",
       "  'mas-usb-137',\n",
       "  'mas-usb-147',\n",
       "  'mas-usb-157',\n",
       "  'mas-usb-167',\n",
       "  'mas-usb-177',\n",
       "  'mas-usb-187',\n",
       "  'mas-usb-197',\n",
       "  'mas-usb-207',\n",
       "  'mas-usb-217',\n",
       "  'mas-usb-227',\n",
       "  'mas-usb-237',\n",
       "  'mas-usb-247',\n",
       "  'mas-usb-317',\n",
       "  'mas-usb-337',\n",
       "  'mas-usb-347',\n",
       "  'mas-usb-357',\n",
       "  'mas-usb-367',\n",
       "  'mas-usb-377',\n",
       "  'mas-usb-387',\n",
       "  'mas-usb-397',\n",
       "  'mas-usb-457',\n",
       "  'mas-usb-467',\n",
       "  'mas-usb-477',\n",
       "  'mas-usb-497',\n",
       "  'mas-usb-507',\n",
       "  'mas-usb-517',\n",
       "  'mas-usb-547',\n",
       "  'mas-usb-577',\n",
       "  'mas-usb-587',\n",
       "  'mas-usb-018',\n",
       "  'mas-usb-048',\n",
       "  'mas-usb-058',\n",
       "  'mas-usb-068',\n",
       "  'mas-usb-078',\n",
       "  'mas-usb-088',\n",
       "  'mas-usb-098',\n",
       "  'mas-usb-108',\n",
       "  'mas-usb-118',\n",
       "  'mas-usb-128',\n",
       "  'mas-usb-138',\n",
       "  'mas-usb-148',\n",
       "  'mas-usb-158',\n",
       "  'mas-usb-168',\n",
       "  'mas-usb-178',\n",
       "  'mas-usb-188',\n",
       "  'mas-usb-198',\n",
       "  'mas-usb-208',\n",
       "  'mas-usb-218',\n",
       "  'mas-usb-228',\n",
       "  'mas-usb-238',\n",
       "  'mas-usb-248',\n",
       "  'mas-usb-318',\n",
       "  'mas-usb-338',\n",
       "  'mas-usb-348',\n",
       "  'mas-usb-358',\n",
       "  'mas-usb-368',\n",
       "  'mas-usb-378',\n",
       "  'mas-usb-388',\n",
       "  'mas-usb-398',\n",
       "  'mas-usb-458',\n",
       "  'mas-usb-468',\n",
       "  'mas-usb-478',\n",
       "  'mas-usb-498',\n",
       "  'mas-usb-508',\n",
       "  'mas-usb-518',\n",
       "  'mas-usb-548',\n",
       "  'mas-usb-578',\n",
       "  'mas-usb-588',\n",
       "  'mas-usb-019',\n",
       "  'mas-usb-049',\n",
       "  'mas-usb-059',\n",
       "  'mas-usb-069',\n",
       "  'mas-usb-079',\n",
       "  'mas-usb-089',\n",
       "  'mas-usb-099',\n",
       "  'mas-usb-109',\n",
       "  'mas-usb-119',\n",
       "  'mas-usb-129',\n",
       "  'mas-usb-139',\n",
       "  'mas-usb-149',\n",
       "  'mas-usb-159',\n",
       "  'mas-usb-169',\n",
       "  'mas-usb-179',\n",
       "  'mas-usb-189',\n",
       "  'mas-usb-199',\n",
       "  'mas-usb-209',\n",
       "  'mas-usb-219',\n",
       "  'mas-usb-229',\n",
       "  'mas-usb-239',\n",
       "  'mas-usb-249',\n",
       "  'mas-usb-319',\n",
       "  'mas-usb-339',\n",
       "  'mas-usb-349',\n",
       "  'mas-usb-359',\n",
       "  'mas-usb-369',\n",
       "  'mas-usb-379',\n",
       "  'mas-usb-389',\n",
       "  'mas-usb-399',\n",
       "  'mas-usb-459',\n",
       "  'mas-usb-469',\n",
       "  'mas-usb-479',\n",
       "  'mas-usb-499',\n",
       "  'mas-usb-509',\n",
       "  'mas-usb-519',\n",
       "  'mas-usb-549',\n",
       "  'mas-usb-579',\n",
       "  'mas-usb-589',\n",
       "  'mas-usb-0110',\n",
       "  'mas-usb-0410',\n",
       "  'mas-usb-0510',\n",
       "  'mas-usb-0610',\n",
       "  'mas-usb-0710',\n",
       "  'mas-usb-0810',\n",
       "  'mas-usb-0910',\n",
       "  'mas-usb-1010',\n",
       "  'mas-usb-1110',\n",
       "  'mas-usb-1210',\n",
       "  'mas-usb-1310',\n",
       "  'mas-usb-1410',\n",
       "  'mas-usb-1510',\n",
       "  'mas-usb-1610',\n",
       "  'mas-usb-1710',\n",
       "  'mas-usb-1810',\n",
       "  'mas-usb-1910',\n",
       "  'mas-usb-2010',\n",
       "  'mas-usb-2110',\n",
       "  'mas-usb-2210',\n",
       "  'mas-usb-2310',\n",
       "  'mas-usb-2410',\n",
       "  'mas-usb-3110',\n",
       "  'mas-usb-3310',\n",
       "  'mas-usb-3410',\n",
       "  'mas-usb-3510',\n",
       "  'mas-usb-3610',\n",
       "  'mas-usb-3710',\n",
       "  'mas-usb-3810',\n",
       "  'mas-usb-3910',\n",
       "  'mas-usb-4510',\n",
       "  'mas-usb-4610',\n",
       "  'mas-usb-4710',\n",
       "  'mas-usb-4910',\n",
       "  'mas-usb-5010',\n",
       "  'mas-usb-5110',\n",
       "  'mas-usb-5410',\n",
       "  'mas-usb-5710',\n",
       "  'mas-usb-5810',\n",
       "  'mas-usb-0111',\n",
       "  'mas-usb-0411',\n",
       "  'mas-usb-0511',\n",
       "  'mas-usb-0611',\n",
       "  'mas-usb-0711',\n",
       "  'mas-usb-0811',\n",
       "  'mas-usb-0911',\n",
       "  'mas-usb-1011',\n",
       "  'mas-usb-1111',\n",
       "  'mas-usb-1211',\n",
       "  'mas-usb-1311',\n",
       "  'mas-usb-1411',\n",
       "  'mas-usb-1511',\n",
       "  'mas-usb-1611',\n",
       "  'mas-usb-1711',\n",
       "  'mas-usb-1811',\n",
       "  'mas-usb-1911',\n",
       "  'mas-usb-2011',\n",
       "  'mas-usb-2111',\n",
       "  'mas-usb-2211',\n",
       "  'mas-usb-2311',\n",
       "  'mas-usb-2411',\n",
       "  'mas-usb-3111',\n",
       "  'mas-usb-3311',\n",
       "  'mas-usb-3411',\n",
       "  'mas-usb-3511',\n",
       "  'mas-usb-3611',\n",
       "  'mas-usb-3711',\n",
       "  'mas-usb-3811',\n",
       "  'mas-usb-3911',\n",
       "  'mas-usb-4511',\n",
       "  'mas-usb-4611',\n",
       "  'mas-usb-4711',\n",
       "  'mas-usb-4911',\n",
       "  'mas-usb-5011',\n",
       "  'mas-usb-5111',\n",
       "  'mas-usb-5411',\n",
       "  'mas-usb-5711',\n",
       "  'mas-usb-5811',\n",
       "  'mas-usb-0112',\n",
       "  'mas-usb-0412',\n",
       "  'mas-usb-0512',\n",
       "  'mas-usb-0612',\n",
       "  'mas-usb-0712',\n",
       "  'mas-usb-0812',\n",
       "  'mas-usb-0912',\n",
       "  'mas-usb-1012',\n",
       "  'mas-usb-1112',\n",
       "  'mas-usb-1212',\n",
       "  'mas-usb-1312',\n",
       "  'mas-usb-1412',\n",
       "  'mas-usb-1512',\n",
       "  'mas-usb-1612',\n",
       "  'mas-usb-1712',\n",
       "  'mas-usb-1812',\n",
       "  'mas-usb-1912',\n",
       "  'mas-usb-2012',\n",
       "  'mas-usb-2112',\n",
       "  'mas-usb-2212',\n",
       "  'mas-usb-2312',\n",
       "  'mas-usb-2412',\n",
       "  'mas-usb-3112',\n",
       "  'mas-usb-3312',\n",
       "  'mas-usb-3412',\n",
       "  'mas-usb-3512',\n",
       "  'mas-usb-3612',\n",
       "  'mas-usb-3712',\n",
       "  'mas-usb-3812',\n",
       "  'mas-usb-3912',\n",
       "  'mas-usb-4512',\n",
       "  'mas-usb-4612',\n",
       "  'mas-usb-4712',\n",
       "  'mas-usb-4912',\n",
       "  'mas-usb-5012',\n",
       "  'mas-usb-5112',\n",
       "  'mas-usb-5412',\n",
       "  'mas-usb-5712',\n",
       "  'mas-usb-5812',\n",
       "  'mas-usb-0113',\n",
       "  'mas-usb-0413',\n",
       "  'mas-usb-0513',\n",
       "  'mas-usb-0613',\n",
       "  'mas-usb-0713',\n",
       "  'mas-usb-0813',\n",
       "  'mas-usb-0913',\n",
       "  'mas-usb-1013',\n",
       "  'mas-usb-1113',\n",
       "  'mas-usb-1213',\n",
       "  'mas-usb-1313',\n",
       "  'mas-usb-1413',\n",
       "  'mas-usb-1513',\n",
       "  'mas-usb-1613',\n",
       "  'mas-usb-1713',\n",
       "  'mas-usb-1813',\n",
       "  'mas-usb-1913',\n",
       "  'mas-usb-2013',\n",
       "  'mas-usb-2113',\n",
       "  'mas-usb-2213',\n",
       "  'mas-usb-2313',\n",
       "  'mas-usb-2413',\n",
       "  'mas-usb-3113',\n",
       "  'mas-usb-3313',\n",
       "  'mas-usb-3413',\n",
       "  'mas-usb-3513',\n",
       "  'mas-usb-3613',\n",
       "  'mas-usb-3713',\n",
       "  'mas-usb-3813',\n",
       "  'mas-usb-3913',\n",
       "  'mas-usb-4513',\n",
       "  'mas-usb-4613',\n",
       "  'mas-usb-4713',\n",
       "  'mas-usb-4913',\n",
       "  'mas-usb-5013',\n",
       "  'mas-usb-5113',\n",
       "  'mas-usb-5413',\n",
       "  'mas-usb-5713',\n",
       "  'mas-usb-5813',\n",
       "  'mas-usb-0114',\n",
       "  'mas-usb-0414',\n",
       "  'mas-usb-0514',\n",
       "  'mas-usb-0614',\n",
       "  'mas-usb-0714',\n",
       "  'mas-usb-0814',\n",
       "  'mas-usb-0914',\n",
       "  'mas-usb-1014',\n",
       "  'mas-usb-1114',\n",
       "  'mas-usb-1214',\n",
       "  'mas-usb-1314',\n",
       "  'mas-usb-1414',\n",
       "  'mas-usb-1514',\n",
       "  'mas-usb-1614',\n",
       "  'mas-usb-1714',\n",
       "  'mas-usb-1814',\n",
       "  'mas-usb-1914',\n",
       "  'mas-usb-2014',\n",
       "  'mas-usb-2114',\n",
       "  'mas-usb-2214',\n",
       "  'mas-usb-2314',\n",
       "  'mas-usb-2414',\n",
       "  'mas-usb-3114',\n",
       "  'mas-usb-3314',\n",
       "  'mas-usb-3414',\n",
       "  'mas-usb-3514',\n",
       "  'mas-usb-3614',\n",
       "  'mas-usb-3714',\n",
       "  'mas-usb-3814',\n",
       "  'mas-usb-3914',\n",
       "  'mas-usb-4514',\n",
       "  'mas-usb-4614',\n",
       "  'mas-usb-4714',\n",
       "  'mas-usb-4914',\n",
       "  'mas-usb-5014',\n",
       "  'mas-usb-5114',\n",
       "  'mas-usb-5414',\n",
       "  'mas-usb-5714',\n",
       "  'mas-usb-5814',\n",
       "  'mas-usb-0115',\n",
       "  'mas-usb-0415',\n",
       "  'mas-usb-0515',\n",
       "  'mas-usb-0615',\n",
       "  'mas-usb-0715',\n",
       "  'mas-usb-0815',\n",
       "  'mas-usb-0915',\n",
       "  'mas-usb-1015',\n",
       "  'mas-usb-1115',\n",
       "  'mas-usb-1215',\n",
       "  'mas-usb-1315',\n",
       "  'mas-usb-1415',\n",
       "  'mas-usb-1515',\n",
       "  'mas-usb-1615',\n",
       "  'mas-usb-1715',\n",
       "  'mas-usb-1815',\n",
       "  'mas-usb-1915',\n",
       "  'mas-usb-2015',\n",
       "  'mas-usb-2115',\n",
       "  'mas-usb-2215',\n",
       "  'mas-usb-2315',\n",
       "  'mas-usb-2415',\n",
       "  'mas-usb-3115',\n",
       "  'mas-usb-3315',\n",
       "  'mas-usb-3415',\n",
       "  'mas-usb-3515',\n",
       "  'mas-usb-3615',\n",
       "  'mas-usb-3715',\n",
       "  'mas-usb-3815',\n",
       "  'mas-usb-3915',\n",
       "  'mas-usb-4515',\n",
       "  'mas-usb-4615',\n",
       "  'mas-usb-4715',\n",
       "  'mas-usb-4915',\n",
       "  'mas-usb-5015',\n",
       "  'mas-usb-5115',\n",
       "  'mas-usb-5415',\n",
       "  'mas-usb-5715',\n",
       "  'mas-usb-5815',\n",
       "  'mas-usb-0116',\n",
       "  'mas-usb-0416',\n",
       "  'mas-usb-0516',\n",
       "  'mas-usb-0616',\n",
       "  'mas-usb-0716',\n",
       "  'mas-usb-0816',\n",
       "  'mas-usb-0916',\n",
       "  'mas-usb-1016',\n",
       "  'mas-usb-1116',\n",
       "  'mas-usb-1216',\n",
       "  'mas-usb-1316',\n",
       "  'mas-usb-1416',\n",
       "  'mas-usb-1516',\n",
       "  'mas-usb-1616',\n",
       "  'mas-usb-1716',\n",
       "  'mas-usb-1816',\n",
       "  'mas-usb-1916',\n",
       "  'mas-usb-2016',\n",
       "  'mas-usb-2116',\n",
       "  'mas-usb-2216',\n",
       "  'mas-usb-2316',\n",
       "  'mas-usb-2416',\n",
       "  'mas-usb-3116',\n",
       "  'mas-usb-3316',\n",
       "  'mas-usb-3416',\n",
       "  'mas-usb-3516',\n",
       "  'mas-usb-3616',\n",
       "  'mas-usb-3716',\n",
       "  'mas-usb-3816',\n",
       "  'mas-usb-3916',\n",
       "  'mas-usb-4516',\n",
       "  'mas-usb-4616',\n",
       "  'mas-usb-4716',\n",
       "  'mas-usb-4916',\n",
       "  'mas-usb-5016',\n",
       "  'mas-usb-5116',\n",
       "  'mas-usb-5416',\n",
       "  'mas-usb-5716',\n",
       "  'mas-usb-5816',\n",
       "  'mas-usb-0117',\n",
       "  'mas-usb-0417',\n",
       "  'mas-usb-0517',\n",
       "  'mas-usb-0617',\n",
       "  'mas-usb-0717',\n",
       "  'mas-usb-0817',\n",
       "  'mas-usb-0917',\n",
       "  'mas-usb-1017',\n",
       "  'mas-usb-1117',\n",
       "  'mas-usb-1217',\n",
       "  'mas-usb-1317',\n",
       "  'mas-usb-1417',\n",
       "  'mas-usb-1517',\n",
       "  'mas-usb-1617',\n",
       "  'mas-usb-1717',\n",
       "  'mas-usb-1817',\n",
       "  'mas-usb-1917',\n",
       "  'mas-usb-2017',\n",
       "  'mas-usb-2117',\n",
       "  'mas-usb-2217',\n",
       "  'mas-usb-2317',\n",
       "  'mas-usb-2417',\n",
       "  'mas-usb-3117',\n",
       "  'mas-usb-3317',\n",
       "  'mas-usb-3417',\n",
       "  'mas-usb-3517',\n",
       "  'mas-usb-3617',\n",
       "  'mas-usb-3717',\n",
       "  'mas-usb-3817',\n",
       "  'mas-usb-3917',\n",
       "  'mas-usb-4517',\n",
       "  'mas-usb-4617',\n",
       "  'mas-usb-4717',\n",
       "  'mas-usb-4917',\n",
       "  'mas-usb-5017',\n",
       "  'mas-usb-5117',\n",
       "  'mas-usb-5417',\n",
       "  'mas-usb-5717',\n",
       "  'mas-usb-5817']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' Give a definition for the term \"artificial neural network\" and mention, how it resembles the human brain!': 'An artificial neural network is a massively parallel distributed processor with simple processing units that has the natural propensity to store experiential knowledge and make use of them.\\n\\nAn artificial neural network is similar to the human brain in two ways:\\n\\n1. The ANN works by the process of learning from its environment.\\n2. Interneuron connections called synaptic weights are used to store the knowledge gained.\\nArtificial neural network consists of:\\n\\n    . Largely parallel distributed processor\\n    \\n    . simple processing units\\n    \\n    . that has ability to store the experential knowledge and making it available to use\\n    \\nIt resembles to human brain in two ways:\\n\\n    . Knowledge is acquired from the environment by the network as learning process\\n    . Synaptic strengths called weights are used to store the knowledgeYOUR ANSWER HERE An artificial neural network is a massive distributed processor. It consists of several information processing units which are able to acquire and store knowledge.An ANN is a layered graphical model containing neurons and weighted connections, resembling the excitatory properties of the human brain. Weights of the ANN are changed after presenting it training examples from an environment, where weights are changed based on the training procedure used. Artificial neurons also are biased, just like real ones, adding a constant level of activation before being activated by a (nonlinear) activation function. Depending on the training procedure, both weights, topology or even activation functions may be learned.Artificial Neural Networks are large parallel processing units that have the natural ability to learn experiential knowledge. They are composed of interconnected neurons as basic units; which in turn cosists of weights, squashing functions and adder functions.\\n\\nANN resembles brain in the manner that like in human brain, it is composed of a network of neurons which help in learning by adjusting the synaptic weights of the connections between neurons. This enables it to learn experiential knowledge.An articial neural network consists of neurons. Each neuron can have several weighted inputs, an activation function and output. Usually several neurons are connected together. Often in layers. The network then calculates the output given an input to the network. The human brain works in a similar way. It also consits of neurons that are connected in several ways.An ANN is a\\n- massivly parallel distibuted Processor\\n- made up of simple processing units\\n- which have the capability of storing experiantal knowlenge\\n- and is made up for use.\\n\\nAn ANN resembles the brain because:\\n\\n1) it gets its knowlenge through a learning process from its environment.\\n\\n2) it stores its knowlenge in its interneuron connections (synaptic weights)A ANN is a massively distributed processor. It has the propensity to store experiental knowledge and make it available for use. The knowledge is gained throug a process of learning. The knowledge is stored in the weights between the neurons. This structure resembles the structure of the brain. Neurons are a the basic information unit in the ANN and act similiar to real neurons.An artificial neural network is defined as a learning machine which is divided by layers and each layer is composed by neurons. The neurons from different layers can be connected between each other, and give an output or multiple outputs by a given input. This structure is very similar with the neurological structure of our brain, where neurons are interconnected by synapses. Also important to mention, if a feature is really important for a given task, this wil have more connections and neurons participating (like in the human brain, the important humasn functions have more synapses).An artificial neural network is a graph of small and identical processing units that these small units called neurons and they are connected to each other in different architectures and the whole network adapt and itself to the environment inputs by trying to decrease the error or the cost function and increase its preciseness by manipulating the free variables of the network which are the synaptic weights.\\n\\nIt is similar to human brain because similar to the human brain we have many small processing units that are connected together and they react to the environment and learn from the environment.Artificial neural network is highly parralel processing. It has a mathematical model similar to human brain, which it was inspired from, as human brain does computation in an extremely parallel manner. Similarities also lay in terminology, ANN is using neurons that are smallest computing unit of a network, similarly to human brain.It is a massive parallel distributed processor made up of smaller processing units, that aquire knowledge through the environmnet through a learning process and makes it available for use. It resembles the brain in two ways:\\n\\n- Knowledge is aquired through a stimulating process in the environment\\n- The knowledge is embedded in the synaptic links (weights) of the neurons. ANN is a learning machine which is composed of neurons as units of computation. The ANN learns via interacting with its environment. The ANN has built-in capacity to dynamically adapt upon input stimulus.\\n\\nThe ANN is motivated from biological brain and resembles human brain in terms of its localized representation for the inputs. In terms of motor cortex, the sensory stimulus to diffrent body-parts activates local part of the brain, similar to ANN local representation of similar type of input.A neural network is a massively parallel distributed prcoessor made up for simple processing units that has a natural propensity for storing experiential knowledge and making it available for use. It resembles the brain in two respects\\n* Knowledge is acquired by the network from its environment through the learning process\\n* Interneuron connection strengths known as synaptic weights, are used to stor the acquired knowledge.Artificial neural network is a massively parallel distributed processor which consists of one or more processing unit called neuron. It resembles the human brain for that it acquires knowledge from the environment through learning process, and that the acquired knowledge is stored in the synapses.Definition:\\n1. Artificial neural networks are massively distributed parallel processor.\\n2. It is made up of small units, \\n3. Which has the propensity for storing the experential knowledge.\\n4. And making it available for use. \\n\\nIt resembles the brain in 2 aspects. \\n1. Similar to the brain, artificial neural network does the process of learning from the environment. \\n2. It as a pair of inter neuron links known as the synaptic weights, which is used for storing the information. Artificial neural network is massive parallely distributed processor. It comprises of small processing units called neurons. It learns from experiencial knowledge which is then stored and can be used for making predictions. It resembles human brain in 2 ways:\\n* It learns from experiencial knowledge\\n* Knowledge is stored in synaptic interneuron connections.YOUR ANSWER HERE: Artificial neural network is a massively distributed parallel processor which is composed of simple processing units called neurons, which have the natural propensity for storing experiential information and making it available for use. It resembles the human brain in the following aspects.\\n- Knowledge is acquired by the network from its environment through a learning process.\\n- Synaptic links are used to store the acquired knowledge.ANN is a learning machine which can perform complex parallel computation. It has the ability to learn through the interactions withthe environment and store the learned knowedge. \\n\\nIt resembles the human brain in performing complex learning tasks, acquiring information, apadpting to the environment, and exploiting the acquired information.An artificial neural network is a massively distributed parallel processor made up of simple processing units that have the natural propensity for storing experiental knowledge and making it available for future use.\\n\\nIt resembles the brain in the following ways:\\n\\n1. Artificial neural networks have the ability to acquire knowledge from the environment in which they are are embedded.\\n2. Inter-neuron connection strenghts called synaptic links activate each neuron during the learning process.An Artificial Neural Network is a massively parallel distributed processor which interacts with its surrounding environment, with a propensity to store knowledge and make it available to use.  \\nIt resembles the brain in two aspects:  \\n1. It has the ability to learn from its environment  \\n2. The knowledge is stored in synaptic weightsArtificial neural network is massively distributed paralled processor containing simple processing units and has natural propensity to store experiential knowledge and use it.It resembles the human brain in two aspects, it gains knowledge from the environment and adapts the synaptic weight to store the knowledge.It is a massively parallel distributed processor consisting of simple processing units, which can store experiential knowledge and make it available for use. it resembles the human brain in 2 ways: 1. knowledge is acquired from environment through a learning process; 2. interneuron connections are used to store the experiential knowledge.Artificial neural network is a massively parallel distributed processor that is made up of simple processing units called neuron. It can replicate human brain by storing information in their weightsArtificial neural network is a **massively parallel distributed processor** with synaptic links that can able to **store experimental knowledge** and make it available for use.\\n\\nIt resembles human brain in two ways,\\n\\n* Knowledge is acquired by the neural network from its environment through learning process.\\n* Interneuron connection called synaptic links stores the acquired knowledge.Artificial neural network are the network of the units that learn data from the environment and store them using synaptic weights.\\n\\nThe structure of the artificial neural network is similar to human brain. It has neurons, ie., the store units and the axoms called synapses which link the stored data.Artificial neural network is massive parallel processor made up of simple processing units called neurons. \\n\\nThey are capable of storing experential knowledge and make it available for later use. \\n\\nSimilarity to human brain: \\n1. they learn from the envirnoment \\n2. they store knowledge as synaptic weight in the interneuron connection An artificial neural network is a highly distributed processor which consists of several simple processing units. It resembles the human brain, because the processing units are neurons, which are connected with weights. The human brain also consists of neurons.A massively distributed processor, consisting of single processing units that have a natural prospensity of storing experimental knowledge and making it available for use.An artificial neural network consists of neurons, which are small computation devices,and synapses, the connections between the neurons. This resembles the brain because it also has neurons and synapses. Also a artificial neural network has weights, which are used to store learned features from the environment. Like the brain a neural network learns from the environment. An artificial neural network also has an activation function, which creates the output.An artificial neural network is a highly parallel computation model with learning and memory capacities. Similar to the brain it learns from the environment by strengthening the synapses between neurons. Once a task is learned it can be quickly used by reactivating those learned synapses.An artificial neural network is a highly parallel working machine which consists of simple processing units (neurons) wich are connected to each other in layers. they are function approximators \\nthe brain is resembled in the architecure, the processing units and thge weights and how the learning process takes place and the properties of the brain: fault tolerance, parallel computing, ... An ANN is a massivly parrallel distributed learing machine made up of small computational units. Computational units are connected via synapses defined by a weight. It resembles the human brain in two aspectes:  \\nArtificial neural network is massively parallel distributed processor made up of simple computing units called neurons which aquires knowledge from environment through learning. It resembles brainlike structure in two ways, \\n\\n1. It aquires knowledge through learning and experience \\n2. It stores knowledge in interneuron connections called synapses. ANN is huge parallel distributed processor ,\\n\\nconsist of simple processing units and \\n\\nwhich has propensity of storing experintial knowlegde \\n\\nand making it available for use.Artificial neural network is a massively parrallal distributed processor made up of simple processing units which has a natural propensity to acquire knowledge from the environment and make it available for future use.\\n\\nIt resembels the human brain in following ways.\\n\\n1. Both of them acquire knowledge from the environment.\\n2. The neurons are connected by synapses cahrecterized by their weights which can be adjusted.YOUR ANSWER HERE\\n\\nAn artificial neural network is a massively distributed parallel processor made of simple processing units. It has natural propensity to store experential knowledge and it makes the knowledge available for further use.\\n\\nAn artificial neural network uses inter neuron connections called synaptic weights to store the knowledge acquired knowledge which is very similar to how human brain works.Ein Neuronales Netz ist ein massiver Parallele Rechen Methode, die aus mehreren einfachen Rechen Einheiten besteht, durch Erfahrung lernt und das Wissen verfügbar macht. Es nahmt das Menschlische Gehirn nach dadruch dass es durch Erfahrung lernt, und dadurch, dass das Wissen in Form von Axionen, hier Gewichte, zwischen den einzelnen Neuronen speichert. ANN is a massively distributed processor, consisting of simple processing units called neurons. These neurons in terms of ANN are similar to neurons in human brain. Both neurons are characterized by synapses(connection links). They represent connections used for data flow between neurons. In both ANN and Human brain, the knowledge is represented by its very structure and activation state of neurons.  ', 'Define the mathematical model of a neuron, use the appropriate technical terms!': \"A neuron is the simplest processing unit of a neural network which has:\\n\\n1. synaptic weights to store the knowledge gained.\\n2. Adder function (linear combiner) which adds the weighted values of the input signals to produce the local field.\\n3. An activation function which squashes the local field to a range of values.\\n\\n$ \\\\phi(\\\\sum_{i=0}^{N} w_i \\\\cdot x_i) $Mathematical model of a neuron is given as :\\n\\n   y = $\\\\phi(V)$ , where activation function is applied to local field(V)\\n   \\n   V = $\\\\summation (w_{i}x_{i} + b)$  . Local field is weighted(w) sum of inputs(x) plus bias(b)\\n   \\n   YOUR ANSWER HERE A neuron is an information processing unit. It consits of: inputs associated with weights, sum of inputs and an acitvation function\\n\\nInput vector $x$\\n\\nWeight matrix $w$\\n\\nNet input $net=\\\\sum x^Tw$\\n\\nNet output $o=\\\\phi(net)$A neuron consists of three basic components:\\n   - *Synaptic Weights*: The synaptic weights are connections between neurons and are adjusted through training.\\n   - *Squashing/Activation Functions*: The squashing functions may be non linear or linear functions that that are applied to the signals from the neurons\\n   - *Adder Functions*: The adder functions help in combining outputs from several neurons. \\n   \\n\\n$N$ number inputs, $x_i$ input i, $v_j$ local field, $\\\\varphi(v_j)$ activiation function, $y_j$ output, $w_{ji}$ weight from node i to j\\n\\n$y_j = \\\\varphi(v_j)$\\n\\n$v_j = \\\\sum_{i=0}^{N}w_{ji}x_i$A neuron is a simple processing unit of an ANN, that is made up of\\n\\n- the synaptik links which are defines by a weights $(w_1,...,w_n)$\\n- a adder function that combines the weighted input $(w_i*x_i)$ plus some bias $(b)$ to the local field  $(\\\\sum{w_i*w_i}) +b=v$\\n- a activation function phi that squaches the local field to the output $(phi(v)=y)$ The neuron consists of synapses/connecting link each characterised by a weight. A linear combiner sums up the weighted sum of inputs to a local field. The local field is then passed through an activation function. The result of the activation function is the output.A neuron is defined by the following elements:\\n- A number of input values x\\n- A number of weights w\\n- A bias b\\n- An activation function $/phi$.\\n\\nThe inputs x are multiplied with the weights, and the result is summed with the bias (also, the bias can be used just as a weight value b and a single connetion with an stable input equal to 1, for mathematical simplicity). The resulting value, known as local field (v), will be the input to the activation function.\\n\\nThe mathematical model can be summarized in the formula:\\n\\n$v = \\\\sum^{n}_{i = 0} x(i)*w(i) + b$\\n\\n$y = \\\\phi(v)$A neuron consists of a set of inputs and a bias which these inputs and predefined bias will be multiplied by a weight and then we have sum the results of all the inputs and bias multiplied by the weights which called induced field and after that we send this to an activation function which can be a linear or non-linear function and the output of this function is the final output of our neuron. Neuron is a simplest computation unit of a neural network that consists of input variables, weights, bias, summation term (combiner), activation function and output variables.The neuron is the basic processing unit of a neural network and is made of three main component:\\n- Weights: $w_1, w_2, ...,w_n$\\n- Adder function: it is the linear combination of the input and weights plus bias. (induced local field) $v = \\\\sum w_i x_i + b$\\n- Squashing function: it is the activation function applied to the local field used to limit the output of the neuron. $\\\\phi(v)$\\nA neuron is a computational unit composed of\\n+ synapses which are stored in the form of weights $w$. These are the variables that can are dynamical.\\n+ summing function that computes the weighted sum of inputs: $v = \\\\sum_i (w_ix_i)$\\n+ activation function $\\\\phi$: gives nonlinear nature to network, determines and normalizes the output produced by neuron. e.g. sigmoid function\\n+ bias: another synaptic tunable variable with input 1. Therefore the net output of neuron: $ y = \\\\sum_i (w_ix_i) +b$.The following equations describe a nonlinear model of a neuron, labeled k.\\n\\n1)u_k = sum from j=1 to m w_{kj} x_{j}\\n\\n2)y_k = phi(u_{k} + b_{k})\\n\\nwhere x_{j} are the input signals; w_{kj} are the weights of the neurons; u_{k} is the linear combiner output due to the input signals; b_{k} is the bias; phi() is the activation function; and y_k is the output signal of te neuron.A neuron is a processing unit that contains three main components: a set of synaptic weights that connect the neuron with other neurons; an adder that computes the induced local field, or the weighted sum of the signals flowing through the neuron; an activation function that constrains the magnitude of the output signal from the neuron.YOUR ANSWER HERE\\nA Mathematical model of a neurons consits of a \\n\\n1. A set of synaptic links which are classified based on weights(w1, w2, w3...w_n)\\n2. It consits of a adder function, which performs the weighted sum of the inputs and the bias.\\n$\\\\Sigma_{i=1....n} w_n.x + b$\\n\\n3. It consists of an activation function, used to minimize the amplitude of the neuron output.\\n$\\\\Phi(\\\\Sigma_{i=1....n} w_n.x + b)$A mathematical model of neuron comprises of 2 main units:\\n* Adder functions: it sums up all the product of all synaptic connections and inputs of neuron\\n* Synaptic weights: these are interneuron connections in which the knowledge is stored\\n* Activation function: it is used for introducing non-linearity YOUR ANSWER HERE: The neuronal model consists of the following:\\n- Synaptic links characterized by their weights which connects the network to the environment it is embedded in.\\n- An adder function which sums up the weighted inputs and outputs the induced local field of the neuron.\\n-  An activation function which takes the induced local field of the neuron as it's input and limits the output of the neuron.A mathematical model of neuron consists of 3 important parts.\\nA neuron is the smallest computaional node with:\\n1) Input vectors : set of vectors of a certan dimension to train the model\\n2) Weights (and biases): each of the input vectors are weighted using weight vectors in accordance withthe output that is required. Bias is added when necessary.\\n3) Activation function : The linear combination of weights and inputs are passed through the activation function which produces an output.The neuron is the fundamental processing unit of an aritificial neural network that is characterised by the followig features:\\n\\n1. A neuron has a set of non-linear synaptic links, an externally applied bias, and possibly one or more linear activation links. The bias is represented by a synaptic link from an input fixed at +1.\\n2. The synaptic links of the neuron weight the respective inputs.\\n3. An adder function (linear combiner) computes the weighted sum of the inputs to the neurons.\\n4. An activation function (squashing function) limits the amplitude of the neuron's output.Let $x_1$, $x_2$, ... , $x_N$ be the inputs to the neuron, $w_i$ be the corresponding weights of connections, $b$ be the bias and $\\\\varphi(.)$ be the activation function. \\nThen, the induced field $v$ is given by -\\n\\n$$v = \\\\sum_{i = 1}^{N} w_i .x_i + b$$\\n\\nThe output $y$ is given by -  \\n$$y = \\\\varphi(v)$$The mathematical model of neuron has three parts:\\n- a set of synapses or connencting links characterized by weight ,w .\\n- an adder function that calculates the weighted sum of inputs plus some bias\\n- an activation function (squashing function) to minimize the amplitude $v_k = \\\\sum_{j=1}^{m} w_{kj} x_j + b_k$, $y_k = \\\\phi(v_k)$, $w_{kj}$ is the synaptic weight  connecting neuron k and input data j, $x_j$ is input data, $b_k$ is bias, $v_k$ is induced local field, $y_k$ is output of neuron.A neuron consists of a synapse connecting link, an adder function or linear combiner and an activation function.\\n$$v = \\\\Sigma w_i \\\\cdot x_{i} + b$$,\\nwhere $x_i$ is the input, $w_i$ is the weight and $b$ is bias.A neuron is a basic information processing unit that have a adder function to compute **weighted sum of inputs plus bias** and apply activation function on the result.\\n\\n$$ \\\\phi(v) = \\\\sum\\\\limits_{i=1}^n \\\\omega(i)x(i) + bias $$Each neuron has a set of inputs and their respective weights.\\n\\nThe local field is,\\n\\n$v = \\\\sum(w_{ij} * x_i)$\\n\\nThe local field is passed through a activation function.\\n\\nSo the output of the neuron is,\\n\\n$y = \\\\phi(v)$\\n\\n$y = \\\\phi(\\\\sum(w_{ij} * x_i))$The neurons are the basic processing units in neural network\\n\\noutput of the neuron = $  \\\\phi (\\\\sum w_{i} x_{i})$ \\n\\n\\nthey consist of three parts\\n\\nSynaptic weight: the connections between the neurons. characterised by weights\\n\\nAdder function: calculates the weighted sum of the inputs of the neuron\\n\\nActivation function: limits the amplitude of the output of the neuron. ($\\\\phi$)The model of a neuron consists of synaptic weights which are applied to the input signals. The weighted inputs are then summed which gives the local field. This local field is put into an activation function whose output will be the output of the neuron.$y = \\\\sum_{i=0} \\\\Phi(w*x_i)$ \\n\\nA neuron consists of inputs $x$, synpatic weights $w$, an extra input $w_0$ which is fixed to 1 for the bias, an Adder function, that creates the local field $v$ and a squashing function $\\\\Phi$.$y = \\\\sum f(wx + b)$, where w are the weights, which change the input according to the learned weights, x is the input from the environment, b is the bias, which shifts the learned decision plane, and f() is the activation function, which limits the output to a desired region of values.A neuron consists of one or multiple inputs which are gathered by a summation function. The hereby induced local field of the neuron is processed by a squashing function and generates the output of the neuron.A neuron consist of input connection links with a synaptic weight, a bias, an adder which adds the input singnals and the bias and produces the local field. The local field is processed by the activation function and produces the output of the neuron. A neuron consists of input nodes x_1 to x_n and weights w_1 to w_n, a linear combiner v= SUM( $ x_i * w_i $) + b, where b is some bias. The result v is called local field and is used as input for an acivation function $ phi(v) $Neuron is consists of three units. \\n\\n1. Synaptic links characterizex by weights which linearly ways the input.\\n2. Adder which adds weighted inputs to generate local field\\n3. Activation function which is nonlinear function sqashing the output of the neuron1) Neuron is consist of sysnaptic  links which measured in terms of weights. neuron is given with inputs.\\\\\\n\\n2)it has adder funtion or combiner which adds all the inputs mulitplied by the weights and bias is extra input to the neuron as well.\\n\\n3) it has a activation link which limit the amplitude of the output of the neuron.\\nNeuron is the basic information processing unit which is the main component of a neural network. A neuron is charecterized by its input ($x_i$), synaptic weight ($w_i$) and activation function $\\\\phi(v)$. Mathematically it can be modelled as $\\\\phi(w_ix_i)$. Activation function bounds the input to a certain level.YOUR ANSWER HERE\\n\\nA neuron has three components\\n\\n* Synaptic weight: w\\n* Adder function: it multiplies input x with the weight\\n* Activation function: It squashes the output of the adder function. Sigmoid, hyperbolic tangent, Rectified linear unit etc.Mehrere Eingänge werden in einer Summationseinheit aufaddiert. Die Summe wird als Eingabe für eine Aktivierungsfunktion verwendet. Derern Ergebnis wird an die nachfolgenden Neuronen weitergegeben.A neuron consist of set of inputs that takes data from environment. Each neuron contains synapses(connection links) that are characterized by weights. All inputs are connected to the summing (adder) function, that computes weighted sum of all input values. This weighted sum is called local field of neuron. The value of this local field (V) is limited(squased) by an activation funtion $\\\\theta(V)$. The result from this squasing funtion is output of a neuron ($y = \\\\theta(V)$).  Additionally, a bias term $(b)$ is added to the input, and its value is always 0, but its associated weight is being changed over training period. Finally, output of neuron is $y = \\\\theta(V)$, where $V = \\\\sum W_j * X_j + b$\", 'Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\n': \"1. Label one class a positive with label +1 and the other class as negative with -1.\\n2. Augment the data with an additional value for the bias term.\\n3. Invert the sign of the data in the negative class.\\n4. Randomly initialize weights.\\n5. If $w^T \\\\cdot x <= 0$, update weight by $ w(n+1) = w(n) + \\\\eta x(n) $, else leave the weight unchanged.\\n6. Continue step 5.\\n7. terminate when there is no longer a change in any weight.1. Initialization: n(time step or iteration) = 1 and weights are small but randomly initialized\\n2. Activation of perceptron:  Apply training pattern to activate the perceptron\\n3. Compute Output: Apply Activation function to the local field(weighted sum of inputs plus bias)\\n4. Adjust Weights: Adjust weight if current output(y) != desired output(d)\\n5. Continuation: We continue by increasing n during each iteration and repeat from step 2 untill all input pattern are applied to network and also error is minimized YOUR ANSWER HERE: \\ny denotes the actual result, d denotes the desired result\\npositive train error: y = 0, d=1 $w_{new} = w_{old} + x $\\nnegative train error: y = 1, d = 0 $w_{new} = w_{old} - x$initialize weights with zero or small values;\\n\\nsample data point, feed into network;\\n\\ncompute net output, use the step activation function;\\n\\ncompute error $e=(d-o)$, where d is the true label, o is the predicted label;\\n\\ncorrect weights based on $w(t+1)=w(t)+\\\\alpha(d-o)x$, where alpha is the training rate and x is the input pattern;\\n\\nrepeat for each pattern until convergence is reached;For this case, the parameters that need to be learned are the slope of the line and the intercept. These are the parameters for the weight vector.\\n\\n\\n1. Initialize random small values for weight vector.\\n2. For input_data $x_i$ in Training Data:\\n     - Apply the input to the weight vector.\\n     - e = the difference between the local field and the desired output $(d_i-y_i)$\\n     - Update weight: w(n+1) = w(n) + $\\\\eta e x_i$     $\\\\varphi(v) = \\\\tanh(v)$, single node network, $\\\\mu$ learning rate\\n\\nrepeat as long as error is too high:\\n\\n1. present sample to network and collect output.\\n2. compare actual output with desired output (d).\\n3. If not equal adapt weights: $w_i(n + 1) = w_i(n) + \\\\mu(d-y)x_i$given $k$ date points $(x_i,y_i)$ and $y_i\\\\in\\\\{1,-1\\\\}$\\n\\ngiven a learning rate\\n\\nfor each point i\\n\\n    add a bias 1 so that point i == (1,x_i,y_i) ;\\n    \\nfor each point i there y_i == -1\\n\\n    point = -1 * point;\\n    \\nw= Nullvector;\\n\\nb = 0;\\n\\nconvergance = false;\\n\\nwhile(convergence == false)\\n\\n    convergance = true;\\n    \\n    for each point i in the training set:\\n\\n        if(w*x<=0) do\\n        \\n            w = w+learningrate*point_i; \\n            convergance = false;\\n    weights # a weight vector\\nphi = activation function\\neta = learning rate\\nfor each datapoint (x_i,y_i) do:\\n    weights[i] = weights[i] + eta * (x_i[i]-y_i)*weights[i]1: w, b = init_weights_bias() // the weights can be initialized to 0 or random initialized\\n\\n2: n = 0\\n\\n3: WHILE !stop_criteria() DO // iteration until stop criteria is fulfilled\\n\\n4: y = w(n) * x(n) + b // calculate output\\n\\n5: IF x is in C1: e = 1 // if the x belongs to class C1, error i 1, otherwise is -1\\n\\n6: ELSE IF x is in C2: e = -1\\n\\n7: w = w + e * x // update weights using the calculated error\\n\\n8: n = n + 1\\n\\n9: END\\n\\nThe stop criteria can be, if the number of misclassified input data is 0, then stop.The learning process consists of three main steps:\\n\\n1- Positive error:\\n    - calculate the error of all the data sets in the learning set \\n    - change the w(weight): w(n+1) = w(n)+positive error\\n    - seperate the data points based on the new w\\n2- Negative error:\\n    - calculate the error of all the data sets in the learning set \\n    - change the w(weight): w(n+1) = w(n)+negative error\\n    - seperate the data points based on the new w\\n3- No error: \\n    - when we have no error this is the end of the trainingDefine a bias in order to be able to trigger to which class data points will be classefied to. Assign initial randomly chosen weights, use a squashing function for example McCullon Pits, start training proccess and stop when error of output and desired output has reached desired percentage.Initialize the weight vector $\\\\hat{w} = 0$\\n- do\\n-    for every training sample x,d\\n\\n         $v = \\\\sum w_i x_i + b$\\n         $y = \\\\phi(v)$\\n         \\n         if d is not equal to y then\\n             $e = d - y$\\n             $w = w + \\\\eta e(i) x_i$\\n-  until convergence\\n   **Pseudo code**\\n+ initiate weights and bias randomly.\\n+ compute output for the given input data $ y' = \\\\sum_i (w_ix_i) +b$.\\n+ compute error between computed $y'$ and desired output $y$.\\n+ update weights: $w(n+1) = w(n) + \\\\eta (y-y') x$\\n+ stop when the error is below some specified threshold or becomes zero in case of data that is perfectly linearly separable.YOUR ANSWER HEREInitialize the perceptron with each weight equal to 0: $w(0) = 0$.\\n\\nPresent the labeled examples $(x_i, d_i)$ to the perceptron.\\n> for each example $(x_i, d_i)$\\n>> Compute actual output $y_i$ and error signal\\n\\n>> Update weight based on the dlelta rule: $w(n+1) = w(n) + \\\\eta (d(n) - y(n)) x(n)$We use threshold function as activation function. \\n \\n if w.x + b >= 1 \\n \\n label class 1.\\n \\n else label class 0.e(n) = current error  <br>\\neps = convergence criteria  <br>\\nn = learning rate  <br>\\nwhile (change in e(n) not less then eps) {<br>\\n    calculate error e(n) <br>\\n    w(n+1) = w(n) + n e(n) x(n)  (Widrow Hoffmen rule) <br>\\n    }YOUR ANSWER HERE: Perceptron learning algorithm:\\n- Initialize the network by assigning random weights to the synaptic links.\\n- Calculate error as the difference of the desired output with the actual output.\\n- If the input is misclassified with positive error, $w_(new)$ = $w_(current) + input$.\\n- If the input is misclassified with negative error, $w_(new)$ = $w_(current) - input$.\\n- If the input is correctly classified, no changes are made in the weights.\\n- Repeat from step 2 as long as the error is under some defined threshold value.The linear binary classifiable data consists of input vector $X$ which when multipled with weights and added bias, fall into class+ or class- depending on the linear combination output of $WX + b$ being above 0(+) or below 0(class -).\\n\\nAlgo:\\nPara,meters : X,Y(desired output), W, b\\n\\n1) weight vector W is initialized with small random values.\\n\\n2) Input vector is chosen with a probabiity and output is computed using $WX + b$ . If the class Y of vector X is + and output is $<0$, or if the class of X is - and output if >0, then the weights are updated accordingly.\\nOtherwise weights are left unchanged.\\n\\n3) iterated over other input vectors until convergence of output.1. Initialization : At time step  n(0), initialize weight vectors with random values $w_j(0)$\\n2. Activation : Apply the input example $(x_i(n),d_i(n))$ to activate the perceptron with heavyside step function as the activation function. \\n3. If output of the perceptron $y(n) \\\\neq d(n) $, adjust the weight vector using the rule : $w(n+1) = w(n) + \\\\eta x(n)(d(n) - y(n))$\\n4. Go to Activation and repeat until no more change in weight vector is observed1. Inputs X: $x_1$, $x_2$, ... , $x_N$  \\n2. Desired outputs y: $y_1$, $y_2$, ... , $y_N$  \\n3. Initialize weight vector $w$ to random small values  \\n4. For each data point $x_n$ in X:  \\n     Calculate $\\\\hat{y}_n$ from $w$ and $x_n$  \\n     Calculate error $e_n = y_n - \\\\hat{y}_n$  \\n     Update $w$ according to delta rule    \\n   end\\n    YOUR ANSWER HEREapply input data to input layer and initialize small values weights\\n\\nminimize error according to difference between desired signal and output signal\\n\\nassign the test vector the class that has smallest error1. Compute the initial weights for all input vector\\n2. Apply matrix multiplication from input to weight vector\\n3. Apply linear combiner\\n4. Apply activation function to produce the output\\n5. Compute the error\\n6. Update weights* Randomly assign values to initial weights\\n* Run the perceptron network and calculate the error (e = y-d) where, e is error, y is output and d is desired response.\\n* Update the weights based on the error.\\n* If error is positive, add the error with the input and update weight.\\n* If error is negative, subtract the error with the input and update weight.\\n* If there is no error, don't update the weights.\\n* Repeat the above process until the calulated error is approximately equal to zero.YOUR ANSWER HEREw = [random number betrween -1 and 1]\\n\\n\\nfor every data in training set\\n\\n\\n{\\n In the first layer: \\n \\n calculate the weighted sum  using adder function\\n \\n calculate the output of the activation function\\n \\n In the ouput layer\\n \\n calculate the output y  \\n \\n calculate the error e = d - y ; d- desired output\\n \\n \\n change the weights using the formula $ \\\\delta w = \\\\eta x_j e_j$\\n\\n} \\n\\ncontinue till the error converges     Initialize as many random weights as the dimension of the data points\\n\\n    For each data point:\\n        if the output matches the desired output\\n            do nothing\\n        else:\\n            change the weights in the direction of the datapoint so that the datapoint is classified correctly\\n        end if\\n    end for\\n\\n    if some weight was changed:\\n        start again with the for loop\\n    end if1. Initialize the weights at random or as 0.\\n2. Activate the Perceptron by giving an example.\\n3. Compute the actual output of the neuron.\\n4. Adjust the parameters of there perceptron.\\n5. Continue until convergence is achieved.\\n\\nw = rand\\n\\ny = sum($\\\\Phi$(w*x))\\n\\nfor w_i in w:\\n\\n    w_i = w_i+$\\\\eta$*e*y Initialize the weights randomly.\\n\\n$y = \\\\sum f(wx + b)$, compute the output of the perceptron using the input x, the weight w, the bias b and the activation function f().\\n\\n$e = d - y$, calculate the error by substracting the actual output from the desired output.\\n\\n$w_{new} = w_{old} + learning\\\\_rate \\\\cdot x \\\\cdot e$, update the weights with this formula. The learning rate is a parameter which changes how fast the perceptron learns.for n iterations\\n\\n    for each datapoint d\\n \\n        error = desired - output\\n        \\n        if error > 0\\n        \\n            weights = weights - error\\n            \\n        if error < 0\\n        \\n            weights = weights + errorpick random decision boundary\\nwhile one of data points is in wrong class\\n    turn decision boundary by using vector of wrong data point\\n    (negative rule or positive)training_set := set of labeled linear seperable data points  \\nw := weight vector with dimension of input data  \\nv := local field  \\nphi(v): activation_function (threshold function)  \\ny:= output  \\ne := error (y - d) where d is the desired output from labeled training data  \\nn := learning rate (0.1)  \\n  \\nassign random values for w  \\n  \\nfor x in training_set:  \\n    v = sum(x_i * w_i)  \\n    y = phi(v)   \\n    e = y - d  \\n    w = w + n*x*e // delta rule  \\nend     \\ninitialize weights w and bias b\\n\\nset learning rate n\\n\\nset error_threshold (upper bound on error)\\n\\nwhile error < error_threshold :\\n\\n    for every datapoint x in tarining dataset :\\n        y = [w, b] . [x, 1]       (bias is represented as weight of fixed input 1)\\n        if y is positive then x belongs to C1 otherwise to C2\\n        store above predicted class.\\n        find error in predicted output with respect to the labels\\n        store error e\\n    e_sum = sum of all errors e for every data point \\n    w = w + n * e_sum\\n    \\n  for a binary classifier we can use threshold activation funtion.\\n\\n1) randomly initilize the weights \\n\\n2) you calculte the output of the neruon \\n\\n3) find out the error by subtracting expected output and current output.\\n\\n4) modify the weights related to that input with respect to the error.\\n\\n5)repeat the process 2-4 till the you get minimal error.\\nYOUR ANSWER HEREYOUR ANSWER HERE\\n\\ncontinue_process = true\\n\\nw = randomly_initialize()\\n\\nwhile continue_process\\n\\n  for x in list of points\\n  \\n    y = w.x\\n    \\n    diff = d-y  // d is  the desired output\\n    \\n    if(diff >= 0)\\n    \\n      w = w + x\\n      \\n    else\\n    \\n      w = w - x\\n      \\n      \\n  if all points are classified without error\\n  \\n    continue_process = falseStart: Gewicht zufällig wählen.\\nüber alle datensätze iterrieren\\n    datenpunkt klassifizieren und ergebniss überprüfen\\n    fals falsch klassifiziert, gewicht soweit anpassen, das dieser Punkt korrekt klassifiziert wird, in die\\n        richtung in die eine kleinere korrektur notwendig ist\\nwenn alle datenpunkte korrekt klassifiziert wurden terminiere\\nn<- learinig rate\\n\\nRepeat until the MSE is small enough:\\n\\nt=t+1\\n\\n    for each point in training set do:\\n\\n        compute local vield of percepron: V = W*X\\n\\n        apply linear activation function: $y =  \\\\theta(V) = V$\\n\\n        compute current error: e = (d-y)\\n        \\n        \\n        apply delta rule: W(t+1) = W(t) + n*e*X \\n\\n    end\", 'Explain classification and regression; what is the difference?': \"Classification:\\n\\nIn classification, the output produced by the NN is a discrete value which indicates which class the input belongs to.\\n\\nRegression:\\n\\nIn regression, the output produced by the NN is a continuous variable. This could be used for instance, to approximate a continuous function.In classification, output values are always discrete.\\n\\n\\nIn regression, output values are continuousYOUR ANSWER HERE\\nA hyerplane is given by y = w*x + b . Regression wants to determine w\\n\\nClassification wants to assign a class to a set of observations.\\n\\nRegression wants to determine separating hyerplane, classification wants to label data points with a classIn classification tasks, we assign discrete labels to data points of our training dataset, either being assigned a specific label or not (binary). For supervised learning, these datapoints are labeled with a label vector ground truth. In regression, we try to model a function which fits the data points of the training data, and thus model a function with continous values._Classification_: \\n   - It refers to classifying given data into discrete classes.\\n   - The output is discrete values.\\n   - Use for activity like pattern recognition, etc.\\n\\n_Regression_: \\n   - It refers to estimating the value of some continuous function given an input.\\n   - The output is continuous value.\\n   - used for activities like motor control, etc.In classification we try to assign classes to input data. Regression we want the network to behave like a given system/formala. This can also be a time series of input and output data.In classification the goal is to saperates points into different classes. The outcome is a class lable. \\n\\nRegression trys to fit a hyperplante to a point cloud best, so that future data is representet by that hyperplane best (LMS). It trys to minimize the distance to all data points. The outcome is a countinius variable.Both are learning tasks of a ANN. \\nIn classification the goal is to assign a class label to new datapoints.\\n\\nIn regression the goal is to eastimate a unkown function.\\n\\nThe only difference between both is that classification uses discrete class labels, while in  regression a continuous output is usedThe approach of classification is to classify sets of input data into their correct classes (for example, used in pattern recognition). The approach of regression is to approximize to a defined function f by calculating the error between this function and the result of an algorithm. THe difference is that, the classification approach is applied to a discret data (the samples are the different points of the input space), and regression is an analogic approach where the whole function must be approximize (for any input given).- Classification: In classification problems we have different groups of data that have some common properties and after training we want that our model can detect the class of the new sample correctly\\n- Regression: In regression we have a series of values and we want to use the previuos values in this series and predict the next valueClassification is a problem of destinguishing to which discrete classes input variables are to be assigned to, regression is estimation of the output, by figuring out the continuous trend of the whole dataset.Classification if to assign a class or category to the data, while regression is when you fit the data to a function.+ Regression: learns model/function that can predict other unseen data well. Target/output is real spaced.\\n+ Classification: learns a model that classifies/maps input to a discrete target label. Targetlabel/output is binary/discrete.Classification describes the application, in which a sample is assigned to one specific pattern of the problem. In comparison to regression is the output deterministic an not continiously. In regression the output is continuous describing Classification is the task of classifying the input signals into a finite number of groups, so the output is a number that indicates a certain class. Regression is the task of approximating a function by estimating the values given the input signals, so the output can be any real number.Classification: \\nWe need to predict the output data discretely. That is the output space is a discrete space. \\n\\nRegression:\\nWe need to predict the output data continuously. That is the output space is continuous space\\n\\nThe main difference is the discreteness and contionousness.Classification is a problem of assigning a particular class to each data point in a given dataset. <br>\\nRegression is a problem of fitting the given dataset on a particular hyperplane which can be used for representing the given data. It finds the hyperplane which minimises the mean square error. YOUR ANSWER HERE:\\n- Classification is a problem of assigning labels or classes to the input. The output is a discrete variable.\\n- Regression is a problem of assigning a continuous variable to the input.\\nClassification is a problem of catergorization into discrete classes where as regression is a problem in a continuous space where the goal is to ether minimize or maximize a cost function.\\n\\nClassification is the process of dividing a set of discrete inputs into classes corresponding to similar patterns such as clustering.\\nRegression could be finding a pattern of the distribution of the data such as ftting a line.Classification in machine learning is used to find a decision surface in the form of a hyperplane that can separate a set of input examples (or set of patterns) into their respective classes. Regression on the other hand is used to find the parameters (i.e, the weight vector $w$ and the bias b) for the function thatcan best fit the given data points $\\\\{x_i,d_i\\\\}$ . Thus classification deals with predicting the class label for discrete data points whereas regression deals with fitting a continuous real valued function.Classification is separating the data into classes and the output is a discontinuous variable. Regression is fitting a model and the output is a continuous variable.Classification is about classifying the given data into different classes, where as regresssion is about finding the local/global minima.We use perceptrons to classify the data and we use unconstrained optimization techniques like newton's method to find regression.classification: assign a test data to a class that is prescribed\\n\\nregression: approximating an unknown function with minimization errors for input-output mapping. Classsification: In classification, the output variable takes class labels or identifying group membership<br>\\nRegression: In regression, the output variable takes continuous values or predicting a responseClassifaction problem is used to classify set of data points into specific groups.\\n\\nRegression is used to predict time series data.\\n\\nClassification works on discreate set of values and regression works on continuous values.Classification: Classification is done between the classes. The machine determines to what class the data belongs to.\\n\\nRegression: Regression is a expecting output for an input. The machine learns from the given data and models a function and when new input is given it expects the output.\\n\\nDifference: Classification is discrete output where as Regression is a continuous output.Regression: \\n\\nTries to fit a line are curve among the given points\\n\\nThe have continuous output\\n\\nthe output is a function\\n\\nClassification: \\n\\nTries to classify the given points into two or more calsses\\n\\nThey have a discrete output\\n\\nthe output is a value representing the classClassification: Each datapoint is assigned with a class\\n\\nRegression: Each datapoint is assigned with a value\\n\\nIn classification we assign classes or labels to datapoints. The error signal here can be only true or false. In regression we try to learn a function, the error for each prediction can be a number.In classification a binary pattern has to be partitioned into the two classes. In regression a line has to be fitted closest to some datapoints. The difference is, that in Classification mthe output is a single class label, while in regression the output is continuousIn classification the input data is split in 2 or more classes. The goal of the neural network is to learn the input data and then be able to classify new input data into the classes. Based on the learned information the network then maps input data into one of the classes, which is discrete space.\\n\\nIn regression the input data is learned aswell. But here the network tries to predict feature values, which are in continuous space. The network tries to predict close as possible to new input data only using the learned model.Classification tries to label discrete data points with distinct classes, while regression tries to approximate a continuous function from discrete data points. Results of these methods are respectively a labeled data set or a continuous function.In classification the task is to give an discrete output value to an input. It assignes one of all defined classes to the current input. Regression try to approximate a function while minimizing error and produces a continous output value. Classification means mapping inptut data a class label, for example 1 and -1. IN regression on the other hand a continuous function is learned in way that f(x) - F(x) is minimized, where f(x) is the function learned by a learning machine and F(x) is the original function.  Classification is supervised learning where underlying function  representing the trining data is learn from training data to predict classes of datapoints or patterns drawn from similar distribution as of tarining data. Weights of the neural network are learned to minimize the error in classification. \\n\\nRegression is supervised learning algorithm where underlying function  representing the trining data is learn from training data to predict the value of label or output of some system  for new datapoint or pattern of similar type. Weights of the neural network are learned to minimize the error in prediction of function.\\n\\n\\nDifferences. :\\n1. Output of classification is discrete ( Class 1,2,3 ) whereas output of regression is continuous \\n2. Error in classification is number of wrong classifications whereas Error in classification regression is distance between lable value and predicted value\\nclassification is type of problem where algorithm needs to saperate the one data class from the another data class. \\nIf there is 2 classes C1 , C2 . algorithm classify the given data into these two classes. \\nit is discreet process.\\n\\nRegression is the pridicting the next point depending on the previous points.\\nit is continuous process.Classification is the problem where the input data has to be put in two or more classes distinctively different from each other. For example in case of binary classification on class can be -1 and the other +1\\n\\nRegression on the other hand is data fitting. THe main aim is to find a hyperplane which can fit a given input pattern.YOUR ANSWER HERE\\n\\nClassification: Is a task to partition the given input into one of several classes. The calsses are descrete values.\\n\\nRegression: Regression is the tasks of predicting output in a continuous range. The prediction can be any value within a range.Classification: Input einer assoziierten Klasse zuordnen. Regression: aus einer Menge von Daten, eine Funktion herleiten, die diese Daten mit minimalen Fehler beschreibt und somit für weitere Eingabe Daten vorhersagen über die Ausgabe machen kann.In classification task the aim to separate data in  different classes, such that output of NN gives value of class index for each input point. E.g in the task is to classify binary data, then the output of the NN will 0 or 1, and each value, represent on class.\\n\\nIn case of regression task, the aim is to fit data, namely a function that perform input-ouput mapping. Output of NN in this case, will be error value, such that we know how close is out function fitted to data points.\", 'Write down the SOM learning in pseudo code.': \"1. Arrange the weights in the required topology according to the problem.\\n2. Initialize the weights randomly such that all the weights are different.\\n3. Sample the input from the input space.\\n4. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron.\\n5. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and reduce the learning rate and make sure learning rate is above zero.\\n6. If ordering and convergence is complete, stop. Else continue to step 3.i. First we initialize random weights for neurons\\n\\nii.  Then we choose random input from input space\\n\\niii. We compute distance between input vector and each weight vector. \\n\\niv. Neuron that have minimium euclidean distance with input vector is considered as winner neuron\\n\\nv. Then, we find the neighborhood neurons of the winning neuron\\n\\nvi. We adjust the weights of all neighborhood neurons\\n\\nvii. Reduce the learning parameter and neighborhood size\\n\\nviii. Continue until it converges.YOUR ANSWER HERE\\nw denote weights\\nt denotes threshold\\nh denotes the neighborhood function, which decreases with distance d from winning neuron\\n\\nh(x, x_{win} //neighborhood function\\n return (exp(-2/||x-x_{win}||)\\n\\nw = rand(); //initialize weights with random value\\nwhile (w_{delta} > t){ //proceed until there are no notieable changes\\n    x_{win} = arg min ||x-w||^{2}//determine x which is closest to w (competetive learing) \\n    // Update weights of winning neuron\\n    // weights of losing neurons are not updated\\n    w_{new} = w_{old} + x*h(x, x_{win})*(x-w)//update weights of neuron which a are in neighborhood of winning neuron\\n    \\n    \\n}\\n\\n    initialize weights with small values (such that all of the weight vectors are different);\\n\\nsample a datapoint, feed into network;\\n\\ndetermine the winning neuron on the lattice, picking the neuron with the least euclidean distance of its weight vector to the input vector;\\n\\ndetermine the neighbourhood of the winning neuron through the neighbourhood function;\\n\\nchange weights of the neurons, namely spatially 'pulling' the weight vectors of the neighbourhood neurons towards the input vector;\\n\\ndepending on the timestep, reduce learning rate and neighbourhood size based on wether we are in the organizing or finetuning step;\\n\\nrepeat until maximum number of steps;1. Randomly initialize weights.\\n2. Randomly select an input from the training data.\\n3. Find the nearest neighbour of this input in the weights. This is done by finding the euclidean distance of the input from each weight. And selecting the weight with least distance.\\n4. Update the weights of all the neurons within the neighbourhood $h(n)$ (which is gaussian function, with an exponentially decaying $\\\\sigma(n)$) of the winning neuron with some learning rate $\\\\eta(n)$.\\n    $$\\\\Delta w_{ij}=\\\\eta(n)h(n)(||x_i-x_j||)$$\\n  where,\\n  $$\\\\eta(n)= \\\\eta_0e^{-n/T_1}$$ and $$\\\\sigma(n)= \\\\sigma_0e^{-n/T_1}$$In SOM we start with randomized weights, $\\\\mu$ learning reate, $d_{ji}$ distance between j and i, $h$ neighbour function\\n\\nrepeat as long as error is too high/max iterations are not reached:\\n\\n1. take input sample\\n2. find closest node/weight\\n3. find all it neighbours\\n4. move the weight and its neighbours closer to the given input, use the neighbour function (e.g. gaussian) to reduce effect to far distance neighbours\\n5. (optional) adapt learning rate and neighbour functiongiven a neigbourhood function $h_{ij}(n)$ and a lerning rate over time\\n\\nrandomly assing different weights from the input layer to the neurons in the second layer\\n\\nfor each training point x_i do:\\n\\n- find the winner-takes-all neuron $k$ with $min ||x_i-w_i||$\\n- find the neighbours of $k$ with the neigbourhood function\\n- compute the new weights for those neurons using the neighbourhood function and the learning rate\\n- update (decrease) the neighbourhood function and the learning rate\\nendYOUR ANSWER HERE1: w = init_weights() // equal to zero or random initialized\\n\\n2: n = 0\\n\\n3: WHILE !stop_criteria() \\n\\n4: winner_neuron, y = (x, w) // find on the map layer which neuron is closer to the input (euclidean distance)\\n\\n5: neighborhood = define_neighboor(winner_neuron, n) // define the neighborhood size (first iterations big, and being reduced)\\n\\n6: eta = define_learning_rate(n) // define the learning rate (large value at the first iterations and being reduced)\\n\\n7: diff_w = adapt_weights(neighborhood, eta) // adapt the weights just for the winner neuron and its neighborhood\\n\\n8: w = w + diff_w // update the weights\\n\\n9: stop_critera = must_stop(y, x) // look if the distance between input and the winner neuron is 0 (or really close to 0)\\n\\n9: END- randomly define some values for the synapitc connections in the network\\n- send the first input to the network\\n- in the output layer(map layer) select the neuron that has lowest error(competition phase)\\n- based on a predefined method define the neighborhood of the selected neuron(cooparation phase)\\n- change the weights of the selected neuron and the neurons located in its neighborhood(adaptation phase)\\n- if the stop condition satisfied stop the processHas three parts in it - Competition, Cooperation, Adaptation\\n\\nget input variable and choose amount of neurons to be more than amount of variables\\n\\nthen run competition, where from the input neurons will be compiting to each other on choosing which fits the most\\nafter finding winning neuron change weight of neighbouring neuron only\\n\\nin cooperation weights of neighbouring neurons are adjusted to clusters\\n\\nin adaptation neurons are pulled to input variables to establish the classification\\n- Find the winning neuron\\n- Find the neighbors of the winning neuron.**Pseudo code**\\n+ 1. Initialize map neurons, based on topology it could be a lattice, on a circle, etc.\\n+ 2. competition: find map neuron that is closest to an input neuron by computing distances $d$.\\n+ 3. update the position of closest map neuron with update rule.\\n+ 4. Do 2 and 3 until all input neurons are assigned a map neuron.\\n+ Do 2,3 and 4 until specified iterations or the net cumulative distance goes below some specified value or becomes zero.Produce train_SOM;\\n\\nbegin:\\n\\n    randomize weights for all neurons;\\n    \\n    for (i=1 to iteration_number) do:\\n    \\n        begin:\\n        \\n            take random input pattern;\\n            \\n            find the winning neuron;\\n            \\n            find neighbors of the winner;\\n            \\n            modify synaptic weights of these neurons;\\n            \\n            reduce learning rate and lambda;\\n            \\n        end;\\n        \\nend;Initialize the network with small and random weights.\\n\\nSample the data set by  picking an input randomly.\\n\\n> Determine the winning neuron based on the output value.\\n\\n> Determine the coopertaing neurons based using the neighborhood function.\\n\\n> Update the weights of the cooperating neurons.\\n\\n> Adjust the learning rate.\\n\\n> Stop if the network converges.Begin\\n n = range of data set\\n \\n Initialise the weights. #We give a small random weights. \\n \\n for the range of n:\\n \\n     Select a input signal, \\n \\n     Find the winning neuron based on the similarity between the weights. \\n \\n     Update the weights of the neighboring neuron\\n \\n     Repeat until the convergence.\\n     \\n     \\n1. initalising \\n2. Sampling\\n3 Similarity matching.\\n4. Updating the weights\\n5. continuation. intialise weights <br>\\nwhile( significant change is observed in topographic pattern) {<br>\\ntake a random input (sampling) <br>\\nfind the winning output neuron     (competition)  <br>\\nadjust the weights of the winning neuron and its neighbourhood neurons (cooperation) <br>\\ncontinue <br>\\n}YOUR ANSWER HERE: SOM learning\\n - Initialization with random small weights.\\n - Sampling: Picking a input pattern with certain probability.\\n - Similarity matching: Finding the most matching neuron i.e., the winning neuron.\\n - Synaptic updation: Updating the weights of the neuron and also the neurons in it's neighbourhood.\\n - Continuation: Repeat steps 2 to 4 till there is no considerable change in the map.Parameters: $X$ data vectors, $W$ weights vectors in lattice , $\\\\eta(n)$ -learning rate, $\\\\sigma(n)$ - neighbourhood width, $h_{ji(x)}$- neighbourhood function \\n\\nalgo:\\n\\n1) Initialize the weights to a small, random , non-repeatible values.\\n\\n2) Sample a data vector with a probabiity \\n\\n3) Compute the euclidean distance to weight vectors from the data points and find the winning neuron with minimum distance .\\n\\n4) Update the weights of the winning neuron and its neighbourhood towards the input direction using neighbourhood function.\\n\\n5) reduce the learning rate and the neighbour hood width and iterate from step 2 until no significant changes between weight vectors and inputs are seen.\\n1. Initialization : Initialize the weight vectors with random values such that the $w_j(0)$ is different for all weights.\\n2. Sampling : Draw sample example $x$ from input space.\\n3. Similarity matching : Find the best matching weight vector for the input vector : $W_i = argmin_i  (x - W_i(n))$\\n4. Adjust the weight vectors of neurons in the neighbourhood of the winning neuron \\n5. Go to Sampling step and repeat until no more changes are observed in the local neighbourhood of the winning neuron.1. Initailization: Initialize the weights of each neuron to small random values such that weight of each neuron is different.\\n2. Sampling: Sample an input from the input set\\n3. Similarity matching: Determine the neuron nearest to the sampled input based on its distance\\n4. Weight updation: Update the weights of the neighbouring neurons chosen by the neighbourhood function $h_{ij}(n)$\\n5. Continuation: Continue from sampling until there is no more change in the weightsSOM is refered to as Self organized maps which is an unsupervised training algorithm for finding spatial pattern in data without using any external help.The process in SOM is explained below:\\n-Initialization: Initialize random weights w_j for input patterns\\n- Sampling: Take n_th random sample from the input (say x)\\n- similarity matching :for the input x, find the best match in the weight vector.\\n  $i(x) = argmin(x - w)$\\n- update: the next step is to update the weights\\n$w(n+1) = w(n) + eta*h_ji(x)*i(x)$\\n- continuation : continue from sampling until there is no significant change in the feature mapInitialization: set random small values to weights wj is different for each neuron\\n\\nSampling: draw n-th sample x from input space\\n\\nCompetition: identify winning neuron i using arg min $||x-w_i||$ which means weight vector of i is most similar to input \\n\\nCooperation: identify neighbors of winning neuron i using neighborhood function $h_{j,i(x)} (n)$ which shrinks with time\\n\\nWeight adaptation: adjustments made to synaptic weights of winning neuron and its neighbors\\n\\ngo to sampling until no large changes in the feature map.\\nGENERATE random weights for all neurons<br>\\nFOR i to max_iteration DO<br>\\n    ------TAKE random input pattern<br>\\n    ------FIND the winning neuron<br>\\n    ------FIND the neighbors of the winning neuron<br>\\n    ------COMPUTE weigths of these neurons<br>\\n    ------REDUCE $\\\\eta$ and $\\\\lambda$<br>\\nEND FOR* Initialize the neuron weights randomly in a way that all neurons have different weights.\\n* Generate random samples x from the input space.\\n* Iterate the samples and **Compare distance between current input and all neurons** in the weight space.\\n* **Find a winning neuron** with shortest distance from current input.\\n* Distance is calculated using **euclidean or manhatten distance**.\\n* Find the neurons in the neighborhood boundary of winning neuron.\\n* **Update the weights** of neighborhood neurons using delta rule.\\n* Adapt the size of neighborhood $(\\\\lambda)$ and learning rate $(\\\\eta)$ at each iteration\\n* Repeat the process until there is no neurons in the neighborhood boundary or all the inputs moved to some neuron.Step1: It selects a datapoint in random through sampling.\\n\\nStep2: Finds the nearest neuron through competitive learning.\\n\\nStep3: Updates the weight of the winner neuron and updates the weight of neighbouring neurons by a fraction.\\n\\nStep4: Continues steps 1, 2, 3 until there is no change in the weights or some stopping criteria is met.\\n{\\ntake a rondom point from the training data\\n\\ncompetitive phase: find the winning neuron - the neuron similar feature, using the eucledian distance formula\\n\\ncooperative phase: find the neighbors of the winning neuron based on the neighbor function (eg: gaussian function)\\n\\nadaption phase: change the weights of the all the neighboring neuron of the winning node using the formula $ \\\\del w = \\\\eta x_j - w_j $\\n\\n}\\n\\n    input: distance function d(x, y), learning rate mu, neighborhood distance n\\n    \\n    Initialize the map layer with random weights\\n    for each input:\\n        find the weight which is closest to the input (minimum d(x, y))\\n        change the weight in the direction of the input depending on the learning rate\\n        change all weights which are within the neighborhood distance n depending on their distance and the learning rate\\n        reduce learning rate and neighborhood distance1. Initialize small random weights.\\n2. Draw the nth sample from the input space.\\n3. Similarity matching: Determine the winning neuron.\\n4. Update the weights of the neuron an the topological neighborhood.\\n5. Repeat steps 2-4 \\n\\nw = random\\n\\nn = example.draw()\\n\\nw_max = get_min(w_i*n)\\n\\nh_n = get_neighborhood(w_max)\\n\\nfor w_i in h_n:\\n\\n    w_i = w_i+$\\\\eta$*h*y Initialize the weights randomly. Create a term T1 and T2, which decrease the learning\\\\_rate and neighbourhood function respectively.\\n\\nCalculate $i(x) = argmin | w - x |$, the weight which is closest to the input data received.\\n\\ni(x) is the neuron, which wins the competetive process, this neuron and its neighbours weights are updated using $w_{new} = w_{old} - learning\\\\_rate \\\\cdot h(x) \\\\cdot (w - x)$. h(x) is the neighbourhood function which determines, which neurons are updated and how strong they are changed by the update. It is defined using the distance between the neurons.\\n\\nThe learning\\\\_rate is updated using $learning\\\\_rate / T1$, also is the neighbourhood function updated in the same way using T2. The learning\\\\_rate cannot get lower than 0.01, while the neighbourhood function can get as low as only the winning neuron. So in the beginning almost every neuron is updated and at the end only a small neighbourhood or the neuron itself is updated.for n iterations\\n\\n    winner = competition_between_neurons()\\n    \\n    neighbourhood = cooperation_with_neighbourhood_function(winner)\\n    \\n    update_weights(neighbourhood)given a map layer \\nset random small values for weights from input to map layer\\nrepeat until not converged:\\n    find best match of input value and weight of the neurons (competitive process)\\n    adapt (increase) weight of winning neuron and neighboorhood (with gauss function and neighboorhood size) (cooperating process and weight adjustment)\\n    decrease neighboorhood size YOUR ANSWER HEREInitialize weight vectors of hidden neurons with same dimmension as of data.\\n\\nNumber of hidden neuron should be signifiacntly greater than number of data points.\\n\\ninitialize learning rate n and neghbouring function h\\n\\nwhile (rate of change in weights is significant):\\n\\n    for every datapoint:\\n\\n        calculate distance of each neuron from data.\\n        select winner neuron w with minimum distance (maximum similarity)\\n        error = distnce of winner form datapoint\\n        adjust weights of neurons with the rule w = w + n*h*error\\n        randomly inilize the weights\\n\\ndraw sample of inputs \\n\\nIncrease the weights of the local neihburhood of winning neuron\\n\\nrepeat the process above process till there is only one winning neuronYOUR ANSWER HEREYOUR ANSWER HERE\\n\\nfor i in num_of_epochs\\n\\n  for p in input_points\\n  \\n    find the winning neuron\\n    find the neighbours of the winning neuron within distance sigma\\n    update winning neuron and neighbours weight\\n    update sigma and learning_rate so that both reduces over timeInput x:\\nFinde neuron winner mit ähnlichstem Gewicht zu x respektive Euclidischer Norm.\\nBestimme Nachbar Neuronen von winner mit nachbarf()\\nÄndere Gewichte in richtung x von winner und nachbarneuronen mit changew()\\nvermindere parameter, für nachbarf() der angbit in welcher reichweite ein neuron noch ein nachbar ist(exponentielle funktion)\\nvermindere lernParam der angibt wie startk die gewichte verändert werden\\nRepeat until Convergenz erreicht\\n\\nchangew() ändert die gewichte in abhängigkeit der distanz des neurons zum winner (exponentielle funktion) je weiter weg desto kleiner die änderung, und in abhängigkeit des lernParam, sodass mit der zeit immer kleinere veränderungen durchgeführt werdenYOUR ANSWER HERE\", 'Give the basic idea of an SVM using the correct terminology!': 'A support vector machine is a maximum margin classifier in which the width of the boundary of separation is maximized. A margin is defined as the width of the boundary before hitting a point. \\n\\nThis maximum margin intuitively feels safe, and is experimentally good.Basic idea of SVM is to best segregate the data into two classes with the help of decision boundary. This decision boundary is margin, we always try to maximize the margin to make sure data is classified correctlyYOUR ANSWER HERE Support Vector Machines goal is to maximize margin between closest data points of separating hyperplane. Separating hyperplane is given by: 0 = w(n)*x(n) + b. By maximizing margin probability of classification errors is reduced. An SVM is a binary, linear classifier spanning a seperating hyperplane between two classes of datapoints. The hyperplane is spanned between both the positive and negative decision boundaries, and supported by a number of support vectors. Support vectors are the outermost datapoints which span the hyperplane. During training, the distance of falsely classified data points to their correct side of the hyperplane is minimized, utilizing a quadratic programming formulation. A SVM is a binary classifier with a maximum width boundary separating the two classes. This uses support vectors (vectors that pushes against the boundaries).\\n\\nThe equations of the lines in an SVM are:\\n   - $wx+b>=1$:for class 1\\n   - $wx+b<=-1$:for class -1\\n   - M is the width between these boundaries.Because SVMs are binary classifiers we can use a border to sperate the data. The border is typically placed where it has the largest possible distance to both classes. The vectors the border touches on both sides with its margin are the support vectors.A SVM is an ANN for supervised learning, whicht is able to saperate two classes of data-points by using a hyperlane found by quadratic programming, by finding the biggest margin. The goal is to classify future data in there two classes.The SVM is a maximum margin classifier. It is used the binary classify datapoints in a dichotomy. The idea is to find a line wich linearly seperates both classes. There perfect position of this line is in right in the middle of these classes. To find this line(descision boundary) we define a positive and a negative boundary which are parallel to this line. The boundarys define the margin between both classes. The idea of SVM is that the datapoints which are next to the boundary can be used to define the margin. They are called support vectors. \\nAddittionaly not every problem is linearly seperable so the idea was to transform the input into many higher dimensions using some kernel functions. We discussed the kernel function of polynomial terms and found out that it easy to compute. Support Vector Machines are a type of learning machines that try to classify different classes of an input space. For linear separable classes, the SVMs try to calculate the line that separates this two classes with maximum margin. The support vectors will be the points closer to this margin. When the input data is noisy, we have an optimization problem of two aspects (maximum margin, proper classification). So, a trade-off (C) will be defined. The trade-off will be calculated by the sum of the distance of misclassified points.\\n\\nFor non-linear separable classes, a kernel will be defined that will transform the input data into a higher dimensional space.In linear SVM we have a linear border line classifier that seperate two different classes(positive and negative planes) and we calculate the distance of the data points from this border line classifier. Also a margin will be defined and this margin will be maximized until it touches some data points in the plane. The data points that the margin pushed agains them will be our support vectors. The error for the wrongly classified datapoints will be calculated by calculating the distance of the data point from its correct plane. The SVM tries to learn the classifier and the margin from the training data. Support vector machines are classifiers that are using support vectors, which are variables of the dataset. These variable are chosen during learning algorithm. Main advantage of SVMs is that it will not be overfitting by choosing correct margin. Activation functions can be both linear and nonlinear. Output of SVM is always TRUE or FALSE for given variable.Support vector machines are a type of neural network that build a desicion boundary around classes such that the margin of separation between classes is maximized. SVMs are binary classifier. They learn the classification by memorizing the marginal data points (called support vectors) that make up the decision boundaries (2 : positive and negative).The abbreviation SVM stands for Suppor Vector Machine. SVMs represent a feedforward category of NN. SVMs are binary learning machines whose functionality can be summarized for classification problem as follows:\\nGiven a training sample, the SVM constructs a hyperplane as the decision surface in such a way that the margin of seperation between positive and negative examples is maximized.\\n\\nOne key innovation associated with SVMs is the kernel trick. The kernel trick consists of observing that many machine learning algorithms can be written exclusively in terms of dot products between examples. It allows us to learn models that are nonlinear as a function of x using convex optimization techniques that are guaranteed to converge efficiently. Besides, the kernel function k often admits an implementation that is significantly more computatinal efficient than naively constructing two vectors and explicetly taking their dot product.An SVM, or support vector machine, is a feedforward network with a hidden layer to learn a task in a supervised learning manner. The network tries to construct a hyperplane that separates the data points of two different classes by maximizing the margin of separation, which is the distance from the hyperplane to the closest data points called support vectors. Given a dataset, support vector machines builds a hyperplane in a such a way that positive and negative samples are seperated to the maximum distance. Width of the margin should be maximum\\n\\nThe vectors to which the margins(margin for positive and negative sample) are pushed on to it are called support vectors. SVM stands for Support Vector Machine. It creates a hyperplane such that margin of separation between positive and negative classes is maximised.  YOUR ANSWER HERE: SVM is a linear machine whose goal is to construct a optimal hyperplane such that the marginal separation is the maximum between the decision boundaries. The decision boundaries are drawn parallel to the hyperplane which just push the datapoints closest to the hyperplane. The datapoints closer to the hyperplane are called support vectors.The idea of SVM is to fit a supervised model onto the training data allowing maximum generalization ability. \\n\\nThis is done by computing maximum margin between different classes of data using the support vectors.\\nThe magrin can be computed using differeent kernels for a higher dimensional data.A SVM is a linear machine which is used in pattern classifcation problems to find a decision surface in the form of a hyperplane for linearly separable classes such that the margin of separation between the classes is as large as possible. SVM\\'s are an approximate implementation of the induction principle of structural risk minimization which is based on the fact that the error rate in testing is bounded by a term that is dependent upon the sum of training error rate and the VC dimension of h. The basic idea of SVM is to determine the best decision boundary i.e. the one which provides maximum margin so that the boundary can be widened most before it touches any datapoint. It is done using Support Vectors which are the the datapoints the margin pushes against.SVM refers to support vector machines.In terms of a linear classification problem svm can be defined as creating a hyper plane which is a decision surface and to maximize the width of decision boundary.In cases where the problem is complex svm can be used as it classifies the data by projecting the data in higher dimension.If the data is to be separated in 3 classes , they can use 3 svm\\'s for three different classes.basic idea of SVM is to construct a hyperplane as the decision surface in such a way that the margin of separation between negative examples and positive examples is maximized.The idea of SVM is to construct a hyperplane as a decision surface such that the margin separation between positive and negative examples is maximized.SVM tries to find a **best hyperplane with widest margin with the help of support vectors** such that all the data points are classified correctly.SVM is used for linearly separable data. A hyperplane is used to separate the data, but there could be so many hyperplanes that separate the data. The best hyperplane is choosen which separates data with a bigger margin. So in SVM we find the hyperplane which has a bigger margin between the hyperplane and both the positive and negative data lines.Given a training set for classification, The basic idea of SVM is to construct a hyperplane as decision boundary such a way that the margin between the positive and negative points is maximum \\n\\nSupport vector is a small subset of the of the training data against which the boundary is pushedA support vector machine classifies given data using a decision boundary. The width of this decision boundary (margin) is maximized to ensure good results, because a maximized width is as robust as possible. The margin width is $\\\\frac{2}{\\\\sqrt{w * w}}$. To maximize it, quadratic programming is used. In order to handle noisy data, slack variables are introduced. To eliminate them, duality is used.An SVM is a linear classifier that divides a binary pattern, by a line that maximizes the margin between its line and the respective support vectors.A SVM learns a decision boundary from the input data. Additionaly it learns two margins, which are parallel to the decision boundary and lie as close as possible at the data points, the support vectors. The decision boundary is chosen so that the margins are maximized. Using kernel functions higher dimensional data and non linearly separable data can be learned aswell.An SVM is a learning machine that tries to learn the support vectors of a two class data set to get the maximum margin, the optimal seperating hyperplane, between the two classes.A SVM uses a few of the data points as support vectors to bild the maximum margin classifier. It searches for the seperating line, which has the maximum margin to the datapoints. In cases of Noise, the seperating line is searched, which minimizes the distance to the points in the wrong category. The data is cast to a higher dimensional space to use covers theorem while using kernels. The data is more likely linearly seperable in the higher dimensional feature space. Using structural risk minimization the dimensionality is reduced. SVMs are used to linearly seperate Data points. The decision boundary is line or hyperplane in higher dimensions that defines the lable of a data point. The decion boundary is choosen in a way that the margin is maximized. Data points on the decion boundary are called support vectors and define the hyperplane. In 2 dimensions if the data is liner seperable the margin is equal to 2/sqrt(w.w) where w is the weight vector. If the data is not linear seperable, the input can be projected into higher dimension space. This increases the chance of linear seperablity.Support vector machine is classifier which maximizes the margin between boundries learned from two classes.\\nMargin is minimum distance by boundries can be increased before hitting datapoints.\\nSupport vectors are the datapoints against which magin pushes up the boundary.support vector machines are the finding classfiers, draw  the dision boundary which push against the support vectors.The basic idea of Support Vector Machine (SVM) is to find the width of a line or hyperplane which which divides the input data into two classes. The points lying on the edge of the defined width are called support vectors.  YOUR ANSWER HERE\\n\\nSVM is a classifier that classifies a set of points in a way that maximizes the margin between the points of two classes. The classification can be linear or non linear.Support Vector Machine: Eine Lern Maschine mit maximalen Margin. Sie kann für Dateneingaben die zugehörigkeit zu genau einer Klasse vorhersagen. Beim lernen wird die Trennung der Datenpunkte mit y=1 oder y=-1 so festgelegt, dass der Abstand zum nächsten \"falschen\" Punkt maximal ist. Rauschen, also Datenpunkte die im falschen Bereich liegen werden versucht zu ignorieren. Das lernen wird mithilfe von Quadratischer Programmierung realisiert. Die Vektoren die den margin berühren sind die support Vektoren. Nicht linear trennbare Daten, werden mithilfe von nichtlinearen basis funktionen, z.B. gauschen oder radialen basisfunktionen getrennt. The idea behing SVM is to find a hyperplane which separate data into classes. First it is required to find a data point which are clossest to hyperplane, and these data points are called support vectors. Next task is to find a maximum possible width of the hyperplain such that support vectors are on the edge of that hyperplane.\\n\\nThis problem is formulated as min-max constrained optimization problem. In order to find a optimum width of hyperplane, (optimimum of a funtction) the idea is to use method of Langrange multiplier. Additionally, when I data is not linearly separable, than an approach is to project data in higher dimension and then to find a hyperplane that separates data in that dimension.', ' What role does the method of steepest decent have when learning a network?': \"In steepest descent, the gradient of the cost function is found by partially differentiating it with respect to the weights. The weights are then updated in the opposite direction if the gradient. This ensures that the weight moves in the steepest direction are reduced. It can also be proven that the weights always reduce. Hence, steepest descent can be used to minimize the cost function.Steepest descent is method of optimizing the algorithm by minimizing the error. Weights are adjusted in the direction of steeping descent, opposite to the direction of the gradient.YOUR ANSWER HERE Steepest descent moves the error within error surface a small step into the opposite direction of gradient. By help of steepest descent we want to minimize error. Steepest descent stops when gradient = 0.When learning weights with a SD method, we try to reduce the error based on following the gradient of an error function in the opposite direction, effectively trailing the error surface towards the minimum. Here, the error function (typically some form of mean squared error) is differentiated w.r.t. the individual weights, expressing how much a weight contributes to the network error and must thus be corrected. Due to the gradient pointing in the direction of steepest ascent, we must thus step in the negative direction.- Steepeset descent is used for error minimization when updating weights.\\n- According to this, we update the weights along a direction which minimizes the error; which is calculated by finiding the slope at the point.Speepest decent is used to minimize the training error of a network given sample inputs and desired outputs. It uses the gradient of the error function to move the weights closer to an optimal weight with lowest output error. Using a learning rate we can influence the speed and stability of this algorithm.The steepest decent is the direction the error function falls the most. We want to change the weights in the direction of the steepest decent (the opposide direction of the gradient) to have a smaller error in the next iteration and to optimize the ANN.The idea of learning a network is to minimize a certain costfunction. We can use steepest descent to minimize this cost function. While there are other optimization techniques which can be used for optimization, steepest decent is a widly used optimization technique. To optimize a network we calulate the partial derivatives(gradient) and use it to update our weights. It is also used in BP.The approach of the method of steepest descent is to find the direction for the minimization of the error in an approximation problem. The cost function e, dependent of the weights w, will be derivated (partial derivative) for all defined weights. This gradient will be used for updating the weights for the next iteration. The direction of the minimization of the error is the oposite direction of the gradien: - g.When the inputs are being send into a network and we calculate the error we need a mechanism to learn and manipopulate the free parameters of the network and the learning uses the error but we must know in which direction in the search(optimization) space we should move so that we can reach the global minima of the error for this we use steepest decent. This method tells us in which direction we need to move by getting the gradient from the error.Steepest descent is a method of weight adaptation. It is using first order derivative to approximate the function. Therefore is rather slow.The steepest descent is an unconstrained optimization method that seeks to minimize an error function. This function is iteratively changed in direction oposite to the gradient vector.Method of steepest descent updates the weights in the direction where the error is minimum.The steepest descent method is an algorithm for finding the nearest local minimum of a function which presupposes that the gradient of the function can be computed. This property is used to determine the optimal weights of the NN.Steepest descent is used to update the synaptic weights of a network based on a cost function expressed by the errors of the output. The weights are adjusted in the direction opposite to the gradient of the cost function.In steepest descent the adjustments done on the weight vector are in the direction of the steepest descent which is in the direction opposite to that of a gradient descent. \\n\\nIn a learning problem, it basically used to reduce the cost based on the weight. The main goal is to find an optimal weight.Method of steepest decent is an unconstrained optimization technique used for learning in a network. It is used in iterative manner to minimize the error in supervised learning. It finds the direction of maximum gradient. So we go in the opposite direction hoping to find the minima. Convergence of the algorithm depends on the learning rate and also the condition that it doesn't get stuck in local minima.YOUR ANSWER HERE: In steepest descent method, the network moves towards the direction of the maximum gradient. The learning with steepest descent method can be slow to converge and can exhibit zigzag behavior.Steepest descent involves weight updation in the direction of maximum steep or maximum derease in the cost function ot in the direction opposite to the gradient funcion. The weight update is $\\\\Delta w(n) = - \\\\eta g(n)$ where $\\\\eta$ is the learning rate which defines the magnitude of learning using the gradient g(n) which is the gradient of the cost function of errorsin the nth iteration. Higher $\\\\eta$ will result in rapid learning but with oscilations in responses.The method of steepest descent is used to find the direction in which the error function viewed as a function of weights is decreasing most rapidly and then take a small step in that direction. When learning a network, steepest descent enables to iteratively adjust the weight vectors until the optimal weight vector that minimises the cost function (i.e, the error function where error is computed as the difference between the desired and actual response of the network) is found.When learning a network the steepest descent algorithm updates the weights in such a way that the error decreases in every iteration.The method of steepest descent moves in the direction opposite to the gradient to minimize the cost funcion .steepest decent method is based on minimization of error cost function $\\\\xi(w) = 0.5 e^2_k(n)$, so synaptic weight of network is updated in a direction opposite to gradient vector of $\\\\xi(w)$, that is $W_k(n+1) = W_k(n) - \\\\eta \\\\nabla \\\\xi(w) = W_k(n) - \\\\eta e_k(n) x(n)$, $\\\\eta $is learning rate.$e_k(n)$ is neuron k error signal, $x_j(n)$ is input data.The steepest is used to find a direction in which E is decreasing most rapidly. The adjustments applied to the weights are in the direction of steepest descent.Steepest decent helps to **minimize the value of error function $E$** by finding the **right direction **to move the weight vector to reach global minima.\\n\\nThe direction is always **opposite to the direction of actual gradient vector**.\\nMethod of steepest descent is used to reduce the error. In backpropogation during backward pass we need to know how by how much amount the weights should be changed, this can be known if we use steepest descent, find the gradient of error and use it to reduce the error.The steepest descent finds the direction of the error function and tries to reduce it by adding in the opposite direction\\n\\n$ del w = - \\\\eta g(n)$\\n\\ng(n)- gradient of the cost functionThe steepest descent is used to find the right direction in which the weights should be changed while learning a network. The derivate of the error is used and weights are changed in that direction which makes the error smaller as fast as possible.The steepest descent can be used to optimize the weights of a network. In steepest descent the error function is a function of the weights. So we determine the direction of the steepest descent on the error surface and go into that direction to minimize the error of the weights on optimize them. The method of steepest descent is used to minimize the error function. The error function is the gradient of the error $\\\\Delta e = d - y$, where d is the desired output and y is the actual output of the neuron.Steepest descent is the basic learning algorithm others are derived from. The goal when learning a network is to minimize the error. This is achieved by starting at a random position and going in the opposite direction of the gradient vector, the steepest descent.the error function is computet. to adapt the weights (learn the network) the error function is followed in small steps in direction of steepest descent to decrease the error. using iterations the error is decreased in each step and end in a (local) minimum\\nused in back-propagationIn error correction learning the weights of a network are learned in a way that e(x) is minimized, where e(x) is some error function. In order to minimize the error function the method of steepest desend is used. The negative gradient of e(x) points in the direction of steepest decend. Doing steepest descend in a single layer feed forward network leads to the delta rule.Steepest descent adjust the parameters (weights and bias) of the NN to minimize the error. It does so by adjustinmg the weights in the direction of steepest descent of the error function. Steepest decent while move in the direction of the max improvement ( in terms of decrasing) in the cost funtion or error.\\n\\nif the learning rate is large then the it follows the zizag motion.\\n\\nif the learning rate is too low then it takes time for converging .The method of steepest descent is responsible for weight adjustments in the network. The weights are adjusted in the direction of the steepest descent that is equal to the negative grad of the error. It ensures that the weights are decreased in every iteration step.YOUR ANSWER HERE\\n\\nSteepest decent method helps in making the adjustments of the weights in a neural network in a way that minimizes the average squared error.\\n\\nIn each step it gives the direction towards which the maximum decrease of the average squared error can be achieved.Steepest decent findet verwendung beim trainieren von Netzen bei verschiedenen Netz Architekturen.Voraussetzung dafür ist, dass beim Training für die Daten die gewünschte Ausgabe bekannt ist und dass die Aktivierungsfunktion der Neuron ableitbar ist. The method of steepest decent is used for finding minimum of a cost(error) function. The steepest decent iterates over possible values of weight vector to optimize the function. It is used for deriving error function in ADALINE (adaptive linear element) algorithm, and it is used also in backpropagation method in training of Multi-layer NNs.\", 'Define: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$': 'A dataset $A \\\\subseteq X $ with N datapoints has $2^N$ binary maps. If for any of these binary maps, a hypothesis $h \\\\in H$ splits the positive data from the negative data such that there is no training error, then it is said that h shatters the dataset A.YOUR ANSWER HERE A a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$ if there exists a an $\\\\alpha for every training set with zero training error... when our learned machine achieves zero training error on every classification problem of the dataset A. Since we got a selection of $n$ points in the dataset A, the number of problems in binary classification is 2 to the power of $n$ (I didnt find the \\'Dach\\' symbol on the english keyboard :) )Considereing a dataset $A \\\\subseteq X$ ,where X is the instance space and A contains N elements. Now there are $2^N$ binary maps or learning problems when we wnat to separate two classes.\\n\\nIf any of these problems can be separated completely by hypothesis $h \\\\in H$ then h is said to shatter A.\\n\\ni.e., a hypothesis shatters a dataset, if it can completely separate the classes with zero error for all possible combination of labels in the dataset.\\nwhen every possible combination of input and desired output can be classified using $h$A hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X$, then for every point $x_i \\\\in A$ there is a label $y_i \\\\in \\\\{1,-1\\\\}$ and the $H$ can saperate these two classes using $h$ with no training error.there exists an arrangement of these points in A sucht that for each possible combination of labels to these points  the hypothesis h has zero training errorAn hypthesis *h* shatters a dataset A, if for a given data set, h is able to distinguish (or separate) the different classes of this data set.\"h\" shatters A if for any set of input data points in A there exist at least one training error of zero.H shatters A when for example in given dataset (X_1,X_2...X_r) output are in a form (X_1, Y_1),(X_2,Y_2)...(X_r,Y_r) there has been found a 0 error.A machine F can shatter a set of points $x_1, x_2, x3_,..., x_n$ if and only if for every training set, there is a weight vector $\\\\alpha$ that produces zero training error.A hypothesis $h \\\\in H$ shatters a dataset $A \\\\subset X \\\\Leftrightarrow$ for each assignable configuration of $(x_i, y_i)\\\\in A$, $h$ perfectly classifies all elements of the set $A$.YOUR ANSWER HEREA hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow$ \\n\\nat least on possible combination of dataset $A$ can be classified by the hypothesis $h \\\\in H$ with zero training error.Given a dataset $A \\\\subseteq X $ where X is the instent data space, for a given problem with the dataset A, if a learning machine is able to successfully split the positive and the negative data, then we say that A is shattered by the learning machine. Suppose X is a training dataset and A is the subset of training dataset then hypothesis h is said to shatter if can correctly classify all the points in A i.e zero training error.YOUR ANSWER HERE: a hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$ if the hypothesis can clearly distinguish the positive examples from the negative examples in A.A hypothesis h is model that separates a dataset consisting of {(x_i , y_i)} samples into positive and negative samples. h is said to shatter a given subset of a dataset if it can successfully separate at least one configuration of the subset of dataset.A hypothesis $h \\\\in H$ shatters a dataset $A \\\\subseteq X $ if there exists an $\\\\alpha$ for which there is zero training errorfor each of the $2^N$ (where N is the size of A) combinations of input output mappings of the form $(X_i, y_i)$, h is able to classify the data correctly that is with zero error. For all possible binary labeling of dataset A, we can find a hypothesis h that can separate the positive examples from negative examples, the H shatters A.The hypothesis $h$ can shatter any points of $x_1$, $x_2$, ..., $x_n$ if and only if for every possible training set of the form $(x_1, y_1), (x_2, y_2), ... (x_n, y_n)$ there exist some values of $\\\\alpha$ that gets zero training error.A hypothesis space H shatters a dataset, if and only if there is a **possbile $\\\\alpha$ (weight vector)** on hypothesis space that **seperates all the positvie data from negative data**.a hypothesis $h \\\\in H$ shatters $A \\\\subseteq X$ if and only if there exists a value of $\\\\alpha$ for which the training error is zeroH is the vc dimension of a learning maching that can shatter h points. \\n\\nVc dimension of a learning machine is the maximum number of points that can be arranged so that the learning machine can shatter them\\n\\nShattering:\\n\\nThe learning machine is said to shatter points $(x_1 ... x_r)$ if and only if all the possible training set of $((x_1,y_1) ... (x_r,y_r))$ can be classified with zero training errorA hypothesis shatters a dataset if it can correctly classify all combinations of labellings of the points in the dataset., if there exists a configuration of $X$, so that $h$ gets zero training error on any dichotomy of the datapoints.there exist w weights, which produce a perfect classification.for all possible classified subsets of dataset A the hypothesis h can seperate itwhen all combinations of position and labeling of the data can be separated in the given classes by the hypothesish shatters A when and only when for all possibilities of (a_1, y_1), (a_2, y2), ... ,(a_n, y_n), where y is the class lable (1 or -1) there exists some $ alpha  $ for a learning machine f that produces 0 training error.If there exist atleast one configuration of A for which training error of h is zero. i.e. it successfully classifies all oints in A. and there exist a linear saperater which saperates positve examples from the negavtive examples correctly. then we say that A can be shatter at h.Ginven a data set A if it is possible to find a hypotheis H which separates the data set into binary form without any error, we can say that hypothesis $h \\\\in H$ shatters dataaet $A \\\\subseteq X \\\\Leftrightarrow \\\\ldots$.YOUR ANSWER HERE\\n\\nIt means that for all the points in A with input output pair (x,y), for any combination of ($x_i$,$y_i$) there exist parameter $\\\\alpha$ of h that enables h to classify the points with zero errorgdw. für alle möglichen Trainingsdaten, die Daten so angeordnet werden können, dass h sie korrekt zuordnet (binäre zuordnung).We say that a hypothesis h shatters a dataset A, iff the h produces a zero training error for certain data set A. In other words, we say that a hypothesis h shatters a dataset A, when h separates data A in two classes without erorr.', 'Write down and explain the Widrow-Hoff learning rule!': \"$ \\\\Delta w = \\\\eta e(n)x(n) $, where $\\\\eta$ is the learning rate.\\n\\nWidrow-Hoff rule states that the change in weights is proportional to the product of the error and the input in the corresponding synapse.Weights adjusted are proportional to the product of error signal and the input vector\\n\\nw(n + 1) = w(n) + $\\\\eta(d-y)x(n)$\\n\\n$\\\\eta$ is learning rate, d is desired output, y is current output. x(n) in input vector. YOUR ANSWER HERE Adaption of weight is proportional to product of input and error:\\n$w_{new} = w_{old} + x*e$For neurons with a linear activation function (ADALINE): $w(t+1)=w(t)+\\\\alpha (d-y)x$, where x is the input pattern, d is the true value and y is the net output. Notice that the delta rule looks similiar to the perceptron learning rule, but was derived from SD, whereas the perceptron works with a step function which is not fully differentiable.Widrow-Hoff learning rule is also known as error correction rule is used to update the weights as:\\n$\\\\Delta w = \\\\eta (d_i-y_i)x_i$ where, d is the desired output and y is the output the network generates and x is the input.$w(n+1) = w(n) + \\\\mu (d(n) - y(n))x(n)$\\n\\nThe change of the weights is determined using the error ($d(n) - y(n)$) and the input that was given to the network. The learning rate can improve learing speed. The new weights are dependent on the old ones and the change calculated$w_{ij}(n)= w_{ij}(n-1)+ learningrate*(d_j-y_j)*x_i$\\n\\nwe change the weights by computing the error $e_j= (d_j-y_j)$ for the input and multiply it by the learningrate and the $x_i$ and adding it to the old weight. This minimises the squared error function (our cost function) and is the online variant of the steepest decent method. $$ \\\\Delta w(n) = \\\\eta * e(n)*w(n) $$\\n$$ e(n) = (y-d) $$\\nThe widrow hoff learning rule is error correction learning. It is used to train a network in a supervised manner. The widrow hoff learning rule can be derived from gradient decent. The rule consists of the error e(n) the neuron has and is muliplied with the weight so that the impact of the weight to the error is incorporated into the update.  A learning rule is use as a adjustment in how much we trust the weight change. The error is calculate by the difference between the current and expected output.The Widrow-Hoff learning rule is defined as: $w(n + 1) = w(n) + \\\\eta * x(n) * e(n)$\\n\\nThe Widrow-Hoff learning rule is a rule for adjusting the weights of a NN for a error correction learning task. This learning rule is derived from the steepest descent method, where the direction for the minimization of the error is the defined as the oposite direction of the cost function's gradient. This gradient can be simplified as $x(n) * e(n)$, where e(n) is defined as the difference between the desired response and the actual response of the learning machine (NN): $e(n) = d(n) - y(n)$.\\n\\n$\\\\eta$ defines the learning rate used.YOUR ANSWER HEREWindrow-Hoff rule is \\n$$W_{new}=x_{input}*W_{old}*(d_{output}-y_{output})*eta*a $$\\n\\nwhere \\n$W_{new}=new weight,W_{old}=old weight,d_{output}=desired output,y_{output}=actual output,x_{input}=input, eta=learning rate, a=learning constant$The Widrow-Holf or delta rule is a gradient descent learning rule used to adapt weight in a perceptron. \\n\\n$\\\\Delta w(n) = - \\\\eta(d(n) - y(n))x(n) $\\n\\n$\\\\Delta w(n) = - \\\\eta e(n)x(n) $The widrow-Hoff (delta) learning rule is given by\\n$$ w(n+1) = w(n) - \\\\eta x(n) e(n)$$\\nwhere $e(n)$ is the error vector, $\\\\eta$ is learning parameter, $x(n)$ is input vector.The Widrow-Hoff Learning rule is also referred to as Delta, or Least Mean Square (LMS) Rule. It is used to minimize the cost function and is defined as follows:\\n\\nDelta w_ji(n) = eta (partial x_i(n) / (partial w_ji(n))\\n\\nwhere eta is the learning rate paramter, x_i(n) is the total instantaneous error energy and w are the weights.The Widrow-Hoff learning rule, also called delta rule, is used for learning a network by adjusting the synaptic weights of the network with the error signals:\\n\\n$$ w(n+1) = w(n) + \\\\eta (d(n) - y(n)) x(n) $$\\n\\nwhere $n$ is the number of iteration, $\\\\eta$ is the learning rate, $d(n)$ is the desired output signal, $y(n)$ is the actual output signal, and $x(n)$ is the input signal. $(d(n) - y(n))$ is the error signal.Widrow hoff's learning rule states that the adjustment of the weight of a synapses are propotional to the product of the error function and the input which is given by the synapses based on the problem. \\nWidrow Hoff rule is based minimising the mean square error using gradient descent alogirthm. Weights are adjusted in following manner:<br> \\nw(n+1) = w(n) - n (gradient of mean square error) <br>\\nIt takes the gradient of the mean square error $0.5 e^{2}(n) = e(n) \\\\frac{\\\\partial e(n)}{\\\\partial w} = e(n) x(n)$YOUR ANSWER HERE: Widrow- Hoff rule:\\n- $\\\\Delta w$ = $\\\\eta e(n) x(n)$\\n- Widrow-Hoff rule states that when an input x(n) produces an error e(n), then the change in the weight is directly proportional to the error signal and the input signal.Widrow Hoff learning rule is also called as error corresction learning rule. The error is defined as the difference between the desired and the actua output of the learning machine. Assuming the desired signa is available, the error is computed and weights of the neural network are upadted in the direction of reduction of errors. The error for each input sample for a neuron k is computed using $e_k(i) = d_k(i) - y_k(i)$. weight change $\\\\Delta W = W*e$, that is the dot product of error and the weights is computed.Given a neuron k excited by an input signal $x_i$, if $w_{ki}$ is the synaptic weight of the neuron, then the Widrow-Hoff learning rule gives the weight adjustment $\\\\Delta w_{ki}$ applied to the neuron k in mathematical terms  as follows: $\\\\Delta w_{ki} = \\\\eta x_i(n)e(n)$ where e(n) is the instantaneous value of the error signal. Thus the  Widrow-Hoff rule states that the synaptic adjustment applied to the weights of a neuron is proportional to the product of the input signal to the neuro and the instantaneous value of the error signal. This rule assumes that the neuron has an external supply of desired response so that the error can be computed. The Widrow-Hoff learning rule is given by\\n$$w(n + 1) = w(n) + \\\\eta e(n) x(n)$$  \\nwhere \\n$w(n)$: Weight in iteration n  \\n$e(n) = d(n) - y(n)$: Error  \\n$d(n)$: Desired output  \\n$y(n)$: Actual output  \\n$x(n$: Input    \\n$\\\\eta$: Learning rateWidrow -Hoff learning rule states that the adaptation made to the synaptic weights is proportional to the product of input and the error function.It basically states that if the error is high then the product of input and error will also be high , and thus the adjustment made to the weight would be more.\\n$w_j(n+1) = w_j(n) + eta*(error)*input$it is based on minimization of error cost function $\\\\xi(w) = 0.5 e^2_k(n)$, so synaptic weight from neuron k to input j is updated in a direction opposite to gradient vector of $\\\\xi(w)$, that is $w_{kj}(n+1) = w_{kj}(n) - \\\\eta \\\\nabla \\\\xi(w) = w_{kj}(n) - \\\\eta e_k(n) x_j(n)$, $\\\\eta $ is learning rate.$e_k(n)$ is neuron k error signal, $x_j(n)$ is input data.Windrow-Hoff or error correction learning rule says that the adjustment of a weight is proportional to the product of the error signal and the input signal of the weight. $$\\\\bigtriangleup \\\\omega_{ji} = e_j * x_i$$\\n$$\\\\omega(n+1) = \\\\omega(n) + \\\\eta \\\\bigtriangleup \\\\omega_{ji}$$\\n\\nWidrow Hoff learning rule says that, the synaptic weight update is directly proportional to the product of error and the input.Widrow-Hoff learning rule: The rules states that the weight update is directly proportional to the product of the input to the neuron and the error.\\n\\n$\\\\Delta w_{ij} = \\\\eta e(n) \\\\sum x_i(n)$delta $ w_{kj} =  \\\\eta e_k . x_j $\\n\\nWidrow hoff rules states that the change in synaptic weight is proportional to the product of the error signal and the input signal $\\\\Delta w(n) = \\\\mu * x(n) * e(n)$\\n\\n$\\\\mu = $ learning rate\\n\\n$x(n) = $ input at timestep n\\n\\n$e(n) = d(n) - y(n)$\\n\\n$d(n) = $ desired signal at timestep n\\n\\n$y(n) = $ output of the network at timestep n\\n\\nThe Widroff-Hoff (or delta rule) changes the weights depending on the input and the error, which is the difference between the output of the network and the desired output. This weight change can be scaled by a learning rate.The Widrow-Hoff rule is used in error-correction learning and uses the current error and output of the system to determine the new weights.\\n\\n$w(n+1) = w(n)+\\\\eta \\\\cdot e(n) \\\\cdot y(n) $$\\\\Delta w(n) = learning\\\\_rate \\\\cdot x(n) \\\\cdot e(n)$, where x is the input data, $e = d - y$ is the error from the desired output and the actual output, and the learning_rate is a parameter chosen as necessery to change the speed of learning.\\n\\n$w_{new} = w_{old} + learning\\\\_rate \\\\cdot x \\\\cdot e$, this is the formula to update the weights and to learn the input data.weights(t) = weights(t-1) * learning_rate * (desired(t) - output(t))\\n\\nThe Widrow-Hoff rule, also the delta rule, is used to update the weights of neural networks in a learning algorithm. It uses the previous weights' result and compares it to the desired result. This discrepancy is then applied to update the weights based on a learning rate.w_new = w_old + learning_parameter * error(n) * input(n)\\n\\nwhile error is: desired_input - current_output\\n\\nthe new value for the synaptic weight is computed of the old value plus a learning rate times the current error and the input. The output error is decreased in each step until the change is to small or the generalization is sufficient Rule: w+1 = w + n * x * ( y - d)  where n is the learning rate, x is the input, y is the ouput of the network d is the desired output  \\nThe widrow-Hoff rule minimizes the error (y-d). The weight change is proportional the ibnput x and the error. It can be derived from steepest descend.YOUR ANSWER HEREThis the basicly the calulating mean squared  error (MSE) from the expected output and real output.\\n\\nModifiying the weights for Minimizing MSE it . Widrow-Hoff rule states that the weight adjustment is proportional to the product of input and the error in the output. It is also called the delta rule.\\n$$\\\\Delta w_{ji}  = \\\\eta x_ie_{ji}$$\\n$\\\\eta$ is the proportional constant also called as learning constant\\n$$W(i)=W(i-1)+\\\\Delta W_{ji}$$YOUR ANSWER HERE\\n\\n$\\\\Delta W_{ji}$ = $\\\\eta e_jx_i$\\n\\nAdjustment made to the weight of a neuron is proportional to the product of the error in that neuron and input applied to the neuron.Fehler korrektur. w(n+1) =learnparam* (1/2 e*e(n))'\\nEs wird der Fehler an der Ausgabe gemeßen, mithilfe der Ableitung der Aktivierungsfunktion wird der Fehler korrigiert. Korrektur erfolgt in abhängigkeit eines lern parameters, der die schrittgröße angibt. Widrow-Hoff learning rule is derived from LMS error method, and it is defined as: $W{t+1} = W{t} + \\\\mu \\\\cdot \\\\Delta W$, where $\\\\mu$ represent learning rate, and  $\\\\Delta W = -(gradient \\\\ of \\\\ instantaneus \\\\ erorr) = -(d - y)X $, Here $d$ represent desired signal, while $y$ represent output signal of a neuron. $X$ represent input of a neuron\", 'Explain back propagation, use the correct technical terms!': \"In backpropagation, the gradient of the error produced at the output layer (by partially differentiating the cost function with respect to the weights) is propogated backwards one layer at a time back to the input layer. This propagated gradient is used to update the weights in the corresponding layer. Backpropagation is necessary because the desired output at every layer is not known and it is only possible to formulate the cost function at the output layer.In back propagation, there are two phases:\\n\\n1. Forward Phase: First we apply input to the network and compute the current output. \\n2. Backward Phase: We compute the error between current and desired output. Error is minimized by computing gradient of error with respect to weight. In return, weights are adjust.\\n\\nAfter adjusting weights in backward phase, we again go to forward phase and compute the current output, check whether error is minimized or not. YOUR ANSWER HERE Back propagation wants to minimize the error function E. E is given by: \\\\( $ \\\\frac{1}{2}\\\\sum e(n)^{2}$  \\\\). THe error function can be minimized by calculating the gradient starting from the output. Term for calculating the gradient differs. It depends on whether the neuron for which the gradient to be calculated is an output neuron or  a hidden neuron.Backpropagation is the general form of the delta rule, formulated for networks with multiple hidden layers. Here, we propagate the error of the network back to the input layer to determine the change of weights, using the error signal in the output layer and subsequently the local gradients in the hidden layers. In the forward pass, we compute the net output forwards. In the backward pass, we propagate the error backwards. The BP rule was derived from the error gradient w.r.t. the weights, and application of the chain rule.Back propagation is propagation of error from the output layer to the hidden layer in network with multiple layers. \\n\\nThis is done by calculating the local gradient of each node and then using this (along with the weight) to determine how much of the error is to be propagated to the particular nodeBack propagation is used in multi layer network. It consits of two phases: Forward and backward.\\nIn the forward phase we give and input to the network and caculate its outputs. Also memorize the local field of each node. The local gradient (delta) is used to adapt the weights of the layers. It is different for output and the remaining layers. \\n\\nFor node i in an output layer: $\\\\delta_i(v_i) = \\\\varphi^\\\\prime(v_i)(d_i - y_i)$\\n\\nFor node i in other layers: $\\\\delta_i(v_i) = \\\\varphi^\\\\prime(v_i)\\\\sum_{j\\\\in C} w_ji \\\\delta_j(v_j)$, where $C$ are all the nodes that use node i output as an input\\n\\nrepeat this process for all input data until error is small enoughThe back propagation algorithim is there to train a mulilayer feedforward ANN. We change the weights by computing the local gradiant at each neuron by using the neurons in the layer befor. The local gradient of the output neurons can be computed easaly. The activation function has to be differantable for the backpropagation algorithm.\\n\\nIn the forwart pass we compute the output y at the output layer.\\n\\nIn the backard pass we use the output y and our desired output d to compute the local gradients at the output layer. Then we go back layer by layer and use the local gradients from before to compute the new local gradients.\\n\\nBy that we minimize the average squared error function.Backpropagation is a learning algorithm for Multilayer FF NN. It is supervised error correction learning. \\nThe weights are initialised randomly\\n\\nThe algorithm has to steps:\\n\\nIn the forward pass the the output is calculated by using the current weights.\\n\\nIn the backward pass the weight update for the outputlayer is as like in single layer ff. The error is used to update the weights. BP allows us to also calculate the error of hidden layers. For each hidden layer we use a local gradient as the error. The local gradient is the sum of weighted error of the following layer, which is passed trough the derivate of the activation function. So it is possible to backpropagate the error from the output layer to to first layer.\\n\\nA common Problem in BP is the vainshing gradient problem. Depending on the activation function used the local gradient gets smaller in each layer until it is eventually less than the floating point precision used. This limits the number of layers that can be stacked.The back propagation algorithm is a learning algorithm for updating in the weights in a multi-layer neural network. For updating the weights of all the layers, the error of each neuron must be calculated. In the back propagation algorithm, two phases will be defined:\\n- Forward phase: the output of the neural network will be calculated and also the error of the neurons in the output layer.\\n- Backward phase: the gradient of each neuron will be calculated, by using the calculated error on the output layer and the defined connections between the hidden layer and the output layer. If multiple hidden layers are defined, the error will be iteratevely will be given backwards and the weights at each neuron will be updated.Back propagation is a steepest decent method that uses the final produced error and the local gradient to define the amount of change needed for each synaptic weight.\\n\\nIn this method we have two phase:\\n    - forward phase: in this phase we feed the input to the network and the network calculate the output\\n    - backward phase: in this phase we first calculate the error and then use the local gradient to propagate the error to the network from the last layer to the first and manipulate the synaptic weightsBack propagation consists of two steps:\\n1. forward pass - data is passed through the network and weights are atapted\\n2. backward pass - by using local field of each neuron error signal is propagated backward by using local field of each neuron from end to beginning and stacking them up. Local field is partial derivative of the output signal of a a neuron, for output neuron it is simplest to calculate as it has only desired output and actual output to deal with.Backpropagation is a learning algorithm in multi layer networks that consists of two phases, a forward pass and a backward pass. In the forward pass, the output is calculated by passing activations layer through layer starting from the input, then through hidden layer and finally output. Then the error is calculated in the output layer and propagated backward through the network. In the forward pass, the weight do not change. In the backward pass, the weights change in proportion to the local gradient.Backpropagation is a neural network based learning algorithm where the network learns by propagating the error through the network. BP consists of two stages:\\n+ Forward pass: where the error is computed by feeding the input to the network.\\n+ Backward pass: where error is propagated through the network for doing the weight updates locally.\\nSince BP has vanishing gradient problem, it is useful to use activation functions which are infinitely differentiable such as sigmoid function.The back propagation algorithm is used to calculate the error contribution of each neuron after a batch of data is processed. Required is a known desired output of each input value. Thus the back propagation algorithm is a supervised method. The algorithm can be subdivided into two phases:\\n\\n1) Propagation:\\n* Propagation forward through the network to generate the output value(s).\\n* Calculation of the cost error term.\\n* Propagation of the output activation back through the network usin the training pattern target in order to generate the deltas (differences between desired and actual output) of all output and hidden neurons / by recursevliy computing the local gradient of each neuron.\\n\\n2) Weight update:\\n\\nFor each weight the following steps need to be applied:\\n* The weight's output delta and input activation are multiplied to find the gradient of the weight.\\n* A ratio (percentage) of the weight's gradient is substracted from the weight. This ration is also referred to as the learning rate and influences the speed and quality of the learning.\\n\\nLearning is repeated for every new batch until the network performs adequately.Backpropagation is an algorithm for training a neural network, and it contains of two main stages. The first stage is to compute the actual output given the input; in this stage, the signal flows forward from the input layer to the output layer, and the synaptic weights are fixed. The second stage is to update the synaptic weights by propagating the error signals backward from the output layer in a layer-by-layer manner; for each neuron, the local gradient, the partial derivative of cost function to the local field, is computed. Back propogation usually occurs in a multi layer perceptron. \\n\\nIt uses a non linear activation function. \\n\\nBasic elements: \\n1. Functional signals: These are the input signals, which passes through the network from left to right. As the name denotes it performs a usefull function at the output of the neuron and another reason for the name is that the functional signals are calculated based on the parameters and the activation function. \\n\\n2. Error signals: Error signals propogate usually in the reverse direction which contains the error based on the desired output. \\n\\nIt consists of 2 phases: \\n1. Forward phase: In the forward phase the signals propogate from left to right. Weights are fixed and passes through all the layers of the network, that is undergo all the activation. \\n\\n2. Reverse phase: In the reverse phase, the local gradients are calcualted and are propogated through in the backward direction. Here weights change.Backpropogation is used for training multi layer networks. It constitutes of forward pass and backward pass. In forward pass network computes the output. Based on this the errors are calculated based on difference between network output and desired output. These errors are the backpropogated to network during backward pass and used for adjusting the synaptic weights. YOUR ANSWER HERE: Backpropagation is used for Multilayer perceptron network. It consists of two passes.\\n- Forward pass: The outputs are calculated at every computational node and passed till the output node where the error is calculated by difference of desired output and the actual output. In this pass, the weights of the synaptic links are not changed.\\n- Backward pass: The error generated at the output neuron is passed in the backward direction i.e., against the direction of the synapses and the local gradient of the error is calculated at every neuron.Back prop is a way of training a neural network by adapting the weights using error produced. It consists of two phases, forward and backward. Forward phase computes the output along the network using the function signal. In the backward phase, the error of thr outpur fromthe derired output is computed and a local gradient of the error is used to update the weights iof the network. The local gradient considers the credit or blame of the corresponding weights of neuron in producing the output.The back propagation algorithm is based on the error correction learning rule and consists of two passes:\\n1. Forward pass : The input signal applied to the source nodes of the network is propagated forwards through the different layers of the network, and the output is computed at the output layer of the network.\\n2. Backward pass : The error signal computed at the output is propagated backwards, with a local gradient computed at each of the hidden layer neurons, in order to adjust the synaptic weightsof the neuron in the network.Back propagation is moving the error backwards recursively through the network by calculating the local field of every neuron to update the weights. It is based on the chaining rule of derivatives.Backpropagation is a neural network which has two stages:\\n-Forward pass: In forward pass the error is calculated in the output layer with the help of the desired output and the given output. e = d - y\\n- Backward pass: It begins in the output layer , in this case the error is passed backwards with the calculation of gradients at each layer of the neural network\\nSo in back propagation the adjustment to weights is made based on the local gradients which is calculated at each layer.It contains forward pass and backward pass. In the forward pass, input is applied to the network and propagate it forward through the network, then compute the output of neurons in output layer and errors for output neurons. In the backward pass, compute local gradients and update the synaptic weights according to error correction rule for each neuron layer by layer in a backward direction.Back-propagation algorithm consists of two passes:<br>\\n1. Forward pass: the input vector is applied to the network layer by layer\\n2. Backward pass: the weight is adjusted based on error correction learning rule.\\n<br>\\n<br>\\nBack propagation uses error correction learning rule and the objective is to minimize the average of squared error.* Backpropagation is a steepest decent method that calcualtes the error at the output neurons and backpropagates those errors backwards to update the weights of each neuron.\\n* The synaptic weight updated is directly proportional to **partial derivatives**\\n* Local gradient is calculated at ouput neurons and hidden neurons.\\n* Local gradient at output neurons are calculated using the observed error.\\n* But the error function is missing in the hidden neurons, so the local gradient of hidden neuron j is calculated recursively from the local gradients of all neurons which are connected directly to the hidden neuron j. Backpropogation has 2 steps. \\n\\nForward pass: In forward pass the data is run through the network and the error is calculated.\\n\\nBackward pass: In Backward pass the weight is adjusted using local gradient of error such that the error is minimized.\\n\\nThere are many ways for weight adjustment like, steepest descent, Newtons method, Gauss newton method.The back propagation is a learning method in neural networks. Back propagation enables the feed forward netwowrk to represent XOR gate.\\n\\nIt has two phases:\\n\\nforward pass: the initial weights are used to calculate the value of the output neuron\\n\\nbackward pass: starts from the output layer and travels backward. During this phase the weights are changed based on the local gradients of each neuron|Back propagation is used to learn weights in a multi-layer feed forward network. It is divided into two steps: forward and backward. In the forward step one input is passed through the network to calculate the output of the network. This output is used to calculate the error of each output neuron given the desired output. After this forward step, in the backward step the weights are changed beginning in the end of the network. Each weight is changed by taking the derivative of the activation function of the neuron times either the error, if the following neuron is an output neuron, or all local gradients of connected neurons times the corresponding weights. The weight changes are the local fields.Backpropagation is used in Multilayer Perceptrons to give a method of adapting the weights. First the forward phase is run like in a regular feedforward network. Then after the output and thus the error is determined the error is backpropagated from ouput layer through the network. Since we have multiple layers, there is only a desired output of the network for the last layer. To counteract this problem a gradient is calculated for every neuron during the backward pass. The gradient is giving a measure of the contribution of this neuron to the final error. The gradient is then used to update the neurons weights. If the neuron is not part of the output layer, the previous gradients are used to calculate the new gradient instead of using the error.Back propagation consists of two steps:\\n1. step - Forward pass: Here the input data is fed into the network and the output is calculated at the output nodes. The usual calculations of the induced local field are done by using this formula $v = \\\\sum wx + b$. The output is then calculated using this formula $y = f(v)$, where f() is the activation function.\\n\\n2. step - Backward pass: Here the error is backpropagated through the network from the output layer to the input layer. In the output layer the error is calculated using this formula $\\\\delta = d - y$, using the desired output d and the actual output y. In the layers before the output layer the local gradient is used to calculate the error using the error from the output layer $\\\\delta = w\\\\delta x$. Additionally the weights are updated using $w_{new} = w_{old} - learning\\\\_rate \\\\cdot \\\\delta x$Back propagation is a learning algorithm for multilayer neural networks. At first, the input is propagated through the network until the end is reached. Here the error is calculated with the desired result. Then the error is used to update the weights from the back to the front. For the output layer the weights can be updated directly with the calculated error. The following layers have to use the local gradient of the previous error, which is calculated with the derivative of the activation function and its error. This is then used to update the weights and repeated until the front is reached.back propagation is used in multilayer feedforward networks. first the forward pass is computed. The given error at the output nodes is used to compute the weight changes using widrow-hoff learning rule. then the error is given back layer by layer in the backward pass to compute the error and weight changing for each layer recursivly. The learning can be done in sequential (online) or batch mode (offline) In multi layer ff networks the error is only available in the last layer. Therefore the error is propagated back through the network using the backpropagtion algorithm. In order to do so the local gradient has to be calculated. \\nUpdate of the weight: w+1 = w + n * x * gradient where the iput x is the output of the previous layer.\\nThe local gradient is calculated diffrently depending if the neuron is in the output layer or in the hidden layer.  \\nOutput layer: $ gradient = phi`(x) * (y -d) $  \\nHidden Layer: $ gradient = phi_j`(x) * SUM(w_i * local gradient_i) $    In steepest gradient weights are adjusted in decreasing direction of error function. But for hidden neurons there is no labels available to calculate the error. Hence final ouput error is backpropogated through the layers inside the hidden layers of NN. This is possible with continuous activation function and chain rule on its derivatives. \\nFinal error is differentiated with respect to hidden weights. Chain rule is applied to find local error on hidden neurons. the back propagation algorithm  it consist of forward pass and backward pass\\n\\ncomputes the output of the neuron \\n\\nthen it propagates in backward direction while recursively compute local gradient of the neuron \\n\\nweights are adjusted accordingle.Back Propagation is the process of learning in Multi Layer Perceptron in which the error from, the output of the network is fed back into the network to adjust the weights in the hidden layer. That is the error back prpagates into the network to enable the network to learn by adjusting the synaptic weights based on it.YOUR ANSWER HERE\\n\\n* Back propagation is a process to make adjustment to the weights of a neural network in a way that minimizes the average squared error of the training data.\\n\\n* It uses steepest decent method. In each step it moves towards the direction that gives maximum decrease of the error.\\n\\n* In back propagation the error is propaged backward from the last layer towards the earlier layers. The adjustments made to the weights is proportional to the partial derivative of the error with respect to the weight.\\n\\n* The partial derivative is calculated using repeated application of the chain rule.Algorithmus zum trainiern eines Feed Forward Netzes. Erwünschte Ausgabe für Trainingsdaten bekannt\\nBesteht aus 2 Schritten:\\nForwardpass: Berechnen des Fehlers an den Ausgabe Neuronen.\\nBackwardpass: Rückpropagieren des Fehlers durch rekursives berechnen des Fehlers der einzelnen Neuronen mithilfe des lokalen Gradienten und anpassen der Gewichte. Dies wurde hergeleitet mithilfe der Chain-RuleThe idea of back propagation method is to propagate error from ouput (final) layer backward to hidden layers, and adjust the weighs of neurond in hidden layer, based of this error. This is required because we do not have error information for hidden layers, only for output neurons. The error from output layer is propagated to hidden layers using idea from steepest descent method. Namely, local gradients are computed for each neuron in backpropagation, and these local gradients define how error changes, in terms of weights. Local gradients are derived from chain rule for each layer. The fact that local gradient for each hidden layer is derived based on local gradient of a previos layer, defines that as we propagate more and more in hidden layers of NN, the gradient of a error function vanishes, which means that as we go deeply back in NN, the change in weights is becominng smaller and smaller. This is a drawback of back propagation method. \", 'When learning using steepest descent, explain the role of the learning rate? What is a danger?': \"Learning rate controls the speed of the descent. When learning rate is low, the weight updation is overdamped and convergence is slow. When the learning rate is high, the weight updation is underdamped and a zigzagging behaviour is exhibited in the weight space. When the learning rate is too large, learning becomes unstable.If learning rate is very smaller, then transition are over-damping, trajectory of weight vector follows the smooth path.\\n\\nIf learning rate is large, then transition are under-damping, trajectory of weight vector exhibits the zigzagging(or oscillatory) behavior\\n\\nIf learning gets higher than some threshold, then learning algorithm gets unstable or divergesYOUR ANSWER HERE Learning rate n determines stride of delta of weight. If learning rate is too large weights starts to ziggerate. When training with SD, the learning rate determines the step size we take towards the negative gradient. When the learning rate is too small, the weights may be overdamped and reach the error function minimum slowly, eventually getting stuck in local minima. When step size is too big, the weights may be underdampened, bouncing between ridges of the error surface and never find the minimum (especially when the minimum is in a steep ravine of the error surface)- Learning rate is used to control how much the wright update is affected by the error correction or so on.\\n- Learning rate too low: Learning is slow and takes more time\\n- Learning rate too high: Learning is fast, but causes zigzagging behaviour in convergence.\\n- If the learning rate is too high, it may result in situations where the zigzagging behaviour will cause it to overshoot, and may never finally converge.The learning rate defines the speed of the weight change. A learning rate too high can lead to oscillation around the optimal weight such that its never reached. A learning rate to low results in very slow learning and slow convergence.The learning rate is needet to make the algorithm more stable. \\n\\nA high learning rate makes the weightchanges zickzacking and the algorithm might not converge\\n\\nA low learning rate makes the path in the W-plane more smooth.\\n\\nIf the learning rate gets to a certan critical value the algorithm might not converge at allThe learning reate is a factor of how much we trust the datapoint. Normally it is in the range of [0,1]. A high learning rate normally results in a faster convergence while a lower rate in a slower conversion. If the rate is choosen to high, it is possible that the cost function diverges. If the rate is to slow it is possible that the rate so conversion is so slow that we never reach a local minimum.The learning rate is a parameter using on updating the weights in a given iteration. This parameter represents the importance that is given to the adaptation of the weights. So when setting the learning rate small, the learning machine will learn slower but also in a more stable way. On the other hand, when setting the learning rate with a large value, the learning machine will learn faster but in an unstable way. The danger here, is that depending on the learning rate's value, the algorithm may never come into the perfect value. If the learning rate is too small, it may land into a local minimum and never approach the global minimum of the function. If the learning rate is too big, the learning progression will have a zig-zagging behaviour and never approach the ideal value.- The learning rate defines the size of steps that the method moves in the search space.\\n- If the learning rate is too small the method needs to take huge number of steps and maybe it stuck in a local minima\\n- If the learning rate is too big the method will converge very fast toward the global minima but there is a probability that it oscilates around the global minima and never reachs it\\nIf learning rate is to large, then proccess will oscillate a lot and might not converge.\\n\\nIf learning rate is to small, then convergance will happen very slowlyThe learning rate tells us how confident we are of the error, and it affect the convergence rate. A low learning rate will slow the convergence, making the system overdamped. A high learning rate will speed the convergence but the value oscilates, making the system underdamped. The system can become unstable if the learning rate is above a threshold value.+ If the learning rate is too small, then the system is overdamped and the algorithm takes a long time to converge.\\n+ If the learning rate is too large, then the system is underdamped and the algorithm oscillates around and optimal solution or could potentially make the system unstable.The steepest descent method is an algorithm for finding the nearest local minimum of a function which presupposes that the gradient of the function can be computed. The method of steepest descent starts at a point P_0 and as many times needed moves from P_i to P_(i+1) by minimizing along the line extending from P_i in the direction of gradient f(P_i) the local downhill gradient. The danger of the algorithm is, that it can get stuck in a local minima.The learning rate determines the rate of learning: the smaller the learing rate is, the slower the learning process is, but the path of weight adjustment is smoother. The larger the value is, the faster the learing process is, but it can result in oscillation and instability. The learning rate is $\\\\eta$ So based on the learning rate, it undergoes various oscillation. \\nWe could see zigzagging behaviours. \\n1. When the learning rate is large, the system is said to be under damped. \\n2. When the learning rate is small, the system is said to be over damped. Here we can see a zigzagging behaviour towards the convergence phase. \\n3. After the learning rate crosses a certain value it becomes unstable. \\n\\nIt may stuck in a local minima which is considered to be another danger Learning rate in steepest descent can directly affect the convergence of the algorithm. If the learning rate is very small then algorithm can take long time to converge i.e response is ovderdamped. But if the learning rate is amde very high then we may observe zig-zagging (oscillatory) behaviour and sometimes algorithm may fail to converge  (underdamped response).YOUR ANSWER HERE:\\n- When the learning rate is small, the learning is very slow.\\n- When the learning rate is large, the learning is unstable and can exhibit zigzag behavior.\\n- When the learning rate is too large, the learning never converges.The learning rate defines the efficiency of learning machine. If it is small, the system response may be overdamped, if large , the response may be underdamped and if it exceeds a critical value, the response may diverge.\\n\\nThe danger is the possibility of the system output to not converge. This should be ensured by scaling the learning rate using the largest eigen value of the correation matrix of the input.The value of the learning rate parameter $\\\\eta$ controls the speed of descent and convergence towards the optimal weight vector. For small values of $\\\\eta$, the transient response of the algorithm is overdamped and the weight trajectory follows a smooth path. On the other hand if the value of $\\\\eta$  is large, the transient repsonse of the algorithm is underdamped, and the weight trajectory follows an oscillatory path in the W-plane.Learning rate $\\\\eta$ has a profound impact on the learning in steepest descent. \\n1. If $\\\\eta$ is too small, the system is underdamped and convergence is slow. \\n2. For larger $\\\\eta$, the system is overdamped and tends to oscillate.  \\n3. If $\\\\eta$ exceeds a certain critical value, steepest descent may even diverge!Learning rate has huge impact on convergence of the network. If the learning rate is low then the transient response of the algorithm is overdamped and the trajectory of w(n) is smooth. If the learning rate is high then the transient response of the algorithm is underdamped and trajectory of the w(n) is zigzag. If we choose the wrong learning rate then  the network might not converge.learning rate controls the speed and convergence of steepest descent method. 1. if it is small, the trajectory of weight vector follows a smooth path in W plane; 2. if it is large, the trajectory of weight vector follows a zigzaging path; 3. if it exceeds a critical value, then the algorithm is unstable.1. Large learning rate $\\\\eta$ results in a zigzagging behavior but it can converge quickly.\\n2. Small learning rate $\\\\eta$ results in a smooth behavior but it is slow to converge* Learning rate tells the network that how much steps it should move towards direction opposite to the gradient vector.\\n* If the learning rate is too large, the weight updation will be high. \\n* So the danger is, learning may oscillate or the network overfit the data.Learning rate is used to regulate the speed of learning. If the learning rate is small then the learning is slow and if the learning rate is high then it oscillates. If it exceeds the critical value then the algorithm is unstable.Learning rate is used to decide how fast the network should converge during the training phase\\n\\nIf the learning rate is too high - the system oscillates and  becomes overdamped\\n\\ntoo low - the system becomes underdamped and learns very slowThe learning rate tells how long one step in the method of steepest descent is. If the learning rate is too high, the learning will oscillate and may not converge. If the learning rate is too small the convergence will take many iterations.If we use steepest descent we use the learning rate to adjust the speed of the convergence to a minimum error. If the learning rate is too small, the learning is going on rather slow. If the rate is high, the error is zigzagging on the error surface towards the minimum. If the learning rate is to high, it might not converge but diverge.The learning rate is a value between 0 and 1, which determines how fast the network learns. When using small values for the learning rate, the network converges slowly and needs alot of processing. When choosing big values the learning oscillates and becomes unstable. The goal is to choose the learning rate in a way that it does not learn to slow, which needs more input data for convergence, and that it does not become unstable.The learning rate defines the speed of the learning convergence. High values lead to faster learning und low values to slower learning. However, high values can lead to oscillations in the learning space and may overshoot the desired result and never reach it.The learning rate gives the speed of learning. It defines the stepwidth in direction of steepest descent. If the learning rate is small, the learning is more stable but slower. When it is high, the learning is more unstable but faster. The danger is to overcome a minimum and result in oscillating behaviour A too small learing rate can lead to a very slow convergence or to no convergence at all if the time learn becomes too long. A high learning rate can lead to an oscillating behavior and prevent convergence.Learning rate is a scalar multiplied with adjustment term to adjsut the weights. It ensures the rate of learning. It is typical greater than 0 and less than equal to 1. It govers the rate of sliding alond the curve towards the minima. \\n\\n1. Lower learning rate will result in slow learning but chances of finding optimal minima are greater. \\n\\n2. Higher learning will result in hopping on either side of minima hence zigzag behaviour.\\n\\n3. Very high learning may not converge. if the learning rate is large then the it follows the zizag motion.\\n\\nif the learning rate is too low then it takes time for converging .\\n\\nif the learning rate is very large or critcal then it becomes unstable.\\n\\nwhile processing there is possiblity that it will get stuck in local minima.When using steepest descent the learning rate($\\\\eta$) determines the speed at which the weihts are adjusted in the NN. There can be two possible danger related to leraning rate depending on its magnitude:\\n1. Low learning rate(eg, $\\\\eta = 0.01$)  results in smooth variation of the weights but makes the process becomes slow.\\n2. Hight learning rate (eg, $\\\\eta = 0.01$) results in faster weight adjustment but it leads to an oscillatory nature in the learning which is unwanted.YOUR ANSWER HERE\\n\\n* With a small learning rate the network will converge very slowly towards the optimal weight of the network but it will give better perfomance in generalization.\\n\\n* With a high learning rate there can be zigzag effect. because of the large rate the network may miss a local minima and jump to a higher point.\\n\\n* With a very high learning rate the network may become unstable.learning rate zu klein: Convergenz wird nur sehr langsamm erreicht.\\nlöearning rate zu groß: Zig zaging um minima herum, Convergenz wird unter umständen nie erreicht.The learning rate defines the speed of steepest descent search for min of a error funtion. In other words, it defines how strong the change in weights will be, throughout optimization procedure. Higher learning rate, faster learning, but then learning is characterized by oscilations in searhc for min. This is dangerous because if learning rate, becomes bigger that a certain value, it can make search with steepest descent unstable. IN this case steepest descent will start to diverge, istead of converging to min. \\nIn other case, when learing rate is small that lerning is slower but safer, and the learining path is not oscilatory.\", 'How does a Reduced Boltzman Machine work (main idea)?': 'The reduced blotzman machine works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.It is a recurrent network. It opreates by flipping. \\n\\nIt has two groups of neurons: Visible neurons and hidden neurons. Visible neurons provides interaction between environment and network. Hidden neurons are running freely. \\n\\nIt has two modes of operation: \\n    . Clamped State: states of the neurons are clamped.\\n    . Free running state: Neurons are running in free conditionYOUR ANSWER HERERBM implement a combination of graphical and probabalistic ideas, using probabilites of activations inspired from energy based networks. We present a training input to the RBM, and determine the hidden activations based on a probability of net input and edge weights. Then, when unclamping the training data from the network, sample from the distribution of the hidden layer, where the RBM tries to rebuild the distribution of the input data. RBM may be used for data completion or denoising, where e.g. incomplete images are complted based on the learned probability distribution.RBM has two layers and are interconnected (recurrent) operates by flipping the internal states (+/- 1)> Unlike the boltzmann machine, reduced boltzmann machine does not contain interconnections among the same layer. The weight update is done by the differnce in correleation in clamped and  free running mode.It consists of only two layers: input and hidden layer. During training data is presented to the input. The hidden layer starts oscillating.The Reduced Boltzman Machine is an stochastical recurrent ANN, that operates with two classes of neurons : hidden and visible. It operates by neuron-flipping with a probability impacted by the neurons arount. So it uses the hebbian rule. An Reduced Boltzman Machine can learn the classify data and can repoduce the learned patterns.The structed of RBM is a bitpartied graph. It uses hebbian learning for training and the neurons used are binary stoachastic neurons, which have a binary state, which fire based on a probability. The training is achived by passing the information a many times between the hidden layer and the input layer. There weightsare updated on the pass into the hidden layer. Weigths between input and activations in the hidden layer are increased, weights between gernerated inputs of the rbm and the hidden layer are decreased.The main idea of an RBM can be defined as follows:\\n\\n- Two layers will be defined, where each neuron will be connected to every neuron of the other layer.\\n- The input will be passed from the first layer to the second one, and the state of each neuron of the second layer will be calculated.\\n- The neurons with active states will pass again its values to the input layer.\\n- The values given from the second layer will be compared with the input values, and with the two states, the weights will be adjusted.YOUR ANSWER HEREThey are neural network with only one hidden layer, neurons from input to hidden layer are fully connected, neurons from hidden layer to output layer are fully connected as well. The Reduced boltzman machine works by flipping neurons. It can operate in clamped or free running state.\\n- If two connected neurons are activated at the same time, the weight is increased.\\n- If any of the two neurons are fired asynchronously, then the weight is reduced or removed.+ Reduced Boltzman Machines (reduced because inputs do not share information via synapses) are one of the initial NNs which consists of input layer and hidden layer. The system adapts its internal weights and tries to reproduce the inputs.A RBM is a shallow two layer network containing a visible and a hidden layer. Each noden in the visible layer is connected to each node of the hidden layer. It is considered as restricted, because no two nodes of one layer share a connection. A RBM is the mathematical equivalent of a two way translator. In the forward pass a RBM takes the inputs and translates them to a set of numbers that encode the inputs. In the backward pass it takes the set of numbers and translates them back to form the reconstructed inputs. A well trained RBM will be able to perform the backward translation with a higher degree of accuracy. \\n\\nThree steps are repeated over and over through the training process:\\n\\n1) Forward pass.\\n\\n2) Backward pass.\\n\\n3) Evaluate quality of reconstruction as visible layer (often solved with KL divergece)YOUR ANSWER HEREYOUR ANSWER HERERBM is an unsupervised learning technique. It has visible neurons and hidden neurons. Neurons are in either +1 or -1 states. It uses the idea of simuilated annealing to flip the neuron states based on energy function and pseudo temperature. It operates in 2 states - clamped state and free flowing state. In clamped state only hidden neurons are flipped and in free flowing state both visible and hidden neurons are flipped. Weights are adjusted based on avergage correlation difference between all the neurons in clamped and free flowing state.YOUR ANSWER HERERBMs work on the principle of binary states, free-running or clamped. The weight update is done based on the Botlzmann\\'s formula using the pseudotemperature, which gives the proobability of error.The Reduced Boltzman Machines function by using two types of neurons : visible neurons that provide an interface between the environment and he network, an hidden neurons that operate freely. The learning can proceed under two conditions, namely:\\n1. Clamped state : where the visible neurons are clamped to a particular state of the environment\\n2. Free running state : where both visible and hidden neurons operate freely. \\n\\nIf $\\\\rho^+_{ij}$ indicates the probability of correlation between the states of neurons i and j in clamped state, $\\\\rho^{-}_{ij}$ indicates the probability of correlation between the states of neurons i and j in free running state, then the weight adjustment $\\\\Delta w_{ij} = \\\\eta (\\\\rho^+_{ij} - \\\\rho^{-}_{ij})$A Reduced Boltzmann machine (RBM) consists of two layers of neurons: visible and hidden. The neurons may only have two states i.e. activated or not and they flip according to a certain probability based on the weights and states of other neurons. The RBM has two modes:\\n1. Clamped: The visible layer is clamped to a certain input while the hidden neurons are allowed to change state until the network settles. The correlation in this state is given by $\\\\rho_{ij}^+$\\n2. Free-running: In this state, the network is allowed to flip all neurons until it settles. The correlation is $\\\\rho_{ij}^-$  \\nThe weight update rule is given by \\n$$\\\\Delta w_{ij} = \\\\eta (\\\\rho_{ij}^+ - \\\\rho_{ij}^-)$$Boltzmann machines is a neural network having recurrent structure.It is in two states either on which is +1 or off which is -1.The energy function is given by \\n\\n$E = 1/(1+exp(-delta E/Temperature))$\\n\\nThe state of the input x is turned from +1 to -1 based on the change of the energy delta_E and the pseudo temeperature T.The neurons operate in a binary states, \"on\" or \"off\". In clamped condition, all visible neurons are clamped into specific states by the environment; in free running condition, all neurons including visible and hidden neurons operate freely.It uses an energy function to oversee the learning processReduced boltzman machine work based on **flipping operation** and calculating the probability invariances of clamped state and freely running state.RBMs run on boltzmann learning rule. The neurons have 2 modes of operation clipped and free running.\\n\\nAll the neurons are binary units. Their status can be changed by flipping. All the neurons that are in on position are clipped together.It has the structure of recurrent neural network. \\n\\nIt has two layers of neuron visible and hidden.\\n\\nthe neuron can store only binary values\\n\\nthey work based on flipping\\n\\ntheere are modes free running and clamped\\n\\nthe weights are changes based on the correlation of the neurons in the free running mode and clamped modeIn a Reduced Boltzman Machine there are one visible and at least one hidden layer. The visible layer is the input and acts as output at the same time. For each input the neurons of the visible layer will be assigned with a value. With their weights, hidden neurons may either be activated or not. Once the input has been passed through the hidden layers, the values are passed all the way back to the visible layer. For this, different weights are used since the values move in the opposite direction. In RBMs there are two states, the free running and the clamped state. During the clamped state, the input neurons are clamped to the output neurons. While the network is clamped the probabilities of the Hidden states to be in a certain state are calculated to determine a probability of the output to be correct.The Reduced Boltzman Machine hast an input layer and a hidden layer. Each neuron has a state and a probability to turn on. If the neuron turns on the data passes trough it and the weights are updated. The probability of turning on is calculated by the network.Two fully connected layers, one input and one hidden layer are used. The input layer is the only connection to the environment. The RBM has a specified energy level which can not be changed. However the distribution of this energy to the nodes can be changed. Based on the data input every node has a chance to flip based on its input connections.the binary state of each neuron is flipped by a given probability. Stochastical learning Neurons have to states e.g. on or off. Each neuron has a probability to flip from one state to another. YOUR ANSWER HEREthe main idea of the RBM is compute the Least mean square error of the difference between expected output and real output.YOUR ANSWER HEREYOUR ANSWER HERE\\n\\n* It is a Recurrent neural netwokr\\n* It uses two groups of neurons, hidden and visible\\n* It process the training data by flipping the neuronsBoltzman lernen ist äquivalent zum simulierten abkühlen.Reduced Boltzman Machine is a biparted (two parts) Reccurent NN, that has two layers visible and hidden layers. In Reduced Boltzman Machine neurons can have two states, namely, + or - 1, depening on current time step. At each time step, the states of neurons are flipped. Here the visible layer represent interface for connection between evironment and hidden layer, and it operates in clamped mode (limited values by environment). WHile hidden layer, operates in free mode.', 'Define: Echo State Network (ESN), how are they different to FF NNs?': 'Echo State Network is a type of Recurrent Neural Network and has atleat one cyclic (feedback) connection. ESN consists of a dynamic reservoir and a output layer with neurons. The dynamic reservoir consists of randomly initialized neurons with random sturcture and connections (with atleast one feedback connection). The output layer combines the dynamic behaviours of the reservoir in a required fashion. Only the weights of the output neurons are updated while learning.\\n\\nAn ESN consists of feedback connections while a FF NN does not.\\n\\nAn ESN could have persisting activations even when there is no input which is not the case in FF NN.\\n\\nAn ESN can approximate dynamic systems while a FF NN cannot.Echo State network are recurrent neural network, which means these networks have feedback. While, in feedforward neural networks, there is no feedback. In feedfoward, training data or inputs are not dependent on each other. They do not have any system memory. In ESNs, training inputs are dependent on each other and they have system memory\\n\\n\\nIn Echo state network, there are fixed, random generate reservoir weights. These weights are not trained. While, only output weights are trainedYOUR ANSWER HERE An ESN is a recurrent neural network with many layers and fixed weights. There are several differences. An ESN has a cycle that means witin the network there are backwarded connections. Withn a FF NN there are only feedforwad connections. Within a FF NN all weights are trained. Within an ESN only output weights are trained. An ESN can produce an output without any input. A FF NN needs an input to produce an output.ESN are different to FFNN in so far that they consist of a reservoir of hidden neurons, which may be connected recurrent, as opposed to having a feed forward architecture. Here, the inputs are connected to the recurrent dynamic reservoir, whereas the DR is connected to the linear output layer. The Output layer may be again connected to the DR, whereas during training only weights of the last layer are learned. Weights of the DR of the ESN are thus initialized and never learning, although since have been extended to minimal complexity architectures.ESN are recurrent neural networls with a large reservoir (or echo chamber) with many nodes (recurrent). The weights are learnt only for the connection between this reservoir and the output layer. The weights are not learnt for the nodes inside the reservoir. The main idea is that during training, the input layer cuases the states inside the reservoir to behave in caertain way, and the weights in the output layer is adjusted to match this and the labelled output.\\n\\nFFNN are feed forward networks, i.e., they do not have any recurrent connections, which is the main difference with respect to ESN\\n\\nESNs are a special class of recurrent neural networks. In contrast to ff they also allow backward node connections and thus are able to memorize data.\\nThey are defined by: $x_i$ input i, $y_i$ output i, a dynamic resaviour, and weights connecting all the components.\\nThe dynamic resaviour is generated randomly and fixed. Its topology including weights is never changed.\\nOnly the weights between output layer and dynamic resaviour are changed during training. Because the dynamic resaviour allows all kinds of connections between its nodes it can contain memory that is able to remember data. It also has a spectral radius.An ESN is a recurrent ANN with randome, sparse and fixed interneuron connections in the hidden layers. Just the output layer weights get trained, because the network itself is so complex, it can model very much. If the training was not successful we can just create a new randome ESN. Training an complete ESN would by very complex and would take very very very long.\\n\\nA FF NN is not recurrent (no feedback) and all its weights get trained and most of the time the interneuron connections are not sparsly.A Echo State Network is a RNN, is has a dynamical reservoir of neurons which are connected with each other and itself. the DR typically consists of more that 100 neurons. The outputlayer consists of linear readouts of the DR. So a neuron in the output layer sums up the weighted behaviours of the DR neurons. The DR is randomly initialised and only the output layer is trained by supevised learning. \\n\\nThe main Diffrence is that ESN is a RNN. In contrast to FFNN it can resemble any dynamical system. Usually it is used for time series prediction.Echo State Networks are a type of recurrent neural networks, where the input layer is interconnected to a reservoir (a random initialized group of neurons with also random interconnections), and this reservoir is connected to the output. The reservoir will not be adjusted, but the output weights. The output weights can also have recurrent connections with the reservoir. The states on the reservoir neurons will be calculated, and with these states and the output weights, the output will be extracted. \\n\\nThe main difference with the Feed Forward Neural Networks (FF NN) is that in the FF-NNs there\\'s no recurrency, so the input values will be passed to the next layer.- In the ESN we have a huge recurrent network which is called \"Dynamic Reservoir(DR)\" and we have an output layer connected to this DR and we will train the network by adapting and manipulating the connection weights just to the output layer\\n- Unlike a feedforward network in a ESN because of the DR we have at least one loops that returns the output of a neuron with some time delay therefore we have memory in our network but in FF NNs we don\\'t have any memoryEcho State Networks are recurrent neural network type, meaning there are feedbacks in its structure. It is usually only 1% connected. Main difference is that it has a reservoir as a hidden layer where neurons are very randomly connected, with random weight etc. During learning phase only weight outputing neurons are changed. It is required more that 100 neurons to be in a reservoir.The ESN is a type of neural network model that uses a recurrent neural network as a large, random, fixed dynamic reservoir that remains unchanged during training and only changes the weight of the reservoir to output layer. \\nESNs are a form of recurrent neural networks with a least one recurrent input. The ESNs are reservior computers which have memory and can be activated without the inputs. In ESNs, instead of training we evolve the network state by feeding it input sequence.\\nESNs are different from FF NNs because ESNs contains at least one recurrent connection (feedback).The basic idea of ESNs is to use a large, random, fixed recurrent NN (referred to as dynamical reservoir) and to train only connections from the reservoir to the output.\\n\\nThe main difference to FF NN lies in the recurrent part of the network, where back passes are built in, giving feedback previous layers. It is not possible to maintain the reservoir beforhand so it suits the given problem. There is a lack of investigation of reservoir construction. An Echo State Network (ESN) is a modified version of a recurrent network. It has a reservoir, which is a large number of hidden neurons with sparsely-connected random and fixed weights. To train an ESN, only the weights connecting the reservoire and the output layer are adjusted; therefore, the efficiency is better than a normal recurrent network. Echo state network provides structure and supervised learning for recurrent neural networks. It mainly 1) Directs the fixed, large reccurent neural netorks by providing an input stimuli and also fix a response signals to the neurons which are present inside the reservoir(Pool of neurons) 2) It can be directed to get the desired response by the trainable linear combiner of the response signals. \\n\\nUnlike FF NNs, ESN\\'s have memory. They can be also activated without an input stimuli, whereas in case of FF NN, they require a external stimuli so that they are activated. Also the neurons needs to connected in one full cycle. Echo State Newtors are type of RNN. It has dynamic reseivoir units which exhibits different dynamics. Weights of these reseivoir units are fixed and are not changed during the training phase. Only the reseivoir to output weights are changed to learn the inputs. These networks converge only if reseivoir units exhibitg echo state property i.e its ouput depends only on the previous inputs. this property is satisfied if spectral norm reseivoir weights is less then 1.YOUR ANSWER HERE:\\n- Echo state networks are recurrent neural networks that have a large resorvoir of oscillator functions that are connected to the input layer.\\n- In FF NNs, consideredthe outputs at the hidden layers are also considered but in ESNs, the ouputs from the reservoir to the final output layer are only considered.ESN are another implementation of RNNs where training method is completely different. They comprise of a dynamic reservoir with fixed hidden to hidden connections which makes up an RNN with sparse connetivity. Only the output weights which connect the dynamic units and the output of the reservoir are trained using error, unlike RNNs, where the hidden weights are also trained. ESNs are less compuationaly expensive since they can be easiliy trained with experimentation .However, RNNs use much less hidden units compared to ESN for a similar task.An Echo State Network (ESN) is a neural network that uses a reccurent neural network (RNN) as dynamic reservoir which is not changed during training, and trains only the connection from the dynamic reservoir to the output layer. An echo state network is different from FF NNs due to the presence of feedback connection with the dynamic reservoirs which enables it to maintain activation even without inputs. Each unit within the dynamic reservoir in ESNs are excited differently to different inputs.ESNs are recurrent neural networks with at least one cyclic connection and are based on the concept of reservoirs. In contrast FF NNs do not have any cyclic connections. Additionally, in ESN the output weights are trained but the reservoir weights are not whereas in FF NNs all weights are trained. The ESN has memory while FF NNs do not have memory.ESN refers to echo state networks. Echo state networks are the recurrent neural networks where the hidden to hidden layer weights are selected randomly and are fixed and hidden to output layer weights are changed by the learning process.Since ESN is recurrent neural network hence the output echoes throgh the network even when there is no input where as in ff nets there is no feedback so there is no output if there is no input.ESN is a kind of Recurrent NN, which has a large, random , fixed RNN called dynamic reservior and only the weights connecting the reservior and output layer are trained. So ESN combine the desired system function and input/output history echo function.ESN provides an architetcure of supervised learning principle for RNNs. It is different from FF NNs, because it has a reservoir (based on RNNs) to find a non linear signal response and combine the desired output by a trainable linear combination of these response.* Echo state networks are recurrent neural netwoks with **Dynamic Reservoirs.**\\n* Weights initialized in the dynamic reservoris will not be updated during training.\\n* Only the weights in output layer (readout states) is updated after each iteration.\\n* In FF NN, all neurons are connected with other neurons in next layer and all the weights are updated in each iteration.\\n* But in ESN, the **neurons are connected randomly** with other neurons and it is **recursive** and the **weights are not updated** in the dynamic reservoir.ESN is a type of RNN. It has a dynamic reservoir. All the neuron are connected to each other. The Echo state netwrork has a large number of recurrent neural network in them. this set of RNN is called the dynamic reservoir. \\n\\nThey can approximate any dynamic model \\n\\nthey train the model by changing only the weights of the connection of output of the dynamic reservoir and output of the network\\n\\nFF: \\n\\nthey can approximate any continuous function\\n\\nThey train by adapting all the weights in the networkAn echo state network contains of an input layer which is connected to a reservoir, which is a big recurrent network. The output layer is connected to the neurons of the reservoir. While learning in an ESN, only the weights between the reservoir and the output layer are changed, no changes within the reservoir.\\n\\nDifferences to feed forwared networks are, that the reservoir is recurrent and that during the training not all weights are changed, but only the ones between ouput layer and reservoir.In contrast to regular feedforward networks, ESN belongs to the group of Recurrent Neural Networks. It has a regular input layer like the FF, then comes a dynamic reservoir, which is a layer of neurons, where at least one full cycle of connections between the neurons is given. The connections inside this reservoir are not constrained and can thus be any possible connection. This reservoir is randomly initilaized and kept that way. Only the respective connections to the output layer are trained during the learning process. ESN have an input layer connected to a reservoir, which is a recurrent neural network. The reservoir is connected to the output layer. On the connections to the output layer are weights, which are updated by the network. The weights of the reservoir are chosen randomly and not updated at all.An ESN is a recurrent neural network, that consists of an input layer, a dynamical reservoir and an output layer. In the dynamical reservoir feedback loops are possible in contrast to a feedforward network. However, this dynamical reservoir is only randomily initialzed and not learned. Only the connections to the output from the reservoir are learned. Normally, in FF NNs all connections are trained.echo state networks have dynamical reservoir as hidden layer. The dynamical reservoir consists of recurrent non-linear neurons. Only the linear connections from dynamical reservoir to the output layer are trained. The difference to FF NN is, that the ESN is a recurrent networkThe core of an ESN is an arbitrary network with recurrence.Echo state networks have dynamic reservoir with echo state property which is a randomely initialized RNN. \\nHence it can maintain its own internal state. Which is not possible in FF NN. RNN have feedback connections which ecoes back the state of reservoir as well as previoulsly applied inputs. Hence it can model dynamic systems which not possible with FFNN. ESN are the RNN recurrent neural network which has at least one feedback cyle. \\n\\nFF NN are normally forward moving networks where the input from one layer is fed into next layer and generated the output . \\n\\nbut IN ESN the out put is again fed back as input . ESN is tend to have Resvoir where its randomly connected.Echo State Network is a type of neural network which has a recurrent network of 100 to 1000 neurons called dynamic reservior, as the hidden layer. The weights are choosen randomly. The synaptic weights from the resorvoir to the output layers are only adjusted during the learning process.  \\n\\nThey are different from the FF NNs in the following regards:\\n1. ESN have atleat one loop wheras the FF NNs dont.\\n2. Only the output weights are adjusted in ESN , in FF NNs both the input and output weights are adjusted.\\n3. ESN s have a memory, FF NNs dont.YOUR ANSWER HERE\\n\\n* ESN uses a large set of recurrent neurons called reservior.\\n* The weight of reservior neurons does not change after initialization.\\n* The network only lears the weight of reservior to output.\\n* It works very well for one dimentional time series data\\n\\nThe Feed forward networks works differently. The input is feed through the network layer by layer and error is propaged backward to make the adjustments till the first layer. In case of ESN the adjustment is made to the reservior to output weight only.ESN besteht aus eingabe Schicht, einem pool und einer Ausgabeschicht.\\nDie eingabe Schicht ist verbunden mit dem Pool, optional auch mit der Ausgabeschicht. Der Pool ist Recurrent verbunden mit sich selbst und forwärts verbunden mit der Ausgabeschicht. Optional kann auch die Ausgabeschicht Rekurrent mit dem Pool verbunden sein. Nur die Verbindung von Pool zur Ausgabeschicht wird verändert der rest ist statisch.\\n\\nUnterschiede:\\nFFNNs haben keine Recurrenten Verbindungen.\\nBei FFNNs werden alle Verbindungen gelernt/verändert\\nBei FFNNs wird das wissen in form der Gewichte gespeichert, bei ESN auch durch den Zustand des NetzesYOUR ANSWER HERE', 'Describe: the structure on an CNN.': \"In a Convolutional Neural Network, the layer order is:\\n\\n1. Convolutional layer (has kernels which convolve over the input image incase of first layer or feature maps otherwise).\\n2. Activation layer (ReLU activation).\\n3. Pooling layer (max or average pooling). These 3 layers can be repeated any number of times.\\n4. Finally one or more fully connected layers followed by softmax layer.\\nIn CNN, there are mainly three layers:\\n\\n    i. Convolution Layer: It is used to capture the low-level and high level features using kernal over the image.\\n    \\n    ii. Pooling Layer: It is used for dimensionality reduction, and for translation invariance\\n    \\n    iii. Fully Connected Layer: This layer is same as regular NNs, where all the nodes are fully connected with each other. There is mostly sigmoid activation function is used to compute the probabilities of each output/class.\\n    \\n    Furthermore, In CNNs, we use Rectified linear unit(ReLU) activation function  YOUR ANSWER HERE A Convolutional Neural Network has a kernel which is much smaller than the input. This is why it can operate much more efficient than a normal neural network. Normal Neural network O(n \\\\times m), convolutional neural network O ( n $ \\\\times $ k), k is much smaller than m. A convolutional Network operates no large images. The input is preproessed in many layers before it is given to a normal neural network. Preprocessing transforms input into a linear separable problem.CNN learn on grid data (images, 3d volumes) using filters instead of matrix multiplication. Here, the filters are convoluted with the input in the Convolution layer per neuron, where we slide the filter (defined by fiter size $S\\\\times S$) over the input with a stride (step size), and optional zero padding. Strictly speaking, since for RGB images we are working have three color channels, we work with volumes of filters (For example for RGB images of size $32\\\\times 32\\\\times 3$, a filter of window size $S=5$ has the dimensions $5\\\\times5\\\\times3$). Instead of learning a volume of weights for each convolution step, we share weights, considering that one feature detected in one part of the image may be of interest in another part. Then, we apply a nonlinearity, commonly the ReLu activation, as to introduce nonlinearity into our model. To reduce spatial size of our input, we can either use higher strided convolutional layers or pooling layers, for example the popular max pooling layer, where the maximum value over a subvolume is picked. These layers are then stacked, while in the last layers fully connected neurons are typically used to reduce data to for example a classification vector.A CNN uses convolution instead of matrix multiplication. After this there is a non linearity which may be a function like ReLU. There is also a pooling stage which is used to pool the important features.\\n\\nCNNs are translation invartiant.A convolutional neural network consits of convolutional layers.\\nA convolutional layer applies one or multiple kernels (matrix) to an input vector/matrix (typically image) instead of connecting all single inputs of the input vector to the next layer with seperate weights.\\nInstead in training only the kernel is updated. \\nAfter a convolutional layer there is typicall a pooling layer.\\nGiven a window size it reduce the dimensional size of the output of the convolutional layer by using e.g. max or avg pooling.\\nAfterwards the activation layer applies an activation function to the output of the pooling layer.\\nIn the end of a cnn there are typically some fully connected regular layers resulting in a softmax activation function, which assigns the probabilities to the classes output.An Convalutional neuron network assumes the input is an image. Because of that it has a achitecture, so that there are (abwechselnt) covalution and subsampling layers. After the last subsampling layer there is a normal FF NN which classifys the input.A CNN typically consists of multiple CNN layers and a few fully connected FF Network Layers. I'll assume the fully connected part is not so relevant to this questions.\\n\\nA CNN layer is typically a convolution layer and a pooling layer\\n\\nIn the convolution layer a kernel is convolved onto the input. If zero padding is used the result is in the same dimensionality. Depeding on the Kernel the convolution can be 1, 2 or 3D. \\n\\nIn the Pooling layer the result of the convolution is reduced to focus ont the importan features. It also helps on translational invariance.A Convolutional Neural Networks has the following structure:\\n - The input is defined in a grid, so any image or video sequence will be used.\\n - A several number of convolutional layers, where also subsampling (pooling) can be used.\\n - In the convolutional steps a filter will be used for each layer.\\n - After applying multiple convolutional layers, a normal feed-forward networks can be applied, where for example a back propagation algorithm can be used for updating the weights in the numerous iterations.A CNN network consists of:\\n    - Input layer\\n    - conolution layer\\n    - Detection layer\\n    - Pooling layer\\n    - Next layer(because CNN consists of many layers this will be another block of layers similar to what described)Convolutional neural networks are so that first layer is not fully connected but in a way that neuron connections overlap, leading to a grid type structure with overlapping circles. Another layer is connected only with nodes that are responsible for a particular feature (convolutions), then next layer is choosing wich of those convolutions from each ensemble is the most apropriate, after that next layer is fully connected to output neurons.- The CNN has an input layer\\n- The input layer is connected to a convolution layer consisting of three phases:\\n    - convolution stage\\n    - Detector stage\\n    - pooling stage\\n- The next layer (can be a traditional FFNN)CNNs are feed forward neural networks which replaces matrix multiplication task with convolution operation which is much sparse. CNN contain followng stages:\\n+ Convolution (learns local features)\\n+ max pooling (coarse-graining to learn better abstraction of input image)In comparison to other NN, in CNN matrix multiplication is replaced with convolution. Everything else remains the same.An CNN (covolutional neural network) contains a set of hidden layers for feature extration (convolutional layers, pooling layers), and fully-connected layers that classifies the features. The covolutional layers are used to carry out the covolution between the incoming signals with a set of filters, resulting in a set of feature maps. The pooling layers are used to reduce the dimensionality of the feature maps, and make the features invariant of rotation or displacement.A CNN \\n1. starts with a input, where we perform the convolution, which provides a piece of activation. \\n2. Next it is being sent through the activation layer otherwise known as the detection layer. \\n3. Then the final stage is the pooling. In CNN we have different Kernels which are used for extracting certain properties of the inputs. These are called feature maps. After this there is a detection phase which introduces non-linearity. Further there is pooling which introduces translational invariance. There can be many such layers of feature maps and pooling. Finally its reduced to single row input and trained using traditional methods like Back Propogation algorithms.YOUR ANSWER HERE: Convolutional neural networks have 4 main layers where input layer is connected to convolutional and subsampling layers followed by another set of convolutional and subsampling layers connected to the output layer. They are designed to specifically recognize 2-d shapes are invariant to skewing, rotation and the actual location of the object.CNN comprises of multile layers of neurons which perform specific tasks. The initia layer is the convolution layer which performs convolution of the input with the elements of a given kernel. Simpler tasks such as edge detectoíon are performed. Detector layer forms a seconf layer here the output of convolution layer if fed through an activation function such as ReLU. Further, the data is pooled in the pooling layers where downsamping is done to reduce dimensionaity. These layers are repeated to perform more complex feature extraction operations.A CNN is a neural network that replaces matrix multiplication with a mathematical operation called convolution in one or more layers. The main idea behind the structure of a CNN is to replace the activation of neuron with a flipped filter (Convolution layer) and then apply another function called pooling to adust the output further.A CNN consists of several stacked Convolutional layers which can be separated by other layers such as Pooling, Activation, Zero-padding and Dropout which is a form of Regularization. The output layer is generally dependent on the task but could be a Softmax Activation from a Fully connected (also called Densely connected) layer. The number of outputs is usually the number of classes.The structure is as follows:\\n-Convolution: In this layer convolution takes place instead of matrix multiplication\\n-Deconvolution: In this layer deconvolution takes place , by matrix multiplication\\n-Average weight layer: This is a max pooling layer 1. first stage, the layer performs several convolution parallel to produce a set of linear activation\\n2. detector stage, each linear activation is run through a non-linear activation\\n3. third stage, use a pooling function to modify the output of layer.1. Convolution or matrix multiplication: it produces output to hidden layer\\n2. Deconvolution matrix multiplication by transpose matrix: apply back propagation error for output to input.\\n3. Weight update: apply back propagation error from output to weight.* Input layer\\n* Convoluton layer (Affine transformation)\\n* Filtering layer (Sampling)\\n* Learning layer\\n* Output layerCNN has basically four types of layers. They are: convolutional layer, ReLU layer, Pooling layer and the fully connected layer.\\n\\nWe can arrange the convolutional layer and ReLU layer in different ways.\\n\\nOne of the ways is to have 1 convolutional layer, 1 Pooling layer, 1 ReLU layer and repreat this 3 layers again and then finally a fully connected layer.\\n\\nAnother way is to have 1 convolutional layer, 1 pooling layer again repeat the convolutional and pooling layer and then 1 ReLU layer and finally fully connected layer.\\n\\nConvolutional layer is used to find the feature space.THe CNN will have a \\n\\ninput layer\\n\\nconvolutional layer - Here the convolution and sub sampling of the feature maps take place\\n\\nFeed Forward - Neural Network layer\\n\\nOutput layerA convolutional neural network uses the steps of convolution and subsampling alternating in the beginning. Using different kernels during convolution, many feature maps are created. The subsampling step merges the maps to reduce their amount. After some of these steps, a classical feed forward network is in the end to transform the different feature maps to one output layer.A Concolutional neural network has alternating layers of convolution and pooling. The convolutional layer is applying a filter to the input, while the pooling layer sub-samples the input. In some networks this is replaced by strided convolution, which combines these two steps into one. The structure at the end of a CNN is equal to that of a regular feedforward network.YOUR ANSWER HEREA basic CNN can be structured into the three layers convolution, detector and pooling.\\n\\nIn the first layer the convolution operation is performed on the inputs.\\n\\nIn the second layer the the activation function, mostly ReLU, is applied to the result of the convolution.\\n\\nThe last layer can be used to reduce the size of the resulting convoluted images, e.g. by max pooling.\\nConvolutional Neural Network\\n\\nit has often images or video sequences as input. the input is computed by convolution (with different kernels) and downsampling in many steps to smaller but many more input matrices. In last step the matrices are connected to a classical FF NN.  A CNN conists of one or more convolution layers as well as subsampling or pooling layers followed by a fully connected standard FFN. In the convolutution layer kernels are used to create feature maps. A kernel is smaller matrix that is apllied to all possible positions on the input matrix. In the pooling stage the dimension of the rfeature map is reduced. for example by max pooling. CNN uses convolutional layers to extract primitive information from pattern. \\n\\nFirst data is convolved with the first layer to extract some features.\\n\\nOutput of this layer is passed through RELU function to rectify it. \\n\\nThen is downsampled by pulling layer. It basicaly chooses only relevant outputs of convolution layer for further processing. \\n\\nRELU is chosen instead of sigmoid because it doesnt allow gradient to vanish in backpropogation. \\n\\nCNN is has multiple layers and they dont use multiplication matrix. Convolutional Neural Network(CNN) has three main layers in them\\n1. Convolutional Layer\\n2. Pooling or Subsampling Layer\\n3. Output layer.YOUR ANSWER HERE\\n\\nCNN has three components,\\n* Input\\n* Convolution stage\\n* Feed forward network\\n\\nIn CNN the input pass through one or more convolution stage befor it is feed into a feed forward network. The convolution stage uses a hierarchical set of filters, RELU and polling to extract low level as well as high level concepts from the input. The feed forward network along uses the output of the convolution stage and back propagation is used to make adjustment to the network weights as well the filters in the convolution stage.Eingabe eine große Menge. Danach kommen mehere Schichten. In jeder Schicht gibt es parallel unabhängige Netze, dabei wird jeweils eine Teilmenge der Eingabe an eines dieser Netze weiter geleitet, diese teilen jedoch die Gewichte. Durch Pooling wird am Ende der Schicht die Datenmenge der Eingabe reduziert und an die nächste Schicht weitergegeben. Am Ende werden alle parallelen Netze mit einer Ausgabe Schicht verbunden. Jedes der parallelen Netze ist für die Klassifizierung einer Klasse zuständig. YOUR ANSWER HERE\", 'What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?': \"Three items to learn in a RBFN:\\n\\n1. Centroids of the input clusters.\\n2. Widths of the clusters.\\n3. Weights of the synapses connecting the hidden layer and the output layer.\\n\\nThe centroids and widths are learned in an unsupervised fashion while the weights in a supervised fashion. So an RBFN combines unsupervised and supervised learning while a regular NN is completely supervised or completely unsupervised.\\n\\nLearning is fast and is not so sensitive to the unsupervised part.In RBF network, we need to learn **centre** and **width** of gaussian function. We also learn **output weights**\\n\\nDifference between RBF and NNs:\\n\\ni. In RBF, there is only one hidden layer, while in NNs, there can be more than one hidden layer\\n\\nii. In RBF, activation function of hidden layer is Gaussian so parameters are in euclidean norm. While, in NNs, parameters for activation function are product of weights and inputs. \\n\\niii. Parameter computation is different in RBF as compute to other NNs. Like, we compute centre of cluster in RBF with the help of K-means clustering.YOUR ANSWER HERE RBF network need to learn center of activation function. Differenec to other NN is that there are as many activation functions as data points. One con of Radial Basis Funtion is that due to many activation function RBF networks have a huge computational effort. In RBF, we learn the centers of the radial basis functions using unsupervised clustering methods, the weights of the last output layer, and the width of our radial basis functions. As opposed to Multi layer NN, we dont need expensive backpropagation as we only need to train the last layer, while the unsupervised training algorithm does the work the RBF centers. A possbile Con would be that if the RBF centers dont represent the training data point distribution well, some data points may be hard to model.1. weights\\n2. centres (or means) of clusters\\n3. $\\\\sigma$ which is the width of the clusters\\n\\nDifference: Uses functions which are radially invariant.\\n\\nPros:\\n- Easy to learn\\n- Non-linearity\\n- Only dependent on the radial distance\\n\\nCons:\\n- Data required is more\\n- OVerfitting\\n\\nAn RBF network relies on a clustering algorithm. This can be e.g. k-means clustering.\\nThe three items to be learned:\\n1. Cluster center\\n2. Cluster size\\n3. weights connecting the hidden nodes to the output layer\\n\\nDifference to other NNs:\\n- only three layers: input, hidden and output\\n- each node in the hidden layer uses a different activation function depended on the cluster assigned to it\\n- only output weights are trainedIf a RBF network used a gauss function as the activation fnction these thinks have to be learned:\\n\\n- centroide $c_i$ (unsupervised)\\n- sigma (unsupervised)\\n- weights of the output layer (supervised)\\n\\nThe RBF network is easy learning and not so sesitive to the unsupervised learning part.\\nThis question is really unspecific: Difference to other NNs...\\n\\n- Centers\\n- Widths\\n- Weights\\n\\nThe main diffrence is that the RBF uses localized activation functions and it has only one hidden layer.\\nIt applys a non-linear transformation from the input space to the hidden space and a linear transformation from the hidden space into output space. \\n\\nIt is important to use regularization for RBF\\nRBF work well for interpolation, so it should work good for regressionA Radial Basis Function Network has the following structure:\\n - An input layer\\n - A hidden layer, where a non-linear dimensional transformation will be used.\\n - Each neuron of the hidden layer will have a defined center (extracted in previous steps).\\n - A linear transformation will be used to the hidden data space, and the output will be calculated.\\n \\nSo, the three items that must be learning in the RBF networks are:\\n - The centers of each hidden neuron (using for example k-means neighbours algorithm).\\n - The radial function that will be used for the non-linear transformation.\\n - The weights applied into the output layer.\\n \\nThe three items that must be learned in RBFs are:\\n    - The center of the kernel\\n    - The size(standard deviation) of the kernel\\n    - Use distance to center as argument for computation of local fields.\\n- Use radial basis functions as activations\\n- RFBs are only global approximators, \\n- splitted learning instead of global learning+ Kernels\\n+ Only neighbourhoods are computed based on distances.\\n+ Radius of neighbourhoods\\n\\nPros\\n+ RBF are simple and easy to compute. \\n\\nCons\\n+ They remember the data pointsDifferences are:\\n* RBFN has a single hidden layer. Nonlinear hidden layer.\\n* Linear output layer.\\n* Argument of hidden units: Euclidean norm. \\n* Universal approximation property. Local approximators. \\n* Splitted Learning.\\nThe mean of the k clusters, the The three items that needs to be learnt are the centers, widths and depth. Compared to other NN they have a standard 3 layer structure. They can have just one hidden layer. In RBF first inputs are transformed to higer dimension using non linear transformation. This is based on unsupervised learning. Inputs are then learned using least square estimation which is an supervised learning. RBF is based on Covers theorem which states that there is higher probability that data will be linearly separable in higher dimension. YOUR ANSWER HERE:\\n- RBFs are only dependent on the radial distance i.e., distance from the center to the inputThe three parametrs to be learnedin Generalized RBF are 1) cluster centers of the basis functions 2) spread or the width of the basis functions $\\\\sigma$ , and 3) weights of connecting the input and the hidden layers.\\n\\nRBF are differenent from NNs in different ways.\\n\\n1) The kernels are localized functions where as NNs are gobablized\\n\\n2) They use euclidean distance in their activation functions where as NNs use inner products\\n\\n3) They have a single hidden layer and output is a linear combinaation but NNs compulsarily are not the same.YOUR ANSWER HEREThe three open parameters of an RBF network are:\\n1. The centers $c_i$\\n2. The widths $\\\\sigma_i$ and\\n3. The weights $w_i$  \\nThe number of centers $k$ has to be determined by trial and error.In rbf the main advantage is that it follows cover's theorem and the complex pattern classification problem can be solved .1. non-linear transformation function from input space to feature space\\n2. centers of input data that is used for each hidden neuron\\n3. synaptic weights connecting hidden layer and output layer-Three items to be learned,\\n* origin\\n* center\\n\\nPros:\\n* It can transform data from n dimension to infinity dimension.\\n* It can solve non linear problems easily.\\n\\nCons:\\n* It may overfit.\\n* Learning is slow.Center of the hidden neurons, synaptic weights connecting the neurons and\\n\\nRBFs have only 1 hidden layer. There is a non0linear tranformation between the inputs and the hidden space and a linear tranformation between the hidden space and the output space. \\n\\nPros: It can be used for non-linearly separable data.\\n1. Weigths in the network\\n2. the center of the clusters\\n3. variation of the cluster ($\\\\sigma$)\\n\\nDifference: \\n\\nRBF always have only three layers\\n\\nRBF can also trained in an unspervised method\\n\\nRBF can also approximate any continuous function- The centroids of the radial basis functions\\n- The weights of the neurons\\n- The amount of needed neurons\\n\\nA difference to other neural networks is that the centroids of the radial basis functions need to be there.The centers of the clusters, the widhts of the clusters and the weights. \\nIn contrast to other NNs the output only depends on the radial distance to the center of the clusters.The weights, the interpolation matrix have to be learned. The RBF maps the input space into a higher dimensional feature space nonlinearly. The feature space is mapped into the output space linearly. The output space is much smaller than the feature space.\\n\\nPros: local learning\\n\\nCons: feature space can be really large, curse of dimensionalityThe clusters, the width of the basis function and the weights.\\n\\nThe clusters and the width are learned in an unsupervised fashion. While the weights are learning by a standard supervised steepest descent method.\\n\\nPros\\nRBFs can be very easily trained.\\nRBFs can achieve better results with less complexity.\\n\\nCons\\nNot as easy to understandCenters of the radial basis functions\\nbest model (rbf)\\ndistance of each input pair \\n\\npros:\\nnon-linear functions application\\nease to compute \\nusing covers theorem \\n\\ncons:\\nhigh-dimensional Centroids, width, and parameter of function  \\nThe learning of an rbfn is splitted in an unsupervised and a supervised part.   \\nOnly one layer, no vanishing gradient.  \\nPros: easy learning  \\nthe unsupervised part is not very sensitive.  \\nCons:   \\nDifficult to approximate constants  1. Input layer connecting RBF to environment. \\n\\n2. Hidden layer: nonolinear tranformation of input space to hidden space \\n\\n3. Output layer: linear tranformation of hidden space to output space. \\n\\nIt is different than other NNs because for learning patterns, it nonlinearly tranforms the input space to higher dimmensional space. Other NNs do not tranform input. \\n\\nAs it tranforms input patterns to high dimmensional nonlinear space, patterns which are not separable in lower dimmensions have greater chance to be separated. \\n\\nBut if we select basis functions equal to datapoints, problem is ill-formulated.\\n\\nProcessing is computationallly heavy. \\n\\nRegualation becomes problem specific.\\n\\nHence, unsupervied learning is employed to clusters data initially. data varaince\\n\\nFeatures\\n\\nRBF uses suport vector machine which is classifier. it uses different kernels , it doesnot have feedback cycle. \\nit also classifies non linear classification problem. it mainly works with 2 classes C1 ,C2.\\n\\nother NN is can also reggression and there can be feedback (RNN)\\nThe difference of RBF to other NNS are\\n1. RBF has only one hidden layer wheras their is no hard limitation on number of hidden layers on other NNs\\n2. The activation function used in RBF is non linear.YOUR ANSWER HERE\\n\\nA RBF network learns,\\n\\n* The radial function\\n* weight of the hidden to output neuron\\n* Centroid of a cluster\\n\\nDifference:\\n\\n* A RBF is composed of input layer, 1 hidden layer and the output layer. Other NN can generally use as many hidden layers as required.\\n* The transformation from input to hidden layer in RBF is non linear and hidden to output is linear. In most other NN both are non linear.\\n\\nPros/Cons:\\n* This is a very simple learner\\n* There are many variations of RBF available.\\nItems:\\nCenter der radialen Basisfunktionen.\\nGewichte des einen Hidden units zur Ausgabeschicht (Lineare Aktivierung)\\nAnzahl und gewählte Center für die radialen Basisfunktionen.\\nUnterschied zu anderen NNs:\\nHier wird Lokal gelernt anstatt global.YOUR ANSWER HERE\", 'Describe how learning based on k-nearest neighbors works, use pseudo code!': \"K-nearest neighbors:\\n\\n1. Take the input data to be classified.\\n2. Find the first nearest neighbour in terms of euclidean distance.\\n3. Push the class of this nearest neighbour into a list of labels.\\n4. Repeat step 2 and 3 for each K which needs to be odd.\\n5. After all K nearest labels are collected in the list, count the labels in each class.\\n6. Assign to the input data, the class which as maximum count (majority vote).i. First we initialize the random points, those points are considered as centroids of clusters\\n\\nii. Then, for each new points, we compute euclidean distance, and points closest to centrodis are assigned their respective clusters\\n\\niii. We again re-calculate the centroids of clusters\\n\\niv. Repeat 2 and 3 untill convergence is achieved, by making sure, no centroids are moving and cost function is minmized YOUR ANSWER HERE k-nearest neighbor wants to determine encoder $\\\\C which assigns N inputs to K clusters based on a rule to be defined.YOUR ANSWER HERE1. get the input\\n2. find the k- nearest neighbours by finding the distance (euclidean) from the input to all the nodes and selecting the k closest ones\\n3. Class of the input is the most frequent class in the k-neighbnours found (as such, k needs to be odd number)$N$ number of clusters.\\n\\nGiven sample data select $N$ different cluster centers by random.\\n\\nAssign all sample points to the closest cluster\\n\\nrepeat until no further change:\\n - recalucate the cluster centers\\n - Assign all sample points to the closest clustergiven a fixed $k$\\n\\ngiven a point to classify $new$\\n\\ngiven an empty $class$\\n\\ngiven a List of all points $L$\\n\\nfrom 1 to k do\\n\\n    find nearest point $x$ to $new$ in $L$\\n    add class of neares point $x$ in list $class$\\n    new list L = L without nearest neighboor $x$\\n    \\nclass of new = most class in $class$\\n\\nDefine K centroids, random intialised\\nassign each data point a class label\\nwhile the is no change anymore\\n    for each k \\n        calculate the centroid of the datapoint beloging to that label\\n        for each datapoint \\n            determine the nearest centroid\\n            assign a new class label which belongs to the centroid.K-nearest neighbors can be seen as an unsupervised learning method, where for a defined number of groups k, the nearest neighbors will be calculated.\\n\\n1: For a given input data\\n\\n2: Define value k\\n\\n3: Get the k points that are closer to the given points. \\n    1- randomly define a predefined number of cluster centers(CC)\\n    2- calculate the distance of each datapoint from each CC\\n    3- Each data point belongs to the cluster that has the least distance from its CC\\n    4- Calculate a new CC by getting the average of all the points inside a cluster\\n    5- Go to 2 and repeat this process untill we reach the termination conditionFirstly identify nearest neighbouring weights\\nthen choose k amount of neighbors and adapt their weights\\nInitialize k_neighbors = {}, \\n\\nfor every neuron\\n\\n find the nearest neighbor and add it to k_neighbors\\n \\nReturn nearest k_neighbors\\n \\n** Pseudo Code **\\n1. Initiate weights randomly\\n2. Assign labels to k-inputs that are map neuron is closest to.\\n3. append all inputs to map neurons using 2.\\n4. Find centroid of the cluster and move the map neuron to the centroid.\\n5. Do 2, and 4 until some convergence criteria is reached, e.g. maximum iterations is reached or no updates are performed or net distance is below some specified distance.Given: L; X_TEST not element of L; k = number of neighbors that will taken into consideration; function class_of()\\n\\nSet x'={}, L_0=L, Classf={};\\n\\nfor j=1,...k do:\\n\\n    L_{j-1} \\\\ x'; //exclude all the data points which have been identified as nearest neighbors already\\n    \\n    x'=find the closest neighbor of X_TEST in L_j; //e.g. compute eucldea distance\\n    \\n    c = class_of(x');\\n    \\n    Classf=push(c)\\n    \\nset c(x_TEST)= most frequently value in Classf;Train the knn by storing the data labeled points.\\n\\nPresent a test point.\\n> Compute the distance between the test point and all the training data points.\\n\\n> Sort the distance, and choose the k datapoints with smallest distance.\\n\\n> Determine the class of the test point by majority vote.L1 - Data set (x1,x2,x3,x_n)\\nL2 - Storing the dataset based on the number of neighbors. \\nx_test - Test data set. \\n\\nSo we basically have the k value to be an odd number, so that we can select a majority value. \\n\\nfor i based on the number of l:\\n    x' = xtest - distance from the neighboring neuron i. \\n    L2 = smallest x' in this based on the number of K \\n\\nx_test = max(L2)\\n\\nWe select the neurons from the neighborhood by calculating the euclidean distance based on weights. \\nThen if K is 3, we have 3 neurons. So from that we select the label which is fixed maximum on the dataset given in the K-fields. \\ndefine criterial for finding k nearest neighbours <br>\\nfind k nearest neigbours of test input in training dataset <br>\\nfind the class to which most of the neghbours belong <br>\\nassign that class to the test input <br>YOUR ANSWER HERE: Learning based on K-nearest neighbors:\\n- All the input-output samples from the training set are stored in the memory.\\n- For a test input, find the k-nearest neighbors.\\n- Assign the test vector with the class of the most of the neighbours in the neighborhood.\\nParameters: k -number of clusters, x datapoints , c classes\\n\\n1) Initialize randomly k centroid of the custers \\n\\n2) select a data point and compute the set of nearest neighbours of the point using euclidean distances.\\n\\n3) Find the class that maximum number of neighbours belong to and assign the class to the datapoint.\\n\\n4) Once the class is assigned, compute the centroid of each cluster or class, considering all the class members.\\n\\n5) Iterate over all the datapoints and repeat over all points (from step 2) until no update in centroids is required.YOUR ANSWER HERE1. Given: Classified data $X$\\n2. For a new sample $x$:  \\n    Determine the $k$ nearest neighbours in X  \\n    Output $y$ := majority vote of the class of nearest neighbours$ L = {x1,x2...x_n} $\\n\\n$L = L_0$\\n\\n$x' = {}$\\n\\nfor the input (x,d) :\\ndo {\\nx_test \\n\\n}1. identify k classified patterns that lie nearest to the test vector\\n2. assign the test vector to the class that is most frequently presented to the k nearest neighbors1. Define the number of cluster (K)\\n2. Generate random weights\\n3. Find the center of each k (mean)\\n4. Cluster the other outputs by determining the closest neighbor\\n5. Update the weights* Choose a value for k\\n* K represents the number of neighbors\\n* Get a sample from the input space\\n* Find the class based on the majority of votes received from the neighbors. \\n* For example, if the value of k is 3, then let say there are 2 neighbors from class one and 1 neighbor from class two, then the new input sample belongs to class one.Step1: We randomly place the n neurons.\\n\\nStep2: For each data point whichever neuron is closer to it, the datapoint is assigned to that neuron.\\n\\nStep3: Once all the datapoints are assigned, the mean of the datapoints attached to each neuron is calculated and the neuron is shifted to the mean value.\\n\\nStep4: Step 2 and 3 are done until there is no more shift in the neurons position.\\n\\nIn this way the neurons are adjusted.Step1 : Randomly select the k centers \\n\\nStep2 : Cluster the datapoints based on the centers\\n\\nStep3 : the centroid of the cluster becomes the new mean\\n\\nStep4 : repeat step 2 and 3 until there is no more evidential cahnge in the network    input: labeled data set, one unlabeled data point, number k\\n\\n    find the k labeled points, which are closest to the given unlabeled point\\n    from these points, find the label which occurs most often\\n    assign this label to the unlabeled data point1. Get the nearest neighbor of the current x'\\n2. Remove it from L \\n3. Get the class of the current x \\n4. Classify x' as the class that occurs the most often in the neighbors\\n\\nfor 1 to K:\\n\\n    L_i = L/x'\\n    \\n    x_nn = min(|x-x'|)\\n    \\n    c = getclassof(x_nn)\\n    \\n    AmountofClasses.add(c)\\n    \\nsetclassof(x') = Max(AmountofClasses)For the input data x the distance to every other data point is calculated using a distance measure.\\n\\nTake the k data points, which have the minimum distance to x. These are the k-nearest neighbours.\\n\\nThe most frequent class from the neighbours is assigned as the class of the input data.This learning is based on the memory introduced into the dataset. For each data point the nearest neighbours are found via a distance function.\\n\\nfor each datapoint d\\n\\n    neighbours = get_k_nearest_neighbours_of(d)\\n\\n    d.class = get_most_represented_class(neighbours) for a given input \\n    compute distances to other input points\\n    pick k nearest neighboors\\n    look at labeling of neighboors\\n    decide labeling (classification) by highest number of neighboors in one class (german: Mehrheitsentscheid)training_set := training data  \\ndefine #clusters  \\nselect #clusters datapoints as centroids randomly  \\n\\nfor datapoint in training_set:   \\n    calculate distance to centroid\\n    lable dataPoint according to closest centroid\\nend for\\n\\niterate over clusters:  \\n    calculate centroid(K-nearest neighbours is memory based learning.)\\n\\ntake input x \\n\\ncalculate calculate distance of x from each training point. \\n\\nSelect K training points with minimum distce from the data. \\n\\nFetch classes of selected K nearest points. \\n\\nCalculate number points per class in k nearest points. \\n\\ndetermine the class C having maximum points in k nearest pioints \\n\\nThe class of the input point is C.K-nearest neighbors basically works as follows\\n\\n1) the they define randomly the cluster points .\\n\\n2) clacluate the mean of the equlidian distance between the data points. here the points from the previous step acts as centrioids.\\n\\n3) check the variance of the clusters. \\n\\n4) repeat 1-2-3 till you get the proper clusters.\\n1. Slect random number of neghbourhood initially\\n2. Find out the input which is nearest to the weight vector using competitive learning\\n3. Change only the input which wins\\n4. decrease the size of neighbourhood\\n5. RepeatYOUR ANSWER HERE\\n\\nfor x in input_points\\n\\n  neighbours = find_nearest_k_points(x)\\n  \\n  for n in neighbours\\n  \\n    v = get_vote_of(n)\\n    \\n    update_votes_count_for(x,v)\\n    \\n  max = get_max_vote_for(x)\\n  YOUR ANSWER HERELet L be set of labeled data in memory, L ={x1,x2....x_n}, while x_prime is nearest point to the x_test point, in term of euclidean distance. Let class_of be funtion that return class type if certain data point x. And let K be constant number of neighboring points consired in algorithm search. \\n\\nInitialize x_prime = {}, L_0 = L, list_of_classes = {}\\n\\nfor j= 1; j<=K; j++ do:\\n    \\n   L_j = L_(j-1)/x_prime\\n   \\n   x_prime = nearest neighbor to X_test form L_j data\\n   \\n   c = class_of(x_prime)\\n   \\n   list_of_classes.append(c)\\n   \\nend\\n\\nc(x_test):=  most frequent class in list_of_classes\", 'Explain the Bias Variance Dilemma!': \"In machine learning, a choice always needs to be made for the tradeoff between bias and variance. Bias determines how close the result is to the true value and variance determines the sensitivity to flutuations in the training dataset. If bias is reduced variance increases and vice versa. So an optimum tradeoff needs to be chosen which presents a dilemma.Bias Variance dilemma is used to analyse the generalization error of the algorithm. \\nIf the value of Bias is very high, then network does not learn relations between features and outputs correctly(overfitting)\\nIf the value of variance is very high, then network may model the random noise, and it does not learn intended ouputs(underfitting)\\n\\nWe have to to tradeoff between Bias and Variance so that our model can generalize properlyYOUR ANSWER HEREWhen training a model on a limited training data set, we must decide wether we accept a biased model which makes assumptions about the test data, but has a better performance on the train data, or a model with more variance which might model the entirety of the data better but be prone to data noise. Usually we have to decide on a trade off between the two, where we may select well balanced models based on VC dimensions or Cross validation results. Bias and variance are both undesirable to the learning. Bias defines how far the generated output differs from the true value. Variance defines how much the o/p change on changing the input dataset. However, in most cases, it is only possible to decrease one at the expesne of other. Thus, it is called  Bias Variance Dilemma.YOUR ANSWER HEREThe bias is the error we make in the assumption by creating the learning machine (how much we we are away from the actual truth)\\n\\nthe variance is how much the learning machine changes with different training data sets.\\n\\nif we have a high bias we habe a low variance and if we habe a low variance we habe a high bias\\n\\nYou have to to a tradeoff between high bias or high variance. You cannot have both. High vaiance means the model is overfitting the data and therefore the variance on input can be quit hight. High bias means the model is generalization is to unspecific. The Bias is defined as the grade of correctness that a learning algorithm will use. The Variance is defined as the grade of flexibility that the algorithm have given a model to learn. When having the Bias high, but the Variance low, the algorithm will not be flexible into data and will discard any data is not exactly the data that fits into the model. On the other hand, when having the variance high but the bias low, the algorithm will be very flexible into the data and will accept any error data as part of the model to learn.- Bias: the bias is the differnce between the predicted value and the desired value in the generalization run\\n- Variance: is the inadequity in the produced value in the regression and the desired value that we expect from the networkBias Variance dilemma is coming from the fact that you can not have both at the same time. Your network can not be equally great at outputing with extremely high accuracy extremely hight amount of variables. Therefore you need to find balance between the two that suits needs of your neural network.It is refers to the problem of tryning to mantain a balance between two causes of errors in learning algorithms such that the network is able to generalize data beyong that used for training. Namely the bias error and the variance error. Having a high bias error may cause the network to miss important features in the training data, which leads to underfitting. High variance will make the network to memorize noise present in the training data rather than learning features, which lead to overfitting.\\n+ One cannot optimize simultaneously the learning algorithm both for learning maximum variance in the data and learning localization which can be termed as bias.The Bias Variance Dilemma tells us that the bias (the difference between the actual and desired output) and the variance (output difference between each trial) cannot be decreased at the same time. A complex model results in small variance and larger variance.So in machine learning problem, minimizing the two main source of error simultenously does not allow the networks to be generalised very easy. \\n \\nIf bias increase, variance decrease. And vice versa also holds.\\n\\n1. Bias tells us how close we are to the true value! \\n2. Variance tells us how they vary for different data set. \\n\\nSo this is a standard problem in NNHigh value of bias means netowrk is unable to learn the data whereas higher variance means its difficult to learn the training data successfully.YOUR ANSWER HEREBias and variances are the estimation errors.\\n\\nBias corresponds to the inability of the learning machine to appropriately approximate the function to be learnt. Hence this induces a deviation from the actual function\\n\\nVariance is the inadequacy of the training data to allow the a learning machine to succesfully learn the function. \\n\\nThe dilemma is that , to completely learn the actual function( to reduce variance-related error), the training data required should consist of infinite samples. However, this resuts in slower convergence, inturn, bias error increases.\\n\\nTherefore trade of between both the errors need to be made.YOUR ANSWER HEREBias is the difference between the predicted and true value. Variance is the range of several predicted values of the same datapoint. It is desirable to have low bias and low variance to ensure the predicted value is consistently close to the true value. The Bias Variance dilemma is that to achieve low bias, the variance becomes high and vice versa. Hence, there is always a tradeoff between the two.Bias variance dilemma refers to the problem of minimizing the two sources of error bias errror and variance error simultaneously which creates probblem in generaliztion of the network.\\nBias error: It is the error that occurs while setting the parameters of the network\\nvariance error:It refers to how sensetive the network is to the fluctuations in the dataset.YOUR ANSWER HEREBias variance dilemma is a process of simultaneously decreasing two sources of error that prevents supervised learning algorithm from generalizing beyond the trained data.Bias is used to affine transform of $u$.\\n\\nIt helps to shift the classifier line.\\n\\n$$v=u+b$$Bias: How close the estimate is to the true value.\\n\\nvariance: How much does the estimate vary for different training sets.\\n\\nwe always have either hugh variance low bias or low variance high bias.Bias : differnce between the estmated output and the actual output\\n\\nVariance: The range of output of a network for different training set. \\n\\nBias and Variance can't be decreased at the same time for many networks. ONly one at a time can be decreased\\n\\nNO ANSWER HEREWhen adapting the parameters of a network we can either have a small bias or a small variance. If we have a small bias the approximation of the network is close to the real one, but the variance between trials is very high. If we have a low variance, the bias can't be minimized and the network has a bigger error between the apüproximation and the real value. YOUR ANSWER HEREIdeally bias and variance would be 0 after learning a machine. However, bias and variance counteract eachother: when bias decreases, variance rises and respectively in the other direction. This leads to the dilemma that either one of the values has to be present.YOUR ANSWER HEREUsualy only one of Bias and Variance can be minimized. In an RBFN for example  \\nfew kernels with greater width leads to a high bias but a low variance. If you choose many kernels with smaller width the bias is low but the variance is high. Higher complexity models need more training data.YOUR ANSWER HEREBias is an proides an affine transformation. and it is treated  a extra inputs. which noramll taken as +1High bias and variance is desirable in input. Bias Variance Dilemma is the property of input data where if the bias is increased the variance decreases and vice versa. It is difficult to find a tradeoff between them. YOUR ANSWER HERE\\n\\nBias: Bias means how much the prediction differs from the true value\\n\\nVariance: Variance means how much the prediction varies for different datasets\\n\\nThe Dilemma is that both generally can not be reduced simultaneously. A learning machine can reduce one at the cost of other.Bias: Unfähigkeit des Netzes korrekt  zu adaptieren.\\nVariance: Unfähigkeit der Daten, die zugrunde liegende Hypothese korrekt zu beschreibenYOUR ANSWER HERE\"}\n"
     ]
    }
   ],
   "source": [
    "newDict = {}\n",
    "\n",
    "for i in range(len(ques_list)):\n",
    "    if ques_list[i] in newDict:\n",
    "        newDict[ques_list[i]] += answer_list[i]\n",
    "    else:\n",
    "        newDict[ques_list[i]] = answer_list[i]\n",
    "\n",
    "print(newDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>student_answer</th>\n",
       "      <th>question_id</th>\n",
       "      <th>student_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Give a definition for the term \"artificial ne...</td>\n",
       "      <td>An artificial neural network is a massively pa...</td>\n",
       "      <td>1</td>\n",
       "      <td>mas-usb-011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Give a definition for the term \"artificial ne...</td>\n",
       "      <td>Artificial neural network consists of:\\n\\n    ...</td>\n",
       "      <td>1</td>\n",
       "      <td>mas-usb-041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Give a definition for the term \"artificial ne...</td>\n",
       "      <td>YOUR ANSWER HERE An artificial neural network ...</td>\n",
       "      <td>1</td>\n",
       "      <td>mas-usb-051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Give a definition for the term \"artificial ne...</td>\n",
       "      <td>An ANN is a layered graphical model containing...</td>\n",
       "      <td>1</td>\n",
       "      <td>mas-usb-061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Give a definition for the term \"artificial ne...</td>\n",
       "      <td>Artificial Neural Networks are large parallel ...</td>\n",
       "      <td>1</td>\n",
       "      <td>mas-usb-071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>Explain the Bias Variance Dilemma!</td>\n",
       "      <td>Bias is an proides an affine transformation. a...</td>\n",
       "      <td>17</td>\n",
       "      <td>mas-usb-5017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>Explain the Bias Variance Dilemma!</td>\n",
       "      <td>High bias and variance is desirable in input. ...</td>\n",
       "      <td>17</td>\n",
       "      <td>mas-usb-5117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>Explain the Bias Variance Dilemma!</td>\n",
       "      <td>YOUR ANSWER HERE\\n\\nBias: Bias means how much ...</td>\n",
       "      <td>17</td>\n",
       "      <td>mas-usb-5417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>Explain the Bias Variance Dilemma!</td>\n",
       "      <td>Bias: Unfähigkeit des Netzes korrekt  zu adapt...</td>\n",
       "      <td>17</td>\n",
       "      <td>mas-usb-5717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>Explain the Bias Variance Dilemma!</td>\n",
       "      <td>YOUR ANSWER HERE</td>\n",
       "      <td>17</td>\n",
       "      <td>mas-usb-5817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>663 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0     Give a definition for the term \"artificial ne...   \n",
       "1     Give a definition for the term \"artificial ne...   \n",
       "2     Give a definition for the term \"artificial ne...   \n",
       "3     Give a definition for the term \"artificial ne...   \n",
       "4     Give a definition for the term \"artificial ne...   \n",
       "..                                                 ...   \n",
       "658                 Explain the Bias Variance Dilemma!   \n",
       "659                 Explain the Bias Variance Dilemma!   \n",
       "660                 Explain the Bias Variance Dilemma!   \n",
       "661                 Explain the Bias Variance Dilemma!   \n",
       "662                 Explain the Bias Variance Dilemma!   \n",
       "\n",
       "                                        student_answer  question_id  \\\n",
       "0    An artificial neural network is a massively pa...            1   \n",
       "1    Artificial neural network consists of:\\n\\n    ...            1   \n",
       "2    YOUR ANSWER HERE An artificial neural network ...            1   \n",
       "3    An ANN is a layered graphical model containing...            1   \n",
       "4    Artificial Neural Networks are large parallel ...            1   \n",
       "..                                                 ...          ...   \n",
       "658  Bias is an proides an affine transformation. a...           17   \n",
       "659  High bias and variance is desirable in input. ...           17   \n",
       "660  YOUR ANSWER HERE\\n\\nBias: Bias means how much ...           17   \n",
       "661  Bias: Unfähigkeit des Netzes korrekt  zu adapt...           17   \n",
       "662                                   YOUR ANSWER HERE           17   \n",
       "\n",
       "       student_id  \n",
       "0     mas-usb-011  \n",
       "1     mas-usb-041  \n",
       "2     mas-usb-051  \n",
       "3     mas-usb-061  \n",
       "4     mas-usb-071  \n",
       "..            ...  \n",
       "658  mas-usb-5017  \n",
       "659  mas-usb-5117  \n",
       "660  mas-usb-5417  \n",
       "661  mas-usb-5717  \n",
       "662  mas-usb-5817  \n",
       "\n",
       "[663 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,17):\n",
    "    z = df.iloc[:,2][i]\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_json_string = json.dumps(ques_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \" Give a definition for the term \\\\\"artificial neural network\\\\\" and mention, how it resembles the human brain!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Define the mathematical model of a neuron, use the appropriate technical terms!\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!\\\\n\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Explain classification and regression; what is the difference?\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Write down the SOM learning in pseudo code.\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \"Give the basic idea of an SVM using the correct terminology!\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \" What role does the method of steepest decent have when learning a network?\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Define: a hypothesis $h \\\\\\\\in H$ shatters a dataset $A \\\\\\\\subseteq X \\\\\\\\Leftrightarrow \\\\\\\\ldots$\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Write down and explain the Widrow-Hoff learning rule!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"Explain back propagation, use the correct technical terms!\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"When learning using steepest descent, explain the role of the learning rate? What is a danger?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"How does a Reduced Boltzman Machine work (main idea)?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Define: Echo State Network (ESN), how are they different to FF NNs?\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"Describe: the structure on an CNN.\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Describe how learning based on k-nearest neighbors works, use pseudo code!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\", \"Explain the Bias Variance Dilemma!\"]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5d76f142dc64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mscript\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvarious\u001b[0m \u001b[0mqueries\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtop\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a simple application for sentence embeddings: semantic search\n",
    "We have a corpus with various sentences. Then, for a given query sentence,\n",
    "we want to find the most similar sentence in this corpus.\n",
    "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = ['A man is eating a food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'A cheetah is running behind its prey.'\n",
    "          ]\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']\n",
    "query_embeddings = embedder.encode(queries)\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "closest_n = 5\n",
    "for query, query_embedding in zip(queries, query_embeddings):\n",
    "    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
    "\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for idx, distance in results[0:closest_n]:\n",
    "        print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36689969285267143, 0.7337993857053429, 1.0000000000000002, 1.0000000000000002]\n"
     ]
    }
   ],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "text1 = ['An artificial neural network is a massively parallel distributed processor with simple processing units that has the natural propensity to store experiential knowledge and make use of them','to the human brain in two ways:','An artificial neural network is similar to the human brain in two ways:','An artificial neural network is similar to the human brain in two ways:']\n",
    "text2 = 'An artificial neural network is similar to the human brain in two ways:'\n",
    "# vector1 = text_to_vector(text1)\n",
    "vector2 = text_to_vector(text2)\n",
    "\n",
    "vector_final = []\n",
    "\n",
    "for i in range(len(text1)):\n",
    "    vectors = text_to_vector(text1[i])\n",
    "    vector_final.append(vectors)\n",
    "    \n",
    "# print(vector_final)\n",
    "\n",
    "cosine_final = []\n",
    "\n",
    "for j in range(len(vector_final)):\n",
    "    cosine = get_cosine(vector2, vector_final[j])\n",
    "    cosine_final.append(cosine)\n",
    "    \n",
    "print(cosine_final)\n",
    "\n",
    "\n",
    "\n",
    "# cosine = get_cosine(vector1, vector2)\n",
    "\n",
    "# print('Cosine:', cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
